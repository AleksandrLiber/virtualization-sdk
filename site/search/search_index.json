{
    "docs": [
        {
            "location": "/",
            "text": "Introduction to Delphix Masking\n\u00b6\n\n\nChallenge\n\u00b6\n\n\nWith data breach incidents regularly making the news and increasing\npressure from regulatory bodies and consumers alike, organizations must\nprotect sensitive data across the enterprise. Contending with insider\nand outsider threats while staying compliant with mandates such as\nHIPAA, PCI, and GDPR is no easy task\u2014especially as teams simultaneously\ntry to make their organizations more agile.\n\n\nTo tackle the problem of protecting sensitive information, companies are\nincreasingly scrutinizing the tools they\u2019ve deployed. Instead of\nreactive perimeter defenses, security minded organizations must focus on\nproactively protecting the interior of their systems: their data.\nMoreover, while mainstay approaches such as encryption may be effective\nfor securing data-in-motion or data resident in hard drives, they are\nill-suited for protecting non-production environments for development,\ntesting, and reporting.\n\n\nSolution\n\u00b6\n\n\nThe masking capability of the Delphix Dynamic Data Platform represents\nan automated approach to protecting non-production environments,\nreplacing confidential information such as social security numbers,\npatient records, and credit card information with fictitious, yet\nrealistic data.\n\n\nUnlike encryption measures that can be bypassed through schemes to\nobtain user credentials, masking irreversibly protects data in\ndownstream environments. Consistent masking of data while maintaining\nreferential integrity across heterogeneous data sources enables Delphix\nmasking to provide superior coverage compared to other solutions\u2014all\nwithout the need for programming expertise. Moreover, the Delphix\nDynamic Data Platform seamlessly integrates masking with data delivery \ncapabilities, ensuring the security of sensitive data before it is made \navailable for development and testing, or sent to an offsite data center \nor the public cloud.\n\n\nDelphix Masking is a multi-user, browser-based web application that\nprovides complete, secure, and scalable software for your sensitive data\ndiscovery, masking and tokenization needs, while meeting\nenterprise-class infrastructure requirements. The Delphix Dynamic Data\nPlatform has several key characteristics to enable your organization to\nsuccessfully protect sensitive data across the enterprise:\n\n\n\n\n\n\nEnd-to-End Masking\n \u2014 The Delphix platform automatically detects\n    confidential information, irreversibly masks data values, then\n    generates reports and email notifications to confirm that all sensitive \n    data has been masked.\n\n\n\n\n\n\nRealistic Data\n \u2014 Data masked with the Delphix platform is\n    production-like in quality. Masked application data in\n    non-production environments remains fully functional and realistic,\n    enabling the development of higher-quality code.\n\n\n\n\n\n\nMasking Integrated with Virtualization\n \u2014 Most masking solutions\n    fail due to the need for repeated, lengthy batch jobs for extracting\n    and masking data and lack delivery capabilities for downstream\n    environments. The Delphix Dynamic Data Platform seamlessly\n    integrates data masking with \ndata\n    virtualization\n,\n    allowing teams to quickly deliver masked, virtual data copies on\n    premises or into private, public, and hybrid cloud environments.\n\n\n\n\n\n\nReferential Integrity\n \u2014 Delphix masks consistently across\n    heterogeneous data sources. To do so, metadata and data is scanned\n    to identify and preserve the primary/foreign key relationships\n    between elements so that data is masked the same way across\n    different tables and databases.\n\n\n\n\n\n\nAlgorithms/Frameworks\n \u2014 Seven algorithm frameworks allow users \n    to create and configure algorithms to match specific security policies.\n    Over twenty five out-of-the-box, preconfigured algorithms help businesses\n    mask everything from names and addresses to credit card numbers and text \n    fields. Moreover, the Delphix platform includes prepackaged profiling \n    sets for healthcare and financial information, as well as the ability \n    to perform tokenization: a process that can be used to obfuscate data \n    sent for processing, then reversed when the processed data set is \n    returned.\n\n\n\n\n\n\nEase of Use\n \u2014 With a single solution, Delphix customers can mask\n    data across a variety of platforms. Moreover, businesses are not\n    required to program their own masking algorithms or rely on\n    extensive administrator involvement. Our web-based UI enables\n    masking with a few mouse clicks and little training.\n\n\n\n\n\n\n\n\n\n\n\nAutomated discovery of sensitive data\n \u2014 The Delphix Profiler\n    automatically identifies sensitive data across databases and files,\n    the time-consuming work associated with a data masking project is\n    reduced significantly.\n\n\n\n\nHigh Level Platform Architecture\n\u00b6\n\n\nThe Delphix Dynamic Data Platform is made up of 4 main services each of\nwhich play a very important part in delivering fresh secure data to\nanybody that needs it. These include:\n\n\n\n\n\n\nVirtualize \u2014\n Delphix compresses the data that it gathers, often to\n    one-third or more of the original size. From that compressed data \n    footprint, Delphix virtualizes the data and allows operators to create\n    lightweight, virtual data copies. Virtual copies are fully\n    readable/writable and independent. They can be spun up or torn down\n    in just minutes. And they take up a fraction of the storage space of\n    physical copies -- 10 virtual copies can fit into the space of one\n    physical copy.\n\n\n\n\n\n\nIdentify and Secure \u2014\n The Delphix platform continuously protects sensitive\n    information with integrated data masking. Masking secures\n    confidential data -- names, email addresses, patient records, SSNs\n    -- by replacing sensitive values with fictitious, yet realistic\n    equivalents. Delphix automatically identifies sensitive values then\n    applies custom or predefined masking algorithms. By seamlessly\n    integrating data masking and provisioning into a single platform,\n    Delphix ensures that secure data delivery is effortless and\n    repeatable.\n\n\n\n\n\n\nManage \u2014\n Data operators can now quickly provision secure data\n    copies -- in minutes -- to users in their target environments. The\n    Delphix platform serves as a single point of control to manage those\n    copies. Data operators maintain full control and visibility into\n    downstream environments. They can easily audit, monitor, and report\n    against access and usage.\n\n\n\n\n\n\nSelf Service\n \n\u2014\n Provides developers, testers, analysts, data\n    scientists, or other users with controls to manipulate data at-will.\n    Users can refresh data to reflect the latest state of production,\n    rewind environments to a prior point in time, bookmark data copies\n    for later use, branch data copies to work across multiple releases,\n    or easily share data with other users.\n\n\n\n\n\n\n\n\nHow Delphix Identifies Sensitive Data\n\u00b6\n\n\nOur platform helps you quickly identify your organization\u2019s sensitive data. This sensitive data identification is done using two different methods, column level profiling and data level profiling.\n\n\nColumn Level Profiling\n\n\nColumn level profiling uses REGEX expressions to scan the column names\n(metadata) of the selected data sources. There are several dozen\npre-configured profile expressions (like the one below) designed to\nidentify common sensitive data types (SSN, Name, Addresses, etc). You\nalso have the ability to write/import your own profile expressions.\n\n\n\n\nData Level Profiling\n\n\nData level profiling also uses REGEX expressions, but to scan the actual\ndata instead of the metadata. Similar to column level profiling, there\nare sefveral dozen pre-configured expressions (like the one below) and\nyou can write/import your own.\n\n\n\n\nFor both column and data level profiling, when data is identified as\nsensitive, Delphix recommends/assigns particular algorithms to be used\nwhen securing the data. The platform comes with several dozen\npre-configured algorithms which are recommended when the profiler finds\ncertain sensitive data.\n\n\nHow Delphix Secures Your Sensitive Data\n\u00b6\n\n\nDelphix strives to make available multiple methods for securing your\ndata, depending on your needs. The two secure methods Delphix currently\nsupports are masking (anonymization) and tokenization\n(pseudonymization).\n\n\nMasking\n\n\nData masking secures your data by replacing values with realistic yet\nfictitious data. Seven out-of-the-box algorithm frameworks help\nbusinesses mask everything from names and social security numbers to\nimages and text fields. Algorithms can also be configured or customized\nto match specific security policies.\n\n\n\n\nTokenization\n\n\nTokenization uses reversible algorithms so that the data can be returned\nto its original state. Tokenization is a form of encryption where the\nactual data \u2013 such as names and addresses \u2013 are converted into tokens\nthat have similar properties to the original data (text, length, etc.)\nbut no longer convey any meaning.",
            "title": "Introduction to Delphix Masking"
        },
        {
            "location": "/#introduction-to-delphix-masking",
            "text": "",
            "title": "Introduction to Delphix Masking"
        },
        {
            "location": "/#challenge",
            "text": "With data breach incidents regularly making the news and increasing\npressure from regulatory bodies and consumers alike, organizations must\nprotect sensitive data across the enterprise. Contending with insider\nand outsider threats while staying compliant with mandates such as\nHIPAA, PCI, and GDPR is no easy task\u2014especially as teams simultaneously\ntry to make their organizations more agile.  To tackle the problem of protecting sensitive information, companies are\nincreasingly scrutinizing the tools they\u2019ve deployed. Instead of\nreactive perimeter defenses, security minded organizations must focus on\nproactively protecting the interior of their systems: their data.\nMoreover, while mainstay approaches such as encryption may be effective\nfor securing data-in-motion or data resident in hard drives, they are\nill-suited for protecting non-production environments for development,\ntesting, and reporting.",
            "title": "Challenge"
        },
        {
            "location": "/#solution",
            "text": "The masking capability of the Delphix Dynamic Data Platform represents\nan automated approach to protecting non-production environments,\nreplacing confidential information such as social security numbers,\npatient records, and credit card information with fictitious, yet\nrealistic data.  Unlike encryption measures that can be bypassed through schemes to\nobtain user credentials, masking irreversibly protects data in\ndownstream environments. Consistent masking of data while maintaining\nreferential integrity across heterogeneous data sources enables Delphix\nmasking to provide superior coverage compared to other solutions\u2014all\nwithout the need for programming expertise. Moreover, the Delphix\nDynamic Data Platform seamlessly integrates masking with data delivery \ncapabilities, ensuring the security of sensitive data before it is made \navailable for development and testing, or sent to an offsite data center \nor the public cloud.  Delphix Masking is a multi-user, browser-based web application that\nprovides complete, secure, and scalable software for your sensitive data\ndiscovery, masking and tokenization needs, while meeting\nenterprise-class infrastructure requirements. The Delphix Dynamic Data\nPlatform has several key characteristics to enable your organization to\nsuccessfully protect sensitive data across the enterprise:    End-to-End Masking  \u2014 The Delphix platform automatically detects\n    confidential information, irreversibly masks data values, then\n    generates reports and email notifications to confirm that all sensitive \n    data has been masked.    Realistic Data  \u2014 Data masked with the Delphix platform is\n    production-like in quality. Masked application data in\n    non-production environments remains fully functional and realistic,\n    enabling the development of higher-quality code.    Masking Integrated with Virtualization  \u2014 Most masking solutions\n    fail due to the need for repeated, lengthy batch jobs for extracting\n    and masking data and lack delivery capabilities for downstream\n    environments. The Delphix Dynamic Data Platform seamlessly\n    integrates data masking with  data\n    virtualization ,\n    allowing teams to quickly deliver masked, virtual data copies on\n    premises or into private, public, and hybrid cloud environments.    Referential Integrity  \u2014 Delphix masks consistently across\n    heterogeneous data sources. To do so, metadata and data is scanned\n    to identify and preserve the primary/foreign key relationships\n    between elements so that data is masked the same way across\n    different tables and databases.    Algorithms/Frameworks  \u2014 Seven algorithm frameworks allow users \n    to create and configure algorithms to match specific security policies.\n    Over twenty five out-of-the-box, preconfigured algorithms help businesses\n    mask everything from names and addresses to credit card numbers and text \n    fields. Moreover, the Delphix platform includes prepackaged profiling \n    sets for healthcare and financial information, as well as the ability \n    to perform tokenization: a process that can be used to obfuscate data \n    sent for processing, then reversed when the processed data set is \n    returned.    Ease of Use  \u2014 With a single solution, Delphix customers can mask\n    data across a variety of platforms. Moreover, businesses are not\n    required to program their own masking algorithms or rely on\n    extensive administrator involvement. Our web-based UI enables\n    masking with a few mouse clicks and little training.      Automated discovery of sensitive data  \u2014 The Delphix Profiler\n    automatically identifies sensitive data across databases and files,\n    the time-consuming work associated with a data masking project is\n    reduced significantly.",
            "title": "Solution"
        },
        {
            "location": "/#high-level-platform-architecture",
            "text": "The Delphix Dynamic Data Platform is made up of 4 main services each of\nwhich play a very important part in delivering fresh secure data to\nanybody that needs it. These include:    Virtualize \u2014  Delphix compresses the data that it gathers, often to\n    one-third or more of the original size. From that compressed data \n    footprint, Delphix virtualizes the data and allows operators to create\n    lightweight, virtual data copies. Virtual copies are fully\n    readable/writable and independent. They can be spun up or torn down\n    in just minutes. And they take up a fraction of the storage space of\n    physical copies -- 10 virtual copies can fit into the space of one\n    physical copy.    Identify and Secure \u2014  The Delphix platform continuously protects sensitive\n    information with integrated data masking. Masking secures\n    confidential data -- names, email addresses, patient records, SSNs\n    -- by replacing sensitive values with fictitious, yet realistic\n    equivalents. Delphix automatically identifies sensitive values then\n    applies custom or predefined masking algorithms. By seamlessly\n    integrating data masking and provisioning into a single platform,\n    Delphix ensures that secure data delivery is effortless and\n    repeatable.    Manage \u2014  Data operators can now quickly provision secure data\n    copies -- in minutes -- to users in their target environments. The\n    Delphix platform serves as a single point of control to manage those\n    copies. Data operators maintain full control and visibility into\n    downstream environments. They can easily audit, monitor, and report\n    against access and usage.    Self Service   \u2014  Provides developers, testers, analysts, data\n    scientists, or other users with controls to manipulate data at-will.\n    Users can refresh data to reflect the latest state of production,\n    rewind environments to a prior point in time, bookmark data copies\n    for later use, branch data copies to work across multiple releases,\n    or easily share data with other users.",
            "title": "High Level Platform Architecture"
        },
        {
            "location": "/#how-delphix-identifies-sensitive-data",
            "text": "Our platform helps you quickly identify your organization\u2019s sensitive data. This sensitive data identification is done using two different methods, column level profiling and data level profiling.  Column Level Profiling  Column level profiling uses REGEX expressions to scan the column names\n(metadata) of the selected data sources. There are several dozen\npre-configured profile expressions (like the one below) designed to\nidentify common sensitive data types (SSN, Name, Addresses, etc). You\nalso have the ability to write/import your own profile expressions.   Data Level Profiling  Data level profiling also uses REGEX expressions, but to scan the actual\ndata instead of the metadata. Similar to column level profiling, there\nare sefveral dozen pre-configured expressions (like the one below) and\nyou can write/import your own.   For both column and data level profiling, when data is identified as\nsensitive, Delphix recommends/assigns particular algorithms to be used\nwhen securing the data. The platform comes with several dozen\npre-configured algorithms which are recommended when the profiler finds\ncertain sensitive data.",
            "title": "How Delphix Identifies Sensitive Data"
        },
        {
            "location": "/#how-delphix-secures-your-sensitive-data",
            "text": "Delphix strives to make available multiple methods for securing your\ndata, depending on your needs. The two secure methods Delphix currently\nsupports are masking (anonymization) and tokenization\n(pseudonymization).  Masking  Data masking secures your data by replacing values with realistic yet\nfictitious data. Seven out-of-the-box algorithm frameworks help\nbusinesses mask everything from names and social security numbers to\nimages and text fields. Algorithms can also be configured or customized\nto match specific security policies.   Tokenization  Tokenization uses reversible algorithms so that the data can be returned\nto its original state. Tokenization is a form of encryption where the\nactual data \u2013 such as names and addresses \u2013 are converted into tokens\nthat have similar properties to the original data (text, length, etc.)\nbut no longer convey any meaning.",
            "title": "How Delphix Secures Your Sensitive Data"
        },
        {
            "location": "/What's New for Masking/",
            "text": "What's New for Masking Engines\n\u00b6\n\n\nSynchronizing Masking Jobs & Universal Settings Across Engines\n\u00b6\n\n\nIn 5.2 we introduced the ability to synchronize Masking Algorithms between engines to ensure consistent masking, regardless of the engine executing the masking. In 5.3 are expanding the list of syncable objects to include:\n\n\n\n\nMasking Jobs\n\n\nConnectors\n\n\nRulesets\n\n\nDomains\n\n\nFile Formats\n\n\n\n\nThe sync of objects is possible through improvements to several sync API endpoints, including:\n\n\n\n\nGET /syncable-objects[?object_type=\n]\n\n\nPOST /export\n\n\nPOST /export-async\n\n\nPOST /import\n\n\nPOST/import-async\n\n\n\n\nThis expansion of syncable objects ensures that users can sync their Masking Jobs and all the objects necessary for that masking job to execute successfully - regardless of the masking engine it lives on, allowing for easier scaling of Delphix Masking across the enterprise. Please see \nManaging Multiple Masking Engines\n  for more details.\n\n\nSupport for Kerberized Connections\n\u00b6\n\n\nIn 5.2.4 we added support for Kerberos for our Oracle Masking Connector. In 5.3 we have expanded the list of connectors that support Kerberos to:\n\n\n\n\nSQL Server\n\n\nSybase\n\n\n\n\nTo enable Kerberized connectors your engine must be configured properly and you must configure your masking Connectors for Kerberos. Kerberos can be enabled by going to the Advanced mode on Oracle, SQL Server and Sybase. Please see \nManaging Connectors\n for more details.\n\n\n\n\nNew API Endpoints\n\u00b6\n\n\nIn 5.2 we released an all-new set of API endpoints allowing for the automation of many masking workflows. In 5.3 we have expanded this list of API endpoints around Algorithms, Users, Roles, File Upload, System Information, Login, Rulesets, and Connector. Below are the net new API endpoints:\n\n\n\n\n\n\n\n\nGroup\n\n\nEndpoints\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAlgorithms\n\n\nPOST /algorithms\n\n\nCreate algorithm\n\n\n\n\n\n\n\n\nDELETE /algorithms/{algorithmName}\n\n\nDelete algorithm by name\n\n\n\n\n\n\n\n\nGET /algorithms/{algorithmName}\n\n\nGet algorithm by name\n\n\n\n\n\n\n\n\nPUT /algorithms/{algorithmName}\n\n\nUpdate algorithm by name\n\n\n\n\n\n\n\n\nPUT /algorithms/{algorithmName}/randomize-key\n\n\nRandomize key by name\n\n\n\n\n\n\nUsers\n\n\nGET /users\n\n\nGet all users\n\n\n\n\n\n\n\n\nPOST /users\n\n\nCreate user\n\n\n\n\n\n\n\n\nDELETE /users/{userId}\n\n\nDelete user by ID\n\n\n\n\n\n\n\n\nGET /users/{userId}\n\n\nGet user by ID\n\n\n\n\n\n\n\n\nPUT /users/{userId}\n\n\nUpdate user by ID\n\n\n\n\n\n\nRoles\n\n\nGET /roles\n\n\nGet all roles\n\n\n\n\n\n\n\n\nPOST /roles\n\n\nCreate role\n\n\n\n\n\n\n\n\nDELETE /roles/{roleId}\n\n\nDelete role by ID\n\n\n\n\n\n\n\n\nGET /roles/{roleId}\n\n\nGet role by ID\n\n\n\n\n\n\n\n\nPUT /roles/{roleId}\n\n\nUpdate role by ID\n\n\n\n\n\n\nRulesets\n\n\nPUT /database-rulesets/{databaseRulesetId}/bulk-table-update\n\n\nUpdate the rule set\u2019s tables\n\n\n\n\n\n\n\n\nPUT /database-rulesets/{databaseRulesetId}/refresh\n\n\nRefresh the rule set\n\n\n\n\n\n\nConnectors\n\n\nPOST /database-connectors/{databaseConnectorId}/test\n\n\nTest a database connector\n\n\n\n\n\n\n\n\nPOST /database-connectors/test\n\n\nTest an unsaved database connector\n\n\n\n\n\n\n\n\nPOST /file-connectors/{fileConnectorId}/test\n\n\nTest a file connector\n\n\n\n\n\n\n\n\nPOST /file-connectors/test\n\n\nTest an unsaved file connector\n\n\n\n\n\n\nAsync Tasks\n\n\nGET /async-tasks\n\n\nGet all asyncTasks\n\n\n\n\n\n\n\n\nGET /async-tasks/{asyncTaskId}\n\n\nGet asyncTask by ID\n\n\n\n\n\n\n\n\nPUT /async-tasks/{asyncTaskId}/cancel\n\n\nCancel asyncTask by ID\n\n\n\n\n\n\nFile Upload/Download\n\n\nDELETE /file-uploads\n\n\nDelete all file uploads\n\n\n\n\n\n\n\n\nPOST /file-uploads\n\n\nUpload file\n\n\n\n\n\n\n\n\nGET /file-downloads/{fileDownloadId}\n\n\nDownload file\n\n\n\n\n\n\nSystem Information\n\n\nGET /system-information\n\n\nGet version, etc.\n\n\n\n\n\n\nLogin/Logout\n\n\nPUT /logout\n\n\nUser logout\n\n\n\n\n\n\nExecutions\n\n\nGET /execution-components\n\n\nStatus for a table, file, or Mainframe file\n\n\n\n\n\n\n\n\nIn addition to the net new API endpoints, we have improved pre-existing API endpoints. Some of the improvements include:\n\n\n\n\nAddition of DB2 iSeries & Mainframe to connector endpoints.\n\n\nAddition of Kerberos configuration on Oracle, SQL Server & Sybase connectors\n\n\nAbility to have ruleset refresh drop tables\n\n\nSupport for XML file types\n\n\nAddition of dataType to column metadata\n\n\n\n\nFor more information please on Delphix Masking APIs please see \nAPI documentation\n.  Please note that the previous generation of Masking APIs (commonly referred to as V4) is EOL and no longer supported in this release. All users are encouraged to migrate to the V5 APIs.",
            "title": "What's New for Masking Engines"
        },
        {
            "location": "/What's New for Masking/#whats-new-for-masking-engines",
            "text": "",
            "title": "What's New for Masking Engines"
        },
        {
            "location": "/What's New for Masking/#synchronizing-masking-jobs-universal-settings-across-engines",
            "text": "In 5.2 we introduced the ability to synchronize Masking Algorithms between engines to ensure consistent masking, regardless of the engine executing the masking. In 5.3 are expanding the list of syncable objects to include:   Masking Jobs  Connectors  Rulesets  Domains  File Formats   The sync of objects is possible through improvements to several sync API endpoints, including:   GET /syncable-objects[?object_type= ]  POST /export  POST /export-async  POST /import  POST/import-async   This expansion of syncable objects ensures that users can sync their Masking Jobs and all the objects necessary for that masking job to execute successfully - regardless of the masking engine it lives on, allowing for easier scaling of Delphix Masking across the enterprise. Please see  Managing Multiple Masking Engines   for more details.",
            "title": "Synchronizing Masking Jobs &amp; Universal Settings Across Engines"
        },
        {
            "location": "/What's New for Masking/#support-for-kerberized-connections",
            "text": "In 5.2.4 we added support for Kerberos for our Oracle Masking Connector. In 5.3 we have expanded the list of connectors that support Kerberos to:   SQL Server  Sybase   To enable Kerberized connectors your engine must be configured properly and you must configure your masking Connectors for Kerberos. Kerberos can be enabled by going to the Advanced mode on Oracle, SQL Server and Sybase. Please see  Managing Connectors  for more details.",
            "title": "Support for Kerberized Connections"
        },
        {
            "location": "/What's New for Masking/#new-api-endpoints",
            "text": "In 5.2 we released an all-new set of API endpoints allowing for the automation of many masking workflows. In 5.3 we have expanded this list of API endpoints around Algorithms, Users, Roles, File Upload, System Information, Login, Rulesets, and Connector. Below are the net new API endpoints:     Group  Endpoints  Description      Algorithms  POST /algorithms  Create algorithm     DELETE /algorithms/{algorithmName}  Delete algorithm by name     GET /algorithms/{algorithmName}  Get algorithm by name     PUT /algorithms/{algorithmName}  Update algorithm by name     PUT /algorithms/{algorithmName}/randomize-key  Randomize key by name    Users  GET /users  Get all users     POST /users  Create user     DELETE /users/{userId}  Delete user by ID     GET /users/{userId}  Get user by ID     PUT /users/{userId}  Update user by ID    Roles  GET /roles  Get all roles     POST /roles  Create role     DELETE /roles/{roleId}  Delete role by ID     GET /roles/{roleId}  Get role by ID     PUT /roles/{roleId}  Update role by ID    Rulesets  PUT /database-rulesets/{databaseRulesetId}/bulk-table-update  Update the rule set\u2019s tables     PUT /database-rulesets/{databaseRulesetId}/refresh  Refresh the rule set    Connectors  POST /database-connectors/{databaseConnectorId}/test  Test a database connector     POST /database-connectors/test  Test an unsaved database connector     POST /file-connectors/{fileConnectorId}/test  Test a file connector     POST /file-connectors/test  Test an unsaved file connector    Async Tasks  GET /async-tasks  Get all asyncTasks     GET /async-tasks/{asyncTaskId}  Get asyncTask by ID     PUT /async-tasks/{asyncTaskId}/cancel  Cancel asyncTask by ID    File Upload/Download  DELETE /file-uploads  Delete all file uploads     POST /file-uploads  Upload file     GET /file-downloads/{fileDownloadId}  Download file    System Information  GET /system-information  Get version, etc.    Login/Logout  PUT /logout  User logout    Executions  GET /execution-components  Status for a table, file, or Mainframe file     In addition to the net new API endpoints, we have improved pre-existing API endpoints. Some of the improvements include:   Addition of DB2 iSeries & Mainframe to connector endpoints.  Addition of Kerberos configuration on Oracle, SQL Server & Sybase connectors  Ability to have ruleset refresh drop tables  Support for XML file types  Addition of dataType to column metadata   For more information please on Delphix Masking APIs please see  API documentation .  Please note that the previous generation of Masking APIs (commonly referred to as V4) is EOL and no longer supported in this release. All users are encouraged to migrate to the V5 APIs.",
            "title": "New API Endpoints"
        },
        {
            "location": "/Getting_Started/Prerequisites/",
            "text": "Prerequisites\n\u00b6\n\n\nThis section will detail the hardware/software requirements needed to\ndeploy the Delphix Engine with the Masking service. The Delphix\nEngine is a self-contained operating environment and application that is\nprovided as a Virtual Appliance. Our Virtual Appliance is certified to\nrun on a variety of platforms including VMware, AWS, and Azure.\n\n\nThe Delphix Engine should be placed on a server where it will not\ncontend with other VMs for network, storage or other compute resources.\nThe Delphix Engine is a CPU and I/O intensive application, and deploying it in\nan environment where it must share resources with other virtual\nmachines, can significantly reduce performance.\n\n\nDelphix Masking and Delphix Virtualization should never be\nrun inside the same virtual machine. Always use seperate, dedicated Delphix Engines for Masking and Virtualization.\n\n\nClient Web Browser\n\u00b6\n\n\nThe Delphix Engine's graphical interface can be accessed from a variety of\ndifferent web browsers. The Delphix Engine currently supports the following web browsers:\n\n\n\n\nMicrosoft Internet Explorer 10.0 or higher\n\n\nMozilla Firefox 35.0 or higher\n\n\nChrome 40 or higher\n\n\n\n\n\n\nTIP - Microsoft Internet Explorer\n\n\nMake sure that Internet Explorer is not configured for compatibility mode.\n\n\n\n\nVMware Virtual Platform\n\u00b6\n\n\nThis section covers the virtual machine requirements for installation of\na dedicated Delphix Masking Engine on the VMware Virtual platform.\n\n\nVMWare Platform\n\u00b6\n\n\nThe Delphix Engine can be run on several version of VMware ESX/ESXI (\nsee\nsupport matrix\n).\n\n\nVirtual CPUs\n\u00b6\n\n\nThe minimum amount of virtual CPUs is 8v CPUs. CPU resource shortfalls\ncan occur under high I/O throughput conditions. Also, CPU reservation is\nstrongly recommended for the Delphix VM, so that Delphix is guaranteed\nthe full complement of vCPUs even when resources are overcommitted.\n\n\nVirtual Memory\n\u00b6\n\n\nThe minimum amount of virtual memory is 16GB vRAM but we highly\nrecommend 32GB or higher. The masking service on the Delphix Engine uses its memory to process database and file blocks. More memory can sometimes improve performance. Memory reservation is a requirement for the Delphix VM.\nOvercommitting memory resources in the ESX server will significantly\nimpact the performance of the Delphix Engine. Reservation ensures that\nthe Delphix Engine will not stall while waiting for the ESX server to\npage in the engine\u2019s memory.\n\n\n\n\nTIP - Do not allocate all memory to the Delphix Engine\n\n\nNever allocate all available physical memory to the Delphix VM. You must set aside memory for the ESX Server to perform hypervisor activities before you assign memory to Delphix and other VMs. The default ESX minimum free memory requirement is 6% of total RAM. When free memory falls below 6%, ESX starts swapping out the Delphix guest OS. We recommend leaving about 8-10% free to avoid swapping\n\n\nFor example, when running on an ESX Host with 512GB of physical memory, allocate no more than 470GB (92%) to the Delphix VM (and all other VMs on that host).\n\n\n\n\nDelphix Engine System Disk Storage\n\u00b6\n\n\nThe minimum recommended storage on the Delphix Engine System Disk is\n300GB. The System disk may need to be substantially larger if bulk\nlogging will be enabled. The actual size will depend on the data being\nmasked. The VMDK for the Delphix Engine system disk storage is often\ncreated in the same VMFS volume as the Delphix VM definition. In that\ncase, the datastore must have sufficient space to hold the Delphix VM\nconfiguration, the VMDK for the system disk, and a paging area if a\nmemory reservation was not enabled for the Delphix Engine.\n\n\nAWS EC2 Platform\n\u00b6\n\n\nThis section covers the virtual machine requirements for installation of\na dedicated Delphix Masking Engine on Amazon's Elastic Cloud Compute\n(EC2) platform.\n\n\nFor best performance, the Delphix Masking Engine and all database\nservers should be in the same AWS region.\n\n\nInstance Types\n\u00b6\n\n\nThe Delphix Engine can run on a variety of different instances,\nincluding large memory instances (preferred) and high I/O instances. The\nDelphix Engine most closely resembles a storage appliance and performs\nbest when provisioned using a storage optimized instance type. We\nrecommend the following large memory and high I/O instances:\n\n\n\n\n\n\n\n\nLarge Memory Instances (preferred)\n\n\nHigh I/O Instances (supported)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr4.2xlarge\n\n\n\n\n\n\nr4.4xlarge\n\n\n\n\n\n\nr4.8xlarge\n\n\n\n\n\n\nr3.2xlarge\n\n\n\n\n\n\nr3.4xlarge\n\n\n\n\n\n\nr3.8xlarge\n\n\n\n\n\n\n\n\n\n\ni3.2xlarge\n\n\n\n\n\n\ni3.4xlarge\n\n\n\n\n\n\ni3.8xlarge\n\n\n\n\n\n\ni2.2xlarge\n\n\n\n\n\n\ni2.4xlarge\n\n\n\n\n\n\ni2.8xlarge\n\n\n\n\n\n\n\n\n\n\n\n\n\nLarger instance types provide more CPU, which can prevent resource\nshortfalls under high I/O throughput conditions. Larger instances also\nprovide more memory, which the Delphix Engine uses to cache database\nblocks. More memory will provide better read performance. For more\ninformation please refer to, \nVirtual Machine\nRequirements for AWS EC2 Platform\n.\n\n\nNetwork Configurations\n\u00b6\n\n\nYou must deploy the Delphix Engine and all database or file hosts\nin a VPC network to ensure that private IP addresses are\nstatic and do not change when you restart instances. When adding\nenvironments to the Delphix Engine, you must use the host's VPC (static\nprivate) IP addresses.\n\n\nThe EC2 Delphix instance must be launched with a static IP address;\nhowever, the default behavior for VPC instances is to launch with a\ndynamic public IP address \u2013 which can change whenever you restart the\ninstance. If you're using a public IP address for your Delphix Engine,\nstatic IP addresses can only be achieved by using assigned AWS Elastic\nIP\nAddresses.\n\n\n\n\nTIP - Port Configuration\n\n\nThe default security group will only open port 22 for secure shell (SSH) access. You must modify the security group to allow access to all of the networking ports used by the Delphix Engine and the various source and target engines. See \nGeneral Network and Connectivity Requirements\n for information about specific port configurations.\n\n\n\n\nEBS Configurations\n\u00b6\n\n\nAll attached storage devices must be EBS volumes. Delphix does not\nsupport the use of instance store volumes. Because EBS volumes are\nconnected to EC2 instances via the network, other network activity on\nthe instance can affect throughput to EBS volumes. EBS optimized\ninstances provide guaranteed throughput to EBS volumes and are required\n(for instance types that support it) in order to provide consistent and\npredictable storage performance.\n\n\nUse EBS volumes with provisioned IOPs in order to provide consistent and\npredictable performance. The number of provisioned IOPs depends on the\nestimated IO workload on the Delphix Engine. Provisioned IOPs volumes\nmust be configured with a volume size at least 30 GiB times the number\nof provisioned IOPs. For example, a volume with 3,000 IOPS must be\nconfigured with at least 100 GiB.\n\n\nI/O requests of up to 256 kilobytes (KB) are counted as a single I/O\noperation (IOP) for provisioned IOPs volumes. Each volume can be\nconfigured for up to 4,000 IOPs.\n\n\nGeneral Storage Configurations\n\u00b6\n\n\nThe minimum recommended storage on the Delphix Engine System Disk is 300GB. The System disk may need to be substantially larger if bulk logging will be enabled. The actual size will depend on the data being masked.\n\n\nAdd storage when storage capacity approaches 30% free. Keep all EBS volumes the same size. Add new storage by provisioning new volumes of the same size.\n\n\nUse at least 3 EBS volumes to maximize performance. This enables the Delphix File System (DxFS) to make sure that its file systems are always consistent on disk without additional serialization. This also enables the Delphix Engine to achieve higher I/O rates by queueing more I/O operations to its storage.\n\n\nAzure Platform\n\u00b6\n\n\nThis section covers the virtual machine requirements for installation of\na dedicated Delphix Masking Engine on Microsoft's Azure cloud platform.  \n\n\nFor best performance, the Delphix Masking Engine and all database\nservers should be in the same Azure Region.\n\n\nInstance Types\n\u00b6\n\n\nThe Delphix Engine can run on a variety of different Azure instances.\nThe Delphix Engine most closely resembles a storage appliance and\nperforms best when provisioned using a storage optimized instance type.\nWe recommend the following memory and storage optimized instances:\n\n\n\n\n\n\nGS3 - 8 CPUs, 112GB, 16 TB (20,000 IOPS)\n\n\n\n\n\n\nGS4 - 16 CPUs, 244GB, 32 TB (40,000 IOPS)\n\n\n\n\n\n\nGS5 - 32 CPUs, 448GB, 64 TB (80,000 IOPS)\n\n\n\n\n\n\nNetwork Configurations\n\u00b6\n\n\nNetwork Configuration for Delphix Engines running on the Azure Platform\ncan be configured on the Azure Virtual Network (VNet). Delphix Engine\nand all the source and target environments must be accessible within the\nsame virtual\nnetwork.\n\n\n\n\nTIP - Port Configuration\n\n\nYou must modify the security group to allow access to all of the networking ports used by the Delphix Engine and the various source and target engines. See \nGeneral Network and Connectivity Requirements\n for information about specific port configurations.\n\n\n\n\nStorage Configurations\n\u00b6\n\n\nWhen configuring storage for the Delphix Engine we recommend Azure\nPremium Storage which uses solid-state drives (SSDs). Devices up to 1024\nare supported with a total maximum of 64tb.\n\n\nI/O requests of up to 256 kilobytes (KB) are counted as a single I/O\noperation (IOP) for provisioned IOPs volumes. IOPS vary based on storage\nsize with a maximum of 5,000 IOPS.",
            "title": "Prerequisites"
        },
        {
            "location": "/Getting_Started/Prerequisites/#prerequisites",
            "text": "This section will detail the hardware/software requirements needed to\ndeploy the Delphix Engine with the Masking service. The Delphix\nEngine is a self-contained operating environment and application that is\nprovided as a Virtual Appliance. Our Virtual Appliance is certified to\nrun on a variety of platforms including VMware, AWS, and Azure.  The Delphix Engine should be placed on a server where it will not\ncontend with other VMs for network, storage or other compute resources.\nThe Delphix Engine is a CPU and I/O intensive application, and deploying it in\nan environment where it must share resources with other virtual\nmachines, can significantly reduce performance.  Delphix Masking and Delphix Virtualization should never be\nrun inside the same virtual machine. Always use seperate, dedicated Delphix Engines for Masking and Virtualization.",
            "title": "Prerequisites"
        },
        {
            "location": "/Getting_Started/Prerequisites/#client-web-browser",
            "text": "The Delphix Engine's graphical interface can be accessed from a variety of\ndifferent web browsers. The Delphix Engine currently supports the following web browsers:   Microsoft Internet Explorer 10.0 or higher  Mozilla Firefox 35.0 or higher  Chrome 40 or higher    TIP - Microsoft Internet Explorer  Make sure that Internet Explorer is not configured for compatibility mode.",
            "title": "Client Web Browser"
        },
        {
            "location": "/Getting_Started/Prerequisites/#vmware-virtual-platform",
            "text": "This section covers the virtual machine requirements for installation of\na dedicated Delphix Masking Engine on the VMware Virtual platform.",
            "title": "VMware Virtual Platform"
        },
        {
            "location": "/Getting_Started/Prerequisites/#vmware-platform",
            "text": "The Delphix Engine can be run on several version of VMware ESX/ESXI ( see\nsupport matrix ).",
            "title": "VMWare Platform"
        },
        {
            "location": "/Getting_Started/Prerequisites/#virtual-cpus",
            "text": "The minimum amount of virtual CPUs is 8v CPUs. CPU resource shortfalls\ncan occur under high I/O throughput conditions. Also, CPU reservation is\nstrongly recommended for the Delphix VM, so that Delphix is guaranteed\nthe full complement of vCPUs even when resources are overcommitted.",
            "title": "Virtual CPUs"
        },
        {
            "location": "/Getting_Started/Prerequisites/#virtual-memory",
            "text": "The minimum amount of virtual memory is 16GB vRAM but we highly\nrecommend 32GB or higher. The masking service on the Delphix Engine uses its memory to process database and file blocks. More memory can sometimes improve performance. Memory reservation is a requirement for the Delphix VM.\nOvercommitting memory resources in the ESX server will significantly\nimpact the performance of the Delphix Engine. Reservation ensures that\nthe Delphix Engine will not stall while waiting for the ESX server to\npage in the engine\u2019s memory.   TIP - Do not allocate all memory to the Delphix Engine  Never allocate all available physical memory to the Delphix VM. You must set aside memory for the ESX Server to perform hypervisor activities before you assign memory to Delphix and other VMs. The default ESX minimum free memory requirement is 6% of total RAM. When free memory falls below 6%, ESX starts swapping out the Delphix guest OS. We recommend leaving about 8-10% free to avoid swapping  For example, when running on an ESX Host with 512GB of physical memory, allocate no more than 470GB (92%) to the Delphix VM (and all other VMs on that host).",
            "title": "Virtual Memory"
        },
        {
            "location": "/Getting_Started/Prerequisites/#delphix-engine-system-disk-storage",
            "text": "The minimum recommended storage on the Delphix Engine System Disk is\n300GB. The System disk may need to be substantially larger if bulk\nlogging will be enabled. The actual size will depend on the data being\nmasked. The VMDK for the Delphix Engine system disk storage is often\ncreated in the same VMFS volume as the Delphix VM definition. In that\ncase, the datastore must have sufficient space to hold the Delphix VM\nconfiguration, the VMDK for the system disk, and a paging area if a\nmemory reservation was not enabled for the Delphix Engine.",
            "title": "Delphix Engine System Disk Storage"
        },
        {
            "location": "/Getting_Started/Prerequisites/#aws-ec2-platform",
            "text": "This section covers the virtual machine requirements for installation of\na dedicated Delphix Masking Engine on Amazon's Elastic Cloud Compute\n(EC2) platform.  For best performance, the Delphix Masking Engine and all database\nservers should be in the same AWS region.",
            "title": "AWS EC2 Platform"
        },
        {
            "location": "/Getting_Started/Prerequisites/#instance-types",
            "text": "The Delphix Engine can run on a variety of different instances,\nincluding large memory instances (preferred) and high I/O instances. The\nDelphix Engine most closely resembles a storage appliance and performs\nbest when provisioned using a storage optimized instance type. We\nrecommend the following large memory and high I/O instances:     Large Memory Instances (preferred)  High I/O Instances (supported)        r4.2xlarge    r4.4xlarge    r4.8xlarge    r3.2xlarge    r3.4xlarge    r3.8xlarge      i3.2xlarge    i3.4xlarge    i3.8xlarge    i2.2xlarge    i2.4xlarge    i2.8xlarge       Larger instance types provide more CPU, which can prevent resource\nshortfalls under high I/O throughput conditions. Larger instances also\nprovide more memory, which the Delphix Engine uses to cache database\nblocks. More memory will provide better read performance. For more\ninformation please refer to,  Virtual Machine\nRequirements for AWS EC2 Platform .",
            "title": "Instance Types"
        },
        {
            "location": "/Getting_Started/Prerequisites/#network-configurations",
            "text": "You must deploy the Delphix Engine and all database or file hosts\nin a VPC network to ensure that private IP addresses are\nstatic and do not change when you restart instances. When adding\nenvironments to the Delphix Engine, you must use the host's VPC (static\nprivate) IP addresses.  The EC2 Delphix instance must be launched with a static IP address;\nhowever, the default behavior for VPC instances is to launch with a\ndynamic public IP address \u2013 which can change whenever you restart the\ninstance. If you're using a public IP address for your Delphix Engine,\nstatic IP addresses can only be achieved by using assigned AWS Elastic\nIP\nAddresses.   TIP - Port Configuration  The default security group will only open port 22 for secure shell (SSH) access. You must modify the security group to allow access to all of the networking ports used by the Delphix Engine and the various source and target engines. See  General Network and Connectivity Requirements  for information about specific port configurations.",
            "title": "Network Configurations"
        },
        {
            "location": "/Getting_Started/Prerequisites/#ebs-configurations",
            "text": "All attached storage devices must be EBS volumes. Delphix does not\nsupport the use of instance store volumes. Because EBS volumes are\nconnected to EC2 instances via the network, other network activity on\nthe instance can affect throughput to EBS volumes. EBS optimized\ninstances provide guaranteed throughput to EBS volumes and are required\n(for instance types that support it) in order to provide consistent and\npredictable storage performance.  Use EBS volumes with provisioned IOPs in order to provide consistent and\npredictable performance. The number of provisioned IOPs depends on the\nestimated IO workload on the Delphix Engine. Provisioned IOPs volumes\nmust be configured with a volume size at least 30 GiB times the number\nof provisioned IOPs. For example, a volume with 3,000 IOPS must be\nconfigured with at least 100 GiB.  I/O requests of up to 256 kilobytes (KB) are counted as a single I/O\noperation (IOP) for provisioned IOPs volumes. Each volume can be\nconfigured for up to 4,000 IOPs.",
            "title": "EBS Configurations"
        },
        {
            "location": "/Getting_Started/Prerequisites/#general-storage-configurations",
            "text": "The minimum recommended storage on the Delphix Engine System Disk is 300GB. The System disk may need to be substantially larger if bulk logging will be enabled. The actual size will depend on the data being masked.  Add storage when storage capacity approaches 30% free. Keep all EBS volumes the same size. Add new storage by provisioning new volumes of the same size.  Use at least 3 EBS volumes to maximize performance. This enables the Delphix File System (DxFS) to make sure that its file systems are always consistent on disk without additional serialization. This also enables the Delphix Engine to achieve higher I/O rates by queueing more I/O operations to its storage.",
            "title": "General Storage Configurations"
        },
        {
            "location": "/Getting_Started/Prerequisites/#azure-platform",
            "text": "This section covers the virtual machine requirements for installation of\na dedicated Delphix Masking Engine on Microsoft's Azure cloud platform.    For best performance, the Delphix Masking Engine and all database\nservers should be in the same Azure Region.",
            "title": "Azure Platform"
        },
        {
            "location": "/Getting_Started/Prerequisites/#instance-types_1",
            "text": "The Delphix Engine can run on a variety of different Azure instances.\nThe Delphix Engine most closely resembles a storage appliance and\nperforms best when provisioned using a storage optimized instance type.\nWe recommend the following memory and storage optimized instances:    GS3 - 8 CPUs, 112GB, 16 TB (20,000 IOPS)    GS4 - 16 CPUs, 244GB, 32 TB (40,000 IOPS)    GS5 - 32 CPUs, 448GB, 64 TB (80,000 IOPS)",
            "title": "Instance Types"
        },
        {
            "location": "/Getting_Started/Prerequisites/#network-configurations_1",
            "text": "Network Configuration for Delphix Engines running on the Azure Platform\ncan be configured on the Azure Virtual Network (VNet). Delphix Engine\nand all the source and target environments must be accessible within the\nsame virtual\nnetwork.   TIP - Port Configuration  You must modify the security group to allow access to all of the networking ports used by the Delphix Engine and the various source and target engines. See  General Network and Connectivity Requirements  for information about specific port configurations.",
            "title": "Network Configurations"
        },
        {
            "location": "/Getting_Started/Prerequisites/#storage-configurations",
            "text": "When configuring storage for the Delphix Engine we recommend Azure\nPremium Storage which uses solid-state drives (SSDs). Devices up to 1024\nare supported with a total maximum of 64tb.  I/O requests of up to 256 kilobytes (KB) are counted as a single I/O\noperation (IOP) for provisioned IOPs volumes. IOPS vary based on storage\nsize with a maximum of 5,000 IOPS.",
            "title": "Storage Configurations"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/",
            "text": "Data Source Support\n\u00b6\n\n\nThe Delphix Masking service supports profiling, masking, and tokenizing\na variety of different data sources including distributed databases,\nmainframe, PaaS databases, and files. At a high level, Delphix Masking\nbreaks up support for data sources into two categories:\n\n\n\n\n\n\nDedicated Delphix Connectors:\n These are data sources that the\n    Delphix Engine can connect to directly using built-in connectors\n    that have been optimized to perform masking, profiling and\n    tokenization.\n\n\n\n\n\n\nFEML Sources:\n FEML (File Extract Mask and Load) is a method\n    used to mask and tokenize data sources that do not have dedicated\n    Delphix Connectors. FEML uses existing APIs from data sources to\n    extract the data to a file, masks the file, and then uses APIs to\n    load the masked file back into the database.\n\n\n\n\n\n\nDedicated Delphix Connectors\n\u00b6\n\n\nThe Delphix Engine has dedicated masking connectors for the following\ndata sources:\n\n\n\n\n\n\nDistributed Database:\n DB2 LUW, Oracle, MS SQL, MySQL, SAP ASE\n    (Sybase), PostgreSQL, MariaDB\n\n\n\n\n\n\nMainframe/Midrange:\n DB2 Z/OS, DB2 iSeries, Mainframe\n\n\n\n\n\n\nPaaS Database:\n AWS RDS Oracle, RDS PostgreSQL, RDS MYSQL, RDS MariaDB,\n    RDS MS SQL Server\n\n\n\n\n\n\nFiles:\n Excel, Fixed Width, Delimited, XML\n\n\n\n\n\n\nFor a detailed view of all the versions, features, etc Delphix supports\non each data source - see the sections below.\n\n\nDB2 LUW Connector\n\u00b6\n\n\nIntroduction\n\u00b6\n\n\nDB2 for Linux, UNIX and Windows is a database server product developed\nby IBM. Sometimes\ncalled DB2 LUW for brevity, it is part of the DB2 family of database\nproducts. DB2 LUW is the\n\"Common Server\" product member of the DB2 family, designed to run on\nthe most popular\noperating systems. By contrast, all other DB2 products are specific to a\nsingle platform.\n\n\nSupport Matrix\n\u00b6\n\n\nThe Delphix DB2 LUW connector supports the following versions of DB2\nLUW:\n\n\n\n\n\n\n\n\nVersion\n\n\nLinux\n\n\nUnix\n\n\nWindows\n\n\n\n\n\n\n\n\n\n\n9.1\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n9.5\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n9.7\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n9.8\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n10.1\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n10.5\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n11.1\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n\n\nAvailable Features\n\u00b6\n\n\nThe DB2 LUW connector supports profiling and masking/tokenization features. Below is a list of which options are & are not available for jobs using the DB2 LUW connector:\n\n\n\n\n\n\n\n\n\n\nFeature\n\n\nAvailability\n\n\n\n\n\n\n\n\n\n\nIn-Place Masking Mode\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams / Threads\n\n\nAvailable\n\n\n\n\n\n\n\n\nBulk Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nBatch Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nDrop Indexes\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nUnavailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nUnavailable\n\n\n\n\n\n\n\n\nIdentity Column Support\n\n\nUnavailable\n\n\n\n\n\n\nOn-The-Fly Masking Mode\n\n\nRestart Ability\n\n\nAvailable\n\n\n\n\n\n\n\n\nTruncate\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nUnavailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nUnavailable\n\n\n\n\n\n\n\n\nCreate Target\n\n\nAvailable\n\n\n\n\n\n\nProfiling\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams\n\n\nAvailable\n\n\n\n\n\n\n\n\nOracle Connector\n\u00b6\n\n\nIntroduction\n\u00b6\n\n\nOracle Database (commonly referred to as Oracle RDBMS or simply as\nOracle) is a multi-model database management system produced and\nmarketed by Oracle Corporation.\n\n\nSupport Matrix\n\u00b6\n\n\n\n\n\n\n\n\nVersion\n\n\nLinux\n\n\nUnix\n\n\nWindows\n\n\nAWS RDS\n\n\n\n\n\n\n\n\n\n\n10g\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nN/A\n\n\n\n\n\n\n11gR1\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nN/A\n\n\n\n\n\n\n11gR2\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n12c\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n12cR2\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nN/A\n\n\n\n\n\n\n18c\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nN/A\n\n\n\n\n\n\n\n\nAvailable Features\n\u00b6\n\n\nThe Oracle connector supports profiling and masking/tokenization features. Below is a list of which options are and are not available for jobs using the Oracle connector:\n\n\n\n\n\n\n\n\n\n\nFeature\n\n\nAvailability\n\n\n\n\n\n\n\n\n\n\nIn-Place Masking Mode\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams / Threads\n\n\nAvailable\n\n\n\n\n\n\n\n\nBulk Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nBatch Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nDrop Indexes\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nAvailable\n\n\n\n\n\n\n\n\nIdentity Column Support\n\n\nAvailable\n\n\n\n\n\n\nOn-The-Fly Masking Mode\n\n\nRestart Ability\n\n\nAvailable\n\n\n\n\n\n\n\n\nTruncate\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nAvailable\n\n\n\n\n\n\n\n\nCreate Target\n\n\nAvailable\n\n\n\n\n\n\nProfiling\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams\n\n\nAvailable\n\n\n\n\n\n\n\n\nMS SQL Connector\n\u00b6\n\n\nIntroduction\n\u00b6\n\n\nMicrosoft SQL Server is a relational database management system\ndeveloped by Microsoft. As a database server, it is a software product\nwith the primary function of storing and retrieving data as requested by\nother software applications\u2014which may run either on the same computer or\non another computer across a network (including the Internet).\n\n\nSupport Matrix\n\u00b6\n\n\n\n\n\n\n\n\nVersion\n\n\nLinux\n\n\nUnix\n\n\nWindows\n\n\nAWS RDS\n\n\n\n\n\n\n\n\n\n\n2005\n\n\nN/A\n\n\nN/A\n\n\nSupported\n\n\nN/A\n\n\n\n\n\n\n2008\n\n\nN/A\n\n\nN/A\n\n\nSupported\n\n\nN/A\n\n\n\n\n\n\n2008 R2\n\n\nN/A\n\n\nN/A\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n2012\n\n\nN/A\n\n\nN/A\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n2014\n\n\nN/A\n\n\nN/A\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n2016\n\n\nSupported\n\n\nN/A\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n\n\nAvailable Features\n\u00b6\n\n\nThe MS SQL connector supports profiling and masking/tokenization features. Below is a list of which options are and are not available for jobs using the MS SQL connector.\n\n\n\n\n\n\n\n\n\n\nFeature\n\n\nAvailability\n\n\n\n\n\n\n\n\n\n\nIn-Place Masking Mode\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams / Threads\n\n\nAvailable\n\n\n\n\n\n\n\n\nBulk Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nBatch Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nDrop Indexes\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nAvailable\n\n\n\n\n\n\n\n\nIdentity Column Support\n\n\nAvailable\n\n\n\n\n\n\nOn-The-Fly Masking Mode\n\n\nRestart Ability\n\n\nAvailable\n\n\n\n\n\n\n\n\nTruncate\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nAvailable\n\n\n\n\n\n\n\n\nCreate Target\n\n\nAvailable\n\n\n\n\n\n\nProfiling\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams\n\n\nAvailable\n\n\n\n\n\n\n\n\nPostgreSQL Connector\n\u00b6\n\n\nIntroduction\n\u00b6\n\n\nPostgreSQL, often simply Postgres, is an object-relational database management system (ORDBMS) with an emphasis on extensibility and standards compliance. PostgreSQL is developed by the PostgreSQL Global Development Group, a diverse group of many companies and individual contributors. It is free and open-source, released under the terms of the PostgreSQL License, a permissive software license.\n\n\nSupport Matrix\n\u00b6\n\n\n\n\n\n\n\n\nVersion\n\n\nLinux\n\n\nUnix\n\n\nWindows\n\n\nAWS RDS\n\n\n\n\n\n\n\n\n\n\n9.2\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nN/A\n\n\n\n\n\n\n9.3\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nN/A\n\n\n\n\n\n\n9.4\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n9.5\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n9.6\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n10\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n\n\nAvailable Features\n\u00b6\n\n\nThe PostgreSQL connector supports profiling and masking/tokenization features. Below is a list of which options are & are not available for jobs using the PostgreSQL connector:\n\n\n\n\n\n\n\n\n\n\nFeature\n\n\nAvailability\n\n\n\n\n\n\n\n\n\n\nIn-Place Masking Mode\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams / Threads\n\n\nAvailable\n\n\n\n\n\n\n\n\nBulk Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nBatch Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nDrop Indexes\n\n\nUnavailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nUnavailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nUnavailable\n\n\n\n\n\n\n\n\nIdentity Column Support\n\n\nAvailable\n\n\n\n\n\n\nOn-The-Fly Masking Mode\n\n\nRestart Ability\n\n\nUnavailable\n\n\n\n\n\n\n\n\nTruncate\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nAvailable\n\n\n\n\n\n\n\n\nCreate Target\n\n\nAvailable\n\n\n\n\n\n\nProfiling\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams\n\n\nUnavailable\n\n\n\n\n\n\n\n\nMySQL / MariaDB Connector\n\u00b6\n\n\nIntroduction\n\u00b6\n\n\nMySQL is an open-source relational database management system (RDBMS).\nMySQL was owned and sponsored by a single for-profit firm, the Swedish\ncompany MySQL AB. MySQL is now owned by Oracle Corporation.\n\n\nMariaDB is a community-developed fork of the MySQL relational database\nmanagement system intended to remain free under the GNU GPL. Development\nis led by some of the original developers of MySQL, who forked it due to\nconcerns over its acquisition by Oracle Corporation.\n\n\nA MySQL Connector may be used to connect to either a MySQL or MariaDB\ndatabase instance.\n\n\nMySQL Support Matrix\n\u00b6\n\n\n\n\n\n\n\n\nVersion\n\n\nLinux\n\n\nUnix\n\n\nWindows\n\n\nAWS RDS\n*\n\n\n\n\n\n\n\n\n\n\n5.5\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n5.6\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n5.7\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n\n\nMariaDB Support Matrix\n\u00b6\n\n\n\n\n\n\n\n\nVersion\n\n\nLinux\n\n\nUnix\n\n\nWindows\n\n\nAWS RDS\n\n\n\n\n\n\n\n\n\n\n10\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n\n\nAvailable Features\n\u00b6\n\n\nThe MySQL connector supports profiling and masking/tokenization features. Below is a list of which options are and are not available for jobs using the MySQL connector:\n\n\n\n\n\n\n\n\n\n\nFeature\n\n\nAvailability\n\n\n\n\n\n\n\n\n\n\nIn-Place Masking Mode\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams / Threads\n\n\nAvailable\n\n\n\n\n\n\n\n\nBulk Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nBatch Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nDrop Indexes\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nUnavailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nUnavailable\n\n\n\n\n\n\n\n\nIdentity Column Support\n\n\nAvailable\n\n\n\n\n\n\nOn-The-Fly Masking Mode\n\n\nRestart Ability\n\n\nUnavailable\n\n\n\n\n\n\n\n\nTruncate\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nUnavailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nUnavailable\n\n\n\n\n\n\n\n\nCreate Target\n\n\nAvailable\n\n\n\n\n\n\nProfiling\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams\n\n\nAvailable\n\n\n\n\n\n\n\n\nSAP ASE (Sybase) Connector\n\u00b6\n\n\nIntroduction\n\u00b6\n\n\nSAP ASE (Adaptive Server Enterprise), originally known as Sybase SQL\nServer, and also commonly known as Sybase DB or Sybase ASE, is a\nrelational model database server product for businesses developed by\nSybase Corporation which became part of SAP AG.\n\n\nSupport Matrix\n\u00b6\n\n\n\n\n\n\n\n\nVersion\n\n\nLinux\n\n\nUnix\n\n\nWindows\n\n\n\n\n\n\n\n\n\n\n15.03\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n15.5\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n15.7\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n16\n\n\nSupported\n\n\nSupported\n\n\nSupported\n\n\n\n\n\n\n\n\nAvailable Features\n\u00b6\n\n\nThe SAP ASE (Sybase) connector supports profiling and masking/tokenization features. Below is a list of which options are and are not available for jobs using the SAP ASE connector:\n\n\n\n\n\n\n\n\n\n\nFeature\n\n\nAvailability\n\n\n\n\n\n\n\n\n\n\nIn-Place Masking Mode\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams / Threads\n\n\nAvailable\n\n\n\n\n\n\n\n\nBulk Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nBatch Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nDrop Indexes\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nAvailable\n\n\n\n\n\n\n\n\nIdentity Column Support\n\n\nAvailable\n\n\n\n\n\n\nOn-The-Fly Masking Mode\n\n\nRestart Ability\n\n\nAvailable\n\n\n\n\n\n\n\n\nTruncate\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nAvailable\n\n\n\n\n\n\n\n\nCreate Target\n\n\nAvailable\n\n\n\n\n\n\nProfiling\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams\n\n\nAvailable\n\n\n\n\n\n\n\n\nDB2 Z/OS and iSeries Connectors\n\u00b6\n\n\nIntroduction\n\u00b6\n\n\nDB2 for z/OS and iSeries are relational database management systems that run on IBM Z(mainframe) and IBM Power Systems.\n\n\nSupport Matrix\n\u00b6\n\n\n\n\n\n\n\n\nVersion\n\n\nz/OS\n\n\ni-Series\n\n\n\n\n\n\n\n\n\n\n7.1\n\n\nN/A\n\n\nSupported\n\n\n\n\n\n\n7.2\n\n\nN/A\n\n\nSupported\n\n\n\n\n\n\n7.3\n\n\nN/A\n\n\nSupported\n\n\n\n\n\n\n9\n\n\nSupported\n\n\nN/A\n\n\n\n\n\n\n10\n\n\nSupported\n\n\nN/A\n\n\n\n\n\n\n11\n\n\nSupported\n\n\nN/A\n\n\n\n\n\n\nMainframe\n\n\nSupported\n\n\nN/A\n\n\n\n\n\n\n\n\nAvailable Features\n\u00b6\n\n\nThe DB2 for z/OS and iSeries connectors support profiling and masking/tokenization features. Below is a list of which options are and are not available for jobs using the DB2 and z/OS and iSeries connectors:\n\n\n\n\n\n\n\n\n\n\nFeature\n\n\nAvailability\n\n\n\n\n\n\n\n\n\n\nIn-Place Masking Mode\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams / Threads\n\n\nAvailable\n\n\n\n\n\n\n\n\nBulk Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nBatch Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nDrop Indexes\n\n\nUnavailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nUnavailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nUnavailable\n\n\n\n\n\n\n\n\nIdentity Column Support\n\n\nUnavailable\n\n\n\n\n\n\nOn-The-Fly Masking Mode\n\n\nRestart Ability\n\n\nUnavailable\n\n\n\n\n\n\n\n\nTruncate\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nUnavailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nUnavailable\n\n\n\n\n\n\n\n\nCreate Target\n\n\nUnavailable\n\n\n\n\n\n\nProfiling\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams\n\n\nAvailable\n\n\n\n\n\n\n\n\n<<<<<<< HEAD\n\u00b6\n\n\nAWS RDS Oracle Connector\n\u00b6\n\n\nIntroduction\n\u00b6\n\n\nOracle Database is a relational database management system developed by\nOracle. Amazon RDS makes it easy to set up, operate, and scale Oracle\nDatabase deployments in the cloud. With Amazon RDS, you can deploy\nmultiple editions of Oracle Database in minutes with cost-efficient and\nre-sizable hardware capacity.\n\n\nSupport Matrix\n\u00b6\n\n\n\n\n\n\n\n\nVersion\n\n\nSupport Level\n\n\n\n\n\n\n\n\n\n\n11.2.04\n\n\nSupported\n\n\n\n\n\n\n\n\nAvailable Features\n\u00b6\n\n\nThe AWS RDS connector supports profiling and masking/tokenization features. Below is a list of which options are and are not available for jobs using the AWS RDS connector:\n\n\n\n\n\n\n\n\n\n\nFeature\n\n\nAvailability\n\n\n\n\n\n\n\n\n\n\nIn-Place Masking Mode\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams / Threads\n\n\nAvailable\n\n\n\n\n\n\n\n\nBulk Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nBatch Update\n\n\nAvailable\n\n\n\n\n\n\n\n\nDrop Indexes\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nAvailable\n\n\n\n\n\n\nOn-The-Fly Masking Mode\n\n\nRestart Ability\n\n\nAvailable\n\n\n\n\n\n\n\n\nTruncate\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Trigger\n\n\nAvailable\n\n\n\n\n\n\n\n\nDisable Constraint\n\n\nAvailable\n\n\n\n\n\n\nProfiling\n\n\nMulti-Tenant\n\n\nAvailable\n\n\n\n\n\n\n\n\nStreams\n\n\nAvailable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9294c62867d82e5d0a57fb8e84de07aef85628d8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiles Connector\n\u00b6\n\n\nIntroduction\n\u00b6\n\n\nMuch of the time data will live outside of databases. The data can be stored in a variety of different formats including Fixed Width, Delimited, etc.\n\n\nSupport Matrix\n\u00b6\n\n\n\n\n\n\n\n\nFile Type/Format\n\n\nSupport Level\n\n\n\n\n\n\n\n\n\n\nExcel (.xls & .xlsx)\n\n\nSupported\n\n\n\n\n\n\nFixed Width\n\n\nSupported\n\n\n\n\n\n\nDelimited\n\n\nSupported\n\n\n\n\n\n\nXML\n\n\nSupported\n\n\n\n\n\n\nJSON\n\n\nNot Supported",
            "title": "Data Source Support"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#data-source-support",
            "text": "The Delphix Masking service supports profiling, masking, and tokenizing\na variety of different data sources including distributed databases,\nmainframe, PaaS databases, and files. At a high level, Delphix Masking\nbreaks up support for data sources into two categories:    Dedicated Delphix Connectors:  These are data sources that the\n    Delphix Engine can connect to directly using built-in connectors\n    that have been optimized to perform masking, profiling and\n    tokenization.    FEML Sources:  FEML (File Extract Mask and Load) is a method\n    used to mask and tokenize data sources that do not have dedicated\n    Delphix Connectors. FEML uses existing APIs from data sources to\n    extract the data to a file, masks the file, and then uses APIs to\n    load the masked file back into the database.",
            "title": "Data Source Support"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#dedicated-delphix-connectors",
            "text": "The Delphix Engine has dedicated masking connectors for the following\ndata sources:    Distributed Database:  DB2 LUW, Oracle, MS SQL, MySQL, SAP ASE\n    (Sybase), PostgreSQL, MariaDB    Mainframe/Midrange:  DB2 Z/OS, DB2 iSeries, Mainframe    PaaS Database:  AWS RDS Oracle, RDS PostgreSQL, RDS MYSQL, RDS MariaDB,\n    RDS MS SQL Server    Files:  Excel, Fixed Width, Delimited, XML    For a detailed view of all the versions, features, etc Delphix supports\non each data source - see the sections below.",
            "title": "Dedicated Delphix Connectors"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#db2-luw-connector",
            "text": "",
            "title": "DB2 LUW Connector"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#introduction",
            "text": "DB2 for Linux, UNIX and Windows is a database server product developed\nby IBM. Sometimes\ncalled DB2 LUW for brevity, it is part of the DB2 family of database\nproducts. DB2 LUW is the\n\"Common Server\" product member of the DB2 family, designed to run on\nthe most popular\noperating systems. By contrast, all other DB2 products are specific to a\nsingle platform.",
            "title": "Introduction"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#support-matrix",
            "text": "The Delphix DB2 LUW connector supports the following versions of DB2\nLUW:     Version  Linux  Unix  Windows      9.1  Supported  Supported  Supported    9.5  Supported  Supported  Supported    9.7  Supported  Supported  Supported    9.8  Supported  Supported  Supported    10.1  Supported  Supported  Supported    10.5  Supported  Supported  Supported    11.1  Supported  Supported  Supported",
            "title": "Support Matrix"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#available-features",
            "text": "The DB2 LUW connector supports profiling and masking/tokenization features. Below is a list of which options are & are not available for jobs using the DB2 LUW connector:      Feature  Availability      In-Place Masking Mode  Multi-Tenant  Available     Streams / Threads  Available     Bulk Update  Available     Batch Update  Available     Drop Indexes  Available     Disable Trigger  Unavailable     Disable Constraint  Unavailable     Identity Column Support  Unavailable    On-The-Fly Masking Mode  Restart Ability  Available     Truncate  Available     Disable Trigger  Unavailable     Disable Constraint  Unavailable     Create Target  Available    Profiling  Multi-Tenant  Available     Streams  Available",
            "title": "Available Features"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#oracle-connector",
            "text": "",
            "title": "Oracle Connector"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#introduction_1",
            "text": "Oracle Database (commonly referred to as Oracle RDBMS or simply as\nOracle) is a multi-model database management system produced and\nmarketed by Oracle Corporation.",
            "title": "Introduction"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#support-matrix_1",
            "text": "Version  Linux  Unix  Windows  AWS RDS      10g  Supported  Supported  Supported  N/A    11gR1  Supported  Supported  Supported  N/A    11gR2  Supported  Supported  Supported  Supported    12c  Supported  Supported  Supported  Supported    12cR2  Supported  Supported  Supported  N/A    18c  Supported  Supported  Supported  N/A",
            "title": "Support Matrix"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#available-features_1",
            "text": "The Oracle connector supports profiling and masking/tokenization features. Below is a list of which options are and are not available for jobs using the Oracle connector:      Feature  Availability      In-Place Masking Mode  Multi-Tenant  Available     Streams / Threads  Available     Bulk Update  Available     Batch Update  Available     Drop Indexes  Available     Disable Trigger  Available     Disable Constraint  Available     Identity Column Support  Available    On-The-Fly Masking Mode  Restart Ability  Available     Truncate  Available     Disable Trigger  Available     Disable Constraint  Available     Create Target  Available    Profiling  Multi-Tenant  Available     Streams  Available",
            "title": "Available Features"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#ms-sql-connector",
            "text": "",
            "title": "MS SQL Connector"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#introduction_2",
            "text": "Microsoft SQL Server is a relational database management system\ndeveloped by Microsoft. As a database server, it is a software product\nwith the primary function of storing and retrieving data as requested by\nother software applications\u2014which may run either on the same computer or\non another computer across a network (including the Internet).",
            "title": "Introduction"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#support-matrix_2",
            "text": "Version  Linux  Unix  Windows  AWS RDS      2005  N/A  N/A  Supported  N/A    2008  N/A  N/A  Supported  N/A    2008 R2  N/A  N/A  Supported  Supported    2012  N/A  N/A  Supported  Supported    2014  N/A  N/A  Supported  Supported    2016  Supported  N/A  Supported  Supported",
            "title": "Support Matrix"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#available-features_2",
            "text": "The MS SQL connector supports profiling and masking/tokenization features. Below is a list of which options are and are not available for jobs using the MS SQL connector.      Feature  Availability      In-Place Masking Mode  Multi-Tenant  Available     Streams / Threads  Available     Bulk Update  Available     Batch Update  Available     Drop Indexes  Available     Disable Trigger  Available     Disable Constraint  Available     Identity Column Support  Available    On-The-Fly Masking Mode  Restart Ability  Available     Truncate  Available     Disable Trigger  Available     Disable Constraint  Available     Create Target  Available    Profiling  Multi-Tenant  Available     Streams  Available",
            "title": "Available Features"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#postgresql-connector",
            "text": "",
            "title": "PostgreSQL Connector"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#introduction_3",
            "text": "PostgreSQL, often simply Postgres, is an object-relational database management system (ORDBMS) with an emphasis on extensibility and standards compliance. PostgreSQL is developed by the PostgreSQL Global Development Group, a diverse group of many companies and individual contributors. It is free and open-source, released under the terms of the PostgreSQL License, a permissive software license.",
            "title": "Introduction"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#support-matrix_3",
            "text": "Version  Linux  Unix  Windows  AWS RDS      9.2  Supported  Supported  Supported  N/A    9.3  Supported  Supported  Supported  N/A    9.4  Supported  Supported  Supported  Supported    9.5  Supported  Supported  Supported  Supported    9.6  Supported  Supported  Supported  Supported    10  Supported  Supported  Supported  Supported",
            "title": "Support Matrix"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#available-features_3",
            "text": "The PostgreSQL connector supports profiling and masking/tokenization features. Below is a list of which options are & are not available for jobs using the PostgreSQL connector:      Feature  Availability      In-Place Masking Mode  Multi-Tenant  Available     Streams / Threads  Available     Bulk Update  Available     Batch Update  Available     Drop Indexes  Unavailable     Disable Trigger  Unavailable     Disable Constraint  Unavailable     Identity Column Support  Available    On-The-Fly Masking Mode  Restart Ability  Unavailable     Truncate  Available     Disable Trigger  Available     Disable Constraint  Available     Create Target  Available    Profiling  Multi-Tenant  Available     Streams  Unavailable",
            "title": "Available Features"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#mysql-mariadb-connector",
            "text": "",
            "title": "MySQL / MariaDB Connector"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#introduction_4",
            "text": "MySQL is an open-source relational database management system (RDBMS).\nMySQL was owned and sponsored by a single for-profit firm, the Swedish\ncompany MySQL AB. MySQL is now owned by Oracle Corporation.  MariaDB is a community-developed fork of the MySQL relational database\nmanagement system intended to remain free under the GNU GPL. Development\nis led by some of the original developers of MySQL, who forked it due to\nconcerns over its acquisition by Oracle Corporation.  A MySQL Connector may be used to connect to either a MySQL or MariaDB\ndatabase instance.",
            "title": "Introduction"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#mysql-support-matrix",
            "text": "Version  Linux  Unix  Windows  AWS RDS *      5.5  Supported  Supported  Supported  Supported    5.6  Supported  Supported  Supported  Supported    5.7  Supported  Supported  Supported  Supported",
            "title": "MySQL Support Matrix"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#mariadb-support-matrix",
            "text": "Version  Linux  Unix  Windows  AWS RDS      10  Supported  Supported  Supported  Supported",
            "title": "MariaDB Support Matrix"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#available-features_4",
            "text": "The MySQL connector supports profiling and masking/tokenization features. Below is a list of which options are and are not available for jobs using the MySQL connector:      Feature  Availability      In-Place Masking Mode  Multi-Tenant  Available     Streams / Threads  Available     Bulk Update  Available     Batch Update  Available     Drop Indexes  Available     Disable Trigger  Unavailable     Disable Constraint  Unavailable     Identity Column Support  Available    On-The-Fly Masking Mode  Restart Ability  Unavailable     Truncate  Available     Disable Trigger  Unavailable     Disable Constraint  Unavailable     Create Target  Available    Profiling  Multi-Tenant  Available     Streams  Available",
            "title": "Available Features"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#sap-ase-sybase-connector",
            "text": "",
            "title": "SAP ASE (Sybase) Connector"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#introduction_5",
            "text": "SAP ASE (Adaptive Server Enterprise), originally known as Sybase SQL\nServer, and also commonly known as Sybase DB or Sybase ASE, is a\nrelational model database server product for businesses developed by\nSybase Corporation which became part of SAP AG.",
            "title": "Introduction"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#support-matrix_4",
            "text": "Version  Linux  Unix  Windows      15.03  Supported  Supported  Supported    15.5  Supported  Supported  Supported    15.7  Supported  Supported  Supported    16  Supported  Supported  Supported",
            "title": "Support Matrix"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#available-features_5",
            "text": "The SAP ASE (Sybase) connector supports profiling and masking/tokenization features. Below is a list of which options are and are not available for jobs using the SAP ASE connector:      Feature  Availability      In-Place Masking Mode  Multi-Tenant  Available     Streams / Threads  Available     Bulk Update  Available     Batch Update  Available     Drop Indexes  Available     Disable Trigger  Available     Disable Constraint  Available     Identity Column Support  Available    On-The-Fly Masking Mode  Restart Ability  Available     Truncate  Available     Disable Trigger  Available     Disable Constraint  Available     Create Target  Available    Profiling  Multi-Tenant  Available     Streams  Available",
            "title": "Available Features"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#db2-zos-and-iseries-connectors",
            "text": "",
            "title": "DB2 Z/OS and iSeries Connectors"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#introduction_6",
            "text": "DB2 for z/OS and iSeries are relational database management systems that run on IBM Z(mainframe) and IBM Power Systems.",
            "title": "Introduction"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#support-matrix_5",
            "text": "Version  z/OS  i-Series      7.1  N/A  Supported    7.2  N/A  Supported    7.3  N/A  Supported    9  Supported  N/A    10  Supported  N/A    11  Supported  N/A    Mainframe  Supported  N/A",
            "title": "Support Matrix"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#available-features_6",
            "text": "The DB2 for z/OS and iSeries connectors support profiling and masking/tokenization features. Below is a list of which options are and are not available for jobs using the DB2 and z/OS and iSeries connectors:      Feature  Availability      In-Place Masking Mode  Multi-Tenant  Available     Streams / Threads  Available     Bulk Update  Available     Batch Update  Available     Drop Indexes  Unavailable     Disable Trigger  Unavailable     Disable Constraint  Unavailable     Identity Column Support  Unavailable    On-The-Fly Masking Mode  Restart Ability  Unavailable     Truncate  Available     Disable Trigger  Unavailable     Disable Constraint  Unavailable     Create Target  Unavailable    Profiling  Multi-Tenant  Available     Streams  Available",
            "title": "Available Features"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#head",
            "text": "",
            "title": "&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#aws-rds-oracle-connector",
            "text": "",
            "title": "AWS RDS Oracle Connector"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#introduction_7",
            "text": "Oracle Database is a relational database management system developed by\nOracle. Amazon RDS makes it easy to set up, operate, and scale Oracle\nDatabase deployments in the cloud. With Amazon RDS, you can deploy\nmultiple editions of Oracle Database in minutes with cost-efficient and\nre-sizable hardware capacity.",
            "title": "Introduction"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#support-matrix_6",
            "text": "Version  Support Level      11.2.04  Supported",
            "title": "Support Matrix"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#available-features_7",
            "text": "The AWS RDS connector supports profiling and masking/tokenization features. Below is a list of which options are and are not available for jobs using the AWS RDS connector:      Feature  Availability      In-Place Masking Mode  Multi-Tenant  Available     Streams / Threads  Available     Bulk Update  Available     Batch Update  Available     Drop Indexes  Available     Disable Trigger  Available     Disable Constraint  Available    On-The-Fly Masking Mode  Restart Ability  Available     Truncate  Available     Disable Trigger  Available     Disable Constraint  Available    Profiling  Multi-Tenant  Available     Streams  Available            9294c62867d82e5d0a57fb8e84de07aef85628d8",
            "title": "Available Features"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#files-connector",
            "text": "",
            "title": "Files Connector"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#introduction_8",
            "text": "Much of the time data will live outside of databases. The data can be stored in a variety of different formats including Fixed Width, Delimited, etc.",
            "title": "Introduction"
        },
        {
            "location": "/Getting_Started/Data_Source_Support/#support-matrix_7",
            "text": "File Type/Format  Support Level      Excel (.xls & .xlsx)  Supported    Fixed Width  Supported    Delimited  Supported    XML  Supported    JSON  Not Supported",
            "title": "Support Matrix"
        },
        {
            "location": "/Getting_Started/Installation_First_Time_Setup/",
            "text": "Installation/First Time Setup\n\u00b6\n\n\nThis section walks you step by step on how to download and install the\nDelphix Engine software onto your infrastructure (VMware, AWS EC2, or\nAzure).\n\n\nInstalling OVA on VMware\n\u00b6\n\n\nFor detailed recommendations on hardware prerequisites for VMware,\nplease see \nGetting Started - Prerequisites\n. Here are the steps to\ngetting your OVA installed:\n\n\n\n\n\n\nDownload the OVA file from Delphix\u2019s Download site.\n    Note, you will need a support login from your sales team or\n    welcome letter. Navigate to \u201cVirtual Appliance\u201d and download the\n    appropriate OVA. If unsure, use the HWv8_Standard type.\n\n\n\n\n\n\nLogin using the vSphere client to the vSphere server\n    (or vCenter Server) where you want to install the Delphix Engine.\n\n\n\n\n\n\nIn the vSphere Client, click File.\n\n\n\n\n\n\nSelect Deploy OVA Template and then browse to the OVA\n    file. Click Next.\n\n\n\n\n\n\nSelect a hostname for the Delphix Engine. This\n    hostname will be used in configuring the Delphix Engine network.\n\n\n\n\n\n\nSelect the data center where the Delphix Engine will\n    be located.\n\n\n\n\n\n\nSelect the cluster and the ESX host.\n\n\n\n\n\n\nSelect one (1) data store for the Delphix OS. This\n    datastore can be thin-provisioned and must have enough free space\n    to accommodate the 300GB comprising the Delphix operating system.\n\n\n\n\n\n\nThe Delphix VM Configuration Storage requires a minimum of 10GB. The VMFS volume should have enough available space to hold all ESX configuration and log files associated with the Delphix Engine.\n\n\nThe Delphix Engine system disk should be stored in a VMDK system drive. The VMFS volume where the .ova is deployed should therefore have at least 300GB of free space prior to deploying the .ova. The VMFS volume must be located on shared storage in order to use vMotion and HA features. \n\n\n\n\n\n\nSelect the virtual network you want to use. If using\n    multiple physical NICs for link aggregation, you must use vSphere\n    NIC teaming. Do not add multiple virtual NICs to the Delphix\n    Engine itself. The Delphix Engine should use a single virtual\n    network. For more information, see Optimal Network Architecture\n    for the Delphix Engine.\n\n\n\n\n\n\nClick Finish. The installation will begin and the\n    Delphix Engine will be created in the location you specified.\n\n\n\n\n\n\nJump to \u201cActivating the Masking Service\u201d section\n    below to learn how to activate the masking service now that you\n    have the software installed.\n\n\n\n\n\n\nInstalling AMI on AWS EC2\n\u00b6\n\n\nFor detailed recommendations on hardware prerequisites for AWS EC2,\nplease see \nGetting Started - Prerequisites\n. Here are the steps to\ngetting your AMI installed:\n\n\n\n\n\n\nOn the Delphix download site, click the AMI you would\n    like to share and accept the Delphix License agreement.\n    Alternatively, follow a link given by your Delphix solutions\n    architect.\n\n\n\n\n\n\nOn the Amazon Web Services Account Details form\n    presented:\n\n\n\n\n\n\nEnter your AWS Account Identifier, which can be found here:\n    https://console.aws.amazon.com/billing/home?#/account. If you\n    want to use the GovCloud AWS Region, be sure to enter the ID\n    for the AWS Account which has GovCloud enabled.\n\n\n\n\n\n\nSelect which AWS Region you would like the AMI to be shared\n    in. If you would like the AMI shared in a different region,\n    contact your Delphix account representative to make the proper\n    arrangements.\n\n\n\n\n\n\n\n\n\n\nClick Share. The Delphix Engine will appear in your\n    list of AMIs in AWS momentarily.\n\n\n\n\n\n\nReference the Installation and Configuration\n    Requirements for AWS/EC2 when deploying the AMI.\n\n\n\n\n\n\nJump to \u201cActivating the Masking Service\u201d section below\n    to learn how to activate the masking service now that you have the\n    software installed.\n\n\n\n\n\n\nInstalling VHD on AZURE\n\u00b6\n\n\nFor detailed recommendations on hardware prerequisites for Azure, please\nsee \nGetting Started - Prerequisites\n. Here are the steps to getting your\nVHD installed:\n\n\n\n\n\n\nOn the \nMicrosoft Azure\n    Marketplace\n,\n    search for Delphix. Click \nGET IT NOW\n.\n\n\n\n\n\n\nReference the Installation and Configuration\n    Requirements for the Delphix Engine in Azure when deploying the\n    VHD.\n\n\n\n\n\n\nJump to \u201cActivating the Masking Service\u201d section below\n    to learn how to activate the masking service now that you have the\n    software installed.\n\n\n\n\n\n\nActivating the Masking Service\n\u00b6\n\n\nOnce you have installed your Delphix Engine, you will need to activate\nthe masking service through the CLI and then set up your first\nadministrator user.\n\n\nTo activate the Masking service via the CLI, do the following:\n\n\n\n\n\n\nConnect to the CLI via SSH as \nsysadmin\n or with\n    other system administrator credentials.\n\n\n\n\n\n\nStart the Delphix Masking Engine with: system ;\n    startMasking ; commit ; exit\n\n\n\n\n\n\nAccess the UI by navigating to http://<Delphix Engine\n    IP or DNS name>:8282/masking.\n\n\n\n\n\n\nLogin as user \nAdmin\n and password \nAdmin-12\n.\n\n\n\n\n\n\nChange the \nAdmin\n password to a unique value for\n    your installation.\n\n\na.  To change the password, go to the \nAdmin\n tab.\n\n\nb.  Click \nUsers\n.\n\n\nc.  Edit the \nAdmin\n user.\n\n\nd.  Change the \nAdmin\n user's password.\n\n\n\n\n\n\nCongratulations! You are now ready to start using the masking service!",
            "title": "Installation/First Time Setup"
        },
        {
            "location": "/Getting_Started/Installation_First_Time_Setup/#installationfirst-time-setup",
            "text": "This section walks you step by step on how to download and install the\nDelphix Engine software onto your infrastructure (VMware, AWS EC2, or\nAzure).",
            "title": "Installation/First Time Setup"
        },
        {
            "location": "/Getting_Started/Installation_First_Time_Setup/#installing-ova-on-vmware",
            "text": "For detailed recommendations on hardware prerequisites for VMware,\nplease see  Getting Started - Prerequisites . Here are the steps to\ngetting your OVA installed:    Download the OVA file from Delphix\u2019s Download site.\n    Note, you will need a support login from your sales team or\n    welcome letter. Navigate to \u201cVirtual Appliance\u201d and download the\n    appropriate OVA. If unsure, use the HWv8_Standard type.    Login using the vSphere client to the vSphere server\n    (or vCenter Server) where you want to install the Delphix Engine.    In the vSphere Client, click File.    Select Deploy OVA Template and then browse to the OVA\n    file. Click Next.    Select a hostname for the Delphix Engine. This\n    hostname will be used in configuring the Delphix Engine network.    Select the data center where the Delphix Engine will\n    be located.    Select the cluster and the ESX host.    Select one (1) data store for the Delphix OS. This\n    datastore can be thin-provisioned and must have enough free space\n    to accommodate the 300GB comprising the Delphix operating system.    The Delphix VM Configuration Storage requires a minimum of 10GB. The VMFS volume should have enough available space to hold all ESX configuration and log files associated with the Delphix Engine.  The Delphix Engine system disk should be stored in a VMDK system drive. The VMFS volume where the .ova is deployed should therefore have at least 300GB of free space prior to deploying the .ova. The VMFS volume must be located on shared storage in order to use vMotion and HA features.     Select the virtual network you want to use. If using\n    multiple physical NICs for link aggregation, you must use vSphere\n    NIC teaming. Do not add multiple virtual NICs to the Delphix\n    Engine itself. The Delphix Engine should use a single virtual\n    network. For more information, see Optimal Network Architecture\n    for the Delphix Engine.    Click Finish. The installation will begin and the\n    Delphix Engine will be created in the location you specified.    Jump to \u201cActivating the Masking Service\u201d section\n    below to learn how to activate the masking service now that you\n    have the software installed.",
            "title": "Installing OVA on VMware"
        },
        {
            "location": "/Getting_Started/Installation_First_Time_Setup/#installing-ami-on-aws-ec2",
            "text": "For detailed recommendations on hardware prerequisites for AWS EC2,\nplease see  Getting Started - Prerequisites . Here are the steps to\ngetting your AMI installed:    On the Delphix download site, click the AMI you would\n    like to share and accept the Delphix License agreement.\n    Alternatively, follow a link given by your Delphix solutions\n    architect.    On the Amazon Web Services Account Details form\n    presented:    Enter your AWS Account Identifier, which can be found here:\n    https://console.aws.amazon.com/billing/home?#/account. If you\n    want to use the GovCloud AWS Region, be sure to enter the ID\n    for the AWS Account which has GovCloud enabled.    Select which AWS Region you would like the AMI to be shared\n    in. If you would like the AMI shared in a different region,\n    contact your Delphix account representative to make the proper\n    arrangements.      Click Share. The Delphix Engine will appear in your\n    list of AMIs in AWS momentarily.    Reference the Installation and Configuration\n    Requirements for AWS/EC2 when deploying the AMI.    Jump to \u201cActivating the Masking Service\u201d section below\n    to learn how to activate the masking service now that you have the\n    software installed.",
            "title": "Installing AMI on AWS EC2"
        },
        {
            "location": "/Getting_Started/Installation_First_Time_Setup/#installing-vhd-on-azure",
            "text": "For detailed recommendations on hardware prerequisites for Azure, please\nsee  Getting Started - Prerequisites . Here are the steps to getting your\nVHD installed:    On the  Microsoft Azure\n    Marketplace ,\n    search for Delphix. Click  GET IT NOW .    Reference the Installation and Configuration\n    Requirements for the Delphix Engine in Azure when deploying the\n    VHD.    Jump to \u201cActivating the Masking Service\u201d section below\n    to learn how to activate the masking service now that you have the\n    software installed.",
            "title": "Installing VHD on AZURE"
        },
        {
            "location": "/Getting_Started/Installation_First_Time_Setup/#activating-the-masking-service",
            "text": "Once you have installed your Delphix Engine, you will need to activate\nthe masking service through the CLI and then set up your first\nadministrator user.  To activate the Masking service via the CLI, do the following:    Connect to the CLI via SSH as  sysadmin  or with\n    other system administrator credentials.    Start the Delphix Masking Engine with: system ;\n    startMasking ; commit ; exit    Access the UI by navigating to http://<Delphix Engine\n    IP or DNS name>:8282/masking.    Login as user  Admin  and password  Admin-12 .    Change the  Admin  password to a unique value for\n    your installation.  a.  To change the password, go to the  Admin  tab.  b.  Click  Users .  c.  Edit the  Admin  user.  d.  Change the  Admin  user's password.    Congratulations! You are now ready to start using the masking service!",
            "title": "Activating the Masking Service"
        },
        {
            "location": "/Getting_Started/Configuring_Virtualization_Service_for_Masked_Provisioning/",
            "text": "Configuring Virtualization Service for Masked Provisioning\n\u00b6\n\n\nIntroduction\n\u00b6\n\n\nDuring the VDB provisioning process, the Virtualization Engine can optionally run a masking job from the Delphix Masking Engine on the VDB. By default, the Virtualization Engine attempts to obtain a list of masking jobs from a Masking Engine on its localhost. It's possible to split the Virtualization Engine and Masking Engine apart on separate hosts. If the Virtualization Engine and Masking Engine are on different hosts, use these instructions to customize the host address, port number, and/or login credentials that the Virtualization Engine will use to contact the Masking Engine.\n\n\n\n\nImportant Validation Notices\n\n\nWhen using seperate Virtualization and Masking engines, ensure that the versions are compatible. See the \ncompatiblity matrix\n.\n\n\nOld versions of the serviceconfig or any information associated with them are not tracked. In particular, if you have been using the local masking service or a remote service and then change to a new remote service Delphix will start throwing out any old job information on the next masking job/fetch or GUI reload. Users should not rely on that information being preserved through serviceconfig updates.\n\n\nDelphix does not validate network availability between the two engines or any other hosts that both engines might want to communicate with. The state or availability of either host is not checked, if either host becomes unduly slow, congested, or unresponsive Delphix will not be able to issue compelling warnings regarding those issues.\n\n\n\n\nInstructions\n\u00b6\n\n\nIf the Virtualization Engine and Masking Engine are on different hosts, use these instructions to customize the host address, port number, and/or login credentials that the Virtualization Engine will use to contact the Masking Engine.\n\n\n\n\nNote\n\n\nThis does not alter the Delphix Masking Engine UI port. It is specific to coordinating communication between the Virtualization Engine and a Masking Engine about available masking jobs and job results.\n\n\n\n\nTo change the Virtualization Engine's connection details for its Masking Engine:\n\n\n\n\nUsing a shell, login to the \nCLI\n using \ndelphix_admin\n.\n\n\nAt the \nCLI\n root prompt, type \nmaskingjob\n.\n\n\nAt the \nmaskingjob\n prompt, type \nserviceconfig\n.\n\n\nTo list service configurations, type \nls\n.\n\n\nAt the serviceconfig, type select \n`MASKING_SERVICE_CONFIG-1\n.\n\n\nTo view the configurations, type \nls\n.\n\n\nWith this service config selected, enter \nupdate\n.\n\n\nIn the update mode, use the \nset\n command to modify the configuration. For example, type \nset port=[YOUR DESIRED PORT NUMBER]\n to change the port number.\n\n\nCommit the change by typing \ncommit\n.\n\n\nType \nls\n to confirm the configurations.\n\n\nType \nexit\n to exit the CLI.",
            "title": "Configuring Virtualization Service for Masked Provisioning"
        },
        {
            "location": "/Getting_Started/Configuring_Virtualization_Service_for_Masked_Provisioning/#configuring-virtualization-service-for-masked-provisioning",
            "text": "",
            "title": "Configuring Virtualization Service for Masked Provisioning"
        },
        {
            "location": "/Getting_Started/Configuring_Virtualization_Service_for_Masked_Provisioning/#introduction",
            "text": "During the VDB provisioning process, the Virtualization Engine can optionally run a masking job from the Delphix Masking Engine on the VDB. By default, the Virtualization Engine attempts to obtain a list of masking jobs from a Masking Engine on its localhost. It's possible to split the Virtualization Engine and Masking Engine apart on separate hosts. If the Virtualization Engine and Masking Engine are on different hosts, use these instructions to customize the host address, port number, and/or login credentials that the Virtualization Engine will use to contact the Masking Engine.   Important Validation Notices  When using seperate Virtualization and Masking engines, ensure that the versions are compatible. See the  compatiblity matrix .  Old versions of the serviceconfig or any information associated with them are not tracked. In particular, if you have been using the local masking service or a remote service and then change to a new remote service Delphix will start throwing out any old job information on the next masking job/fetch or GUI reload. Users should not rely on that information being preserved through serviceconfig updates.  Delphix does not validate network availability between the two engines or any other hosts that both engines might want to communicate with. The state or availability of either host is not checked, if either host becomes unduly slow, congested, or unresponsive Delphix will not be able to issue compelling warnings regarding those issues.",
            "title": "Introduction"
        },
        {
            "location": "/Getting_Started/Configuring_Virtualization_Service_for_Masked_Provisioning/#instructions",
            "text": "If the Virtualization Engine and Masking Engine are on different hosts, use these instructions to customize the host address, port number, and/or login credentials that the Virtualization Engine will use to contact the Masking Engine.   Note  This does not alter the Delphix Masking Engine UI port. It is specific to coordinating communication between the Virtualization Engine and a Masking Engine about available masking jobs and job results.   To change the Virtualization Engine's connection details for its Masking Engine:   Using a shell, login to the  CLI  using  delphix_admin .  At the  CLI  root prompt, type  maskingjob .  At the  maskingjob  prompt, type  serviceconfig .  To list service configurations, type  ls .  At the serviceconfig, type select  `MASKING_SERVICE_CONFIG-1 .  To view the configurations, type  ls .  With this service config selected, enter  update .  In the update mode, use the  set  command to modify the configuration. For example, type  set port=[YOUR DESIRED PORT NUMBER]  to change the port number.  Commit the change by typing  commit .  Type  ls  to confirm the configurations.  Type  exit  to exit the CLI.",
            "title": "Instructions"
        },
        {
            "location": "/Getting_Started/Users_Roles/",
            "text": "Users and Roles\n\u00b6\n\n\nThe Delphix Masking Service has a flexible and robust users and roles\nsystem that allows you to give users fine grain privileges over what\nenvironments they have access to and what tasks they can and can not\nperform.\n\n\nWhat are Roles?\n\u00b6\n\n\nA defined role is what is used to give a certain user privileges over\ncertain environments and tasks. Roles can be defined by selecting a\nsubset of actions that can be taken on certain objects.\n\n\nActions\n\u00b6\n\n\nWhen defining a role, you can select one or more of the following actions\nfor the role to be able to perform:\n\n\n\n\n\n\nView:\n Be able to view the object and important information\n    about the object.\n\n\n\n\n\n\nAdd:\n Be able to add an instance of an object.\n\n\n\n\n\n\nUpdate:\n Be able to update/edit an instance of an object.\n\n\n\n\n\n\nDelete:\n Be able to delete an instance of an object.\n\n\n\n\n\n\nCopy:\n Be able to create a copy of an object.\n\n\n\n\n\n\nExport:\n Be able to export an object from a Delphix Engine.\n\n\n\n\n\n\nImport:\n Be able to import an exported object into a Delphix Engine\n\n\n\n\n\n\nPlease note that not all of these actions are available for all objects\nin the masking service.\n\n\nObjects\n\u00b6\n\n\nWhen defining a role, permission to perform the above actions can be\ndefined on a per object basis. These objects include:\n\n\n\n\n\n\n\n\nGeneral\n\n\nJobs\n\n\nSettings\n\n\n\n\n\n\n\n\n\n\nEnvironment\n\n\nProfile Job\n\n\nDomains\n\n\n\n\n\n\nConnection\n\n\nMasking Job\n\n\nAlgorithms\n\n\n\n\n\n\nRuleset\n\n\nScheduler\n\n\nProfiler\n\n\n\n\n\n\nInventory\n\n\n\n\nProfile Set\n\n\n\n\n\n\n\n\n\n\nMapping\n\n\n\n\n\n\n\n\n\n\nFile Format\n\n\n\n\n\n\n\n\n\n\nUsers\n\n\n\n\n\n\n\n\nPlease see \nDelphix Masking Terminology\n for definitions of these objects.\n\n\nAdding A Role\n\u00b6\n\n\nTo add a role follow these steps:\n\n\n1.\n Login into the \nMasking Engine\n and select the\n    \nSettings\n tab.\n\n\n2.\n Click the \nAdd Roles\n button.\n\n\n3.\n Enter a \nRole Name\n. The far-left column lists the\n    items for which you can set privileges.\n\n\n4.\n Select the check boxes for the corresponding\n    privileges that you want to apply. If there is no check box, that\n    privilege is not available. For example, if you want this role to\n    have View, Add, Update, and Run privileges for masking jobs,\n    select the corresponding check boxes in the \nMasking Job\n row.\n\n\n5.\n When you are finished assigning privileges for this\n    Role, click \nSubmit\n.\n\n\nRecommended Roles\n\u00b6\n\n\nWhile every organization will differ in what users and roles they\ndefine, we have found that the following roles are common/popular:\n\n\n\n\n\n\nAnalyst role\n \u2014 Can profile data and update inventories (but\n    not create environments or connections)\n\n\n\n\n\n\nDeveloper role\n \u2014 Can create masking jobs and view reports\n\n\n\n\n\n\nOperator role\n \u2014 Can execute jobs (but cannot update\n    inventories)\n\n\n\n\n\n\nApplication owner role\n \u2014 Can define\nconnections\n\n\n\n\n\n\n\n\nNOTE - One Role per User\n\n\nPlease note that each defined user can only have one role assigned to them.\n\n\n\n\nWhat are Users?\n\u00b6\n\n\nOnce you have your roles defined, it is time to create users with those\nroles. We highly recommend creating independent users for each\nindividual who will have access to the masking service.\n\n\nAdding a User\n\u00b6\n\n\nTo create a new user follow these steps:\n\n\n\n\n\n\nLogin into the \nMasking Engine\n and select the\n    \nAdmin\n tab.\n\n\n\n\n\n\nClick \nAdd\n \nUser\n at the upper right of the Users\n    screen.\n\n\n\n\n\n\nYou will be prompted for the following information:\n\n\n\n\n\n\nFirst Name\n \u2014 The user's given name\n\n\n\n\n\n\nLast Name\n \u2014 The user's surname\n\n\n\n\n\n\nUser Name\n \u2014 The login name for the user\n\n\n\n\n\n\nEmail\n \u2014 The user's e-mail address (mailable from the\n    Delphix Masking Engine server for purposes of job completion\n    e-mail messages)\n\n\n\n\n\n\nPassword\n \u2014 The password that the Delphix Masking Engine\n    uses to authenticate the user on the login page. The password\n    must be at least six characters long, and contain a minimum of\n    one uppercase character, one wild character (!@#$%^&*), and\n    one number.\n\n\n\n\n\n\nConfirm Password\n \u2014 Confirm the password with double-entry\n    to avoid data entry error.\n\n\n\n\n\n\nAdministrator\n \u2014 (Optional) Select the Administrator check\n    box if you want to give this user Administrator privileges.\n    (Administrator privileges allow the user to perform all\n    Delphix Masking Engine tasks, including creating and editing\n    users in the Delphix Masking Engine.) If you select the\n    Administrator check box, the Roles and Environments fields\n    disappear because Administrator privileges include all roles\n    and environments.\n\n\n\n\n\n\nRole\n \u2014 Select the role to grant to this user. The choices\n    here depend on the custom roles that you have created. You can\n    assign one role per user name.\n\n\n\n\n\n\nEnvironment\n \u2014 Enter as many environments as this user will\n    be able to access. Granting a user access to a given\n    environment does not give them unlimited access to that\n    environment. The user's access is still limited to their\n    assigned role.\n\n\n\n\n\n\n\n\n\n\nWhen you are finished, click \nSave\n.",
            "title": "Users and Roles"
        },
        {
            "location": "/Getting_Started/Users_Roles/#users-and-roles",
            "text": "The Delphix Masking Service has a flexible and robust users and roles\nsystem that allows you to give users fine grain privileges over what\nenvironments they have access to and what tasks they can and can not\nperform.",
            "title": "Users and Roles"
        },
        {
            "location": "/Getting_Started/Users_Roles/#what-are-roles",
            "text": "A defined role is what is used to give a certain user privileges over\ncertain environments and tasks. Roles can be defined by selecting a\nsubset of actions that can be taken on certain objects.",
            "title": "What are Roles?"
        },
        {
            "location": "/Getting_Started/Users_Roles/#actions",
            "text": "When defining a role, you can select one or more of the following actions\nfor the role to be able to perform:    View:  Be able to view the object and important information\n    about the object.    Add:  Be able to add an instance of an object.    Update:  Be able to update/edit an instance of an object.    Delete:  Be able to delete an instance of an object.    Copy:  Be able to create a copy of an object.    Export:  Be able to export an object from a Delphix Engine.    Import:  Be able to import an exported object into a Delphix Engine    Please note that not all of these actions are available for all objects\nin the masking service.",
            "title": "Actions"
        },
        {
            "location": "/Getting_Started/Users_Roles/#objects",
            "text": "When defining a role, permission to perform the above actions can be\ndefined on a per object basis. These objects include:     General  Jobs  Settings      Environment  Profile Job  Domains    Connection  Masking Job  Algorithms    Ruleset  Scheduler  Profiler    Inventory   Profile Set      Mapping      File Format      Users     Please see  Delphix Masking Terminology  for definitions of these objects.",
            "title": "Objects"
        },
        {
            "location": "/Getting_Started/Users_Roles/#adding-a-role",
            "text": "To add a role follow these steps:  1.  Login into the  Masking Engine  and select the\n     Settings  tab.  2.  Click the  Add Roles  button.  3.  Enter a  Role Name . The far-left column lists the\n    items for which you can set privileges.  4.  Select the check boxes for the corresponding\n    privileges that you want to apply. If there is no check box, that\n    privilege is not available. For example, if you want this role to\n    have View, Add, Update, and Run privileges for masking jobs,\n    select the corresponding check boxes in the  Masking Job  row.  5.  When you are finished assigning privileges for this\n    Role, click  Submit .",
            "title": "Adding A Role"
        },
        {
            "location": "/Getting_Started/Users_Roles/#recommended-roles",
            "text": "While every organization will differ in what users and roles they\ndefine, we have found that the following roles are common/popular:    Analyst role  \u2014 Can profile data and update inventories (but\n    not create environments or connections)    Developer role  \u2014 Can create masking jobs and view reports    Operator role  \u2014 Can execute jobs (but cannot update\n    inventories)    Application owner role  \u2014 Can define\nconnections     NOTE - One Role per User  Please note that each defined user can only have one role assigned to them.",
            "title": "Recommended Roles"
        },
        {
            "location": "/Getting_Started/Users_Roles/#what-are-users",
            "text": "Once you have your roles defined, it is time to create users with those\nroles. We highly recommend creating independent users for each\nindividual who will have access to the masking service.",
            "title": "What are Users?"
        },
        {
            "location": "/Getting_Started/Users_Roles/#adding-a-user",
            "text": "To create a new user follow these steps:    Login into the  Masking Engine  and select the\n     Admin  tab.    Click  Add   User  at the upper right of the Users\n    screen.    You will be prompted for the following information:    First Name  \u2014 The user's given name    Last Name  \u2014 The user's surname    User Name  \u2014 The login name for the user    Email  \u2014 The user's e-mail address (mailable from the\n    Delphix Masking Engine server for purposes of job completion\n    e-mail messages)    Password  \u2014 The password that the Delphix Masking Engine\n    uses to authenticate the user on the login page. The password\n    must be at least six characters long, and contain a minimum of\n    one uppercase character, one wild character (!@#$%^&*), and\n    one number.    Confirm Password  \u2014 Confirm the password with double-entry\n    to avoid data entry error.    Administrator  \u2014 (Optional) Select the Administrator check\n    box if you want to give this user Administrator privileges.\n    (Administrator privileges allow the user to perform all\n    Delphix Masking Engine tasks, including creating and editing\n    users in the Delphix Masking Engine.) If you select the\n    Administrator check box, the Roles and Environments fields\n    disappear because Administrator privileges include all roles\n    and environments.    Role  \u2014 Select the role to grant to this user. The choices\n    here depend on the custom roles that you have created. You can\n    assign one role per user name.    Environment  \u2014 Enter as many environments as this user will\n    be able to access. Granting a user access to a given\n    environment does not give them unlimited access to that\n    environment. The user's access is still limited to their\n    assigned role.      When you are finished, click  Save .",
            "title": "Adding a User"
        },
        {
            "location": "/Getting_Started/Definitions_-_Terms_and_Meanings/",
            "text": "Delphix Masking Terminology\n\u00b6\n\n\nBefore getting started with the Delphix Masking Engine, an overview of\nuniversal terms and concepts will build and unify how different masking\ncomponents come together. The following provides a brief overview of the\nkey concepts within the masking service.\n\n\nHigh Level Concepts\n\u00b6\n\n\nThese concepts are the high level concepts users run into.\n\n\n\n\n\n\n\n\nTerm\n\n\nDefinition\n\n\n\n\n\n\n\n\n\n\nApplication\n\n\nAn Application is a tag that is assigned to one or more environments. We recommend using an application name that is the same as the application associated with the environments.\n\n\n\n\n\n\nConnector\n\n\nConnectors are any set of data (database, file, etc) that have been connected to the Delphix Data Platform. These data sources can be physical or virtualized data sources.\n\n\n\n\n\n\nDomain\n\n\nA domain represents a correlation between various sensitive data categories (social security numbers) and the way it should be secured.\n\n\n\n\n\n\nEnvironment\n\n\nAn environment is a construct that can be used to describe a collection of masking jobs associated with a group of data sources.\n\n\n\n\n\n\nIn-place\n\n\nIn-place masking is 1 of 2 procedures that can be used to apply masking algorithms to a data source. By choosing the In-place option, Delphix will read data from the data source, secure the data in the Engine and then update the data source with the secure data.\n\n\n\n\n\n\nOn-the-fly\n\n\nOn-the-fly masking is the second procedure that can be used to apply masking algorithms to a data source. By choosing the On-the-fly option, Delphix will read data from the data source, secure the data in the Engine and then place the secure data in a target source (different from the location of the original data source).\n\n\n\n\n\n\nInventory\n\n\nAn inventory describes all of the data present in a particular data source and defines the methods which will be used to secure it. Inventories typically include the table name, column name, the data classification, and the chosen algorithm.\n\n\n\n\n\n\nProfile\n\n\nProfiling uses a variety of different methods to classify data in a data source into different categories. These categories are known as domains.\n\n\nThe profile process also assigns recommended algorithms for securing the data based on the the domain.\n\n\n\n\n\n\nRuleset\n\n\nA rule set is group of tables or flat files within a particular data source that a user may choose to run profile, masking, or tokenization jobs on.\n\n\n\n\n\n\n\n\n\nMasking Algorithms\n\u00b6\n\n\nThe following terminology is around the different Algorithms that users\nmay use to secure their data.\n\n\n\n\n\n\n\n\nTerm\n\n\nDefinition\n\n\n\n\n\n\n\n\n\n\nSecure Lookup\n\n\nThe most commonly used type of algorithm. It is easy to generate and works with different languages. When this algorithm replaces real, sensitive data with fictional data, it is possible that it will create repeating data patterns, known as \u201ccollisions.\u201d For example, the name \u201cTom\u201d and \u201cPeter\u201d could both be masked as \u201cMatt.\u201d Because names and addresses naturally recur in real data, this mimics an actual data set. However, if you want the masking engine to mask all data to unique outputs, you should use segmented mapping, described below.\n\n\n\n\n\n\nSegment Mapping\n\n\nProduces no overlap or repetition in the masked data. You can mask up to a maximum of 36 values using segmented mapping. You might use this method if you need columns with unique values, such as Social Security Numbers, primary key columns, or foreign key columns. You can set the algorithm to produce alphanumeric results (letters and numbers) or only numbers.\n\n\n\n\n\n\nMapping\n\n\nAllows you to state what value will replace the original data. There will be no collisions in the masked data, because it always matches the same input to the same output. For example \u201cDavid\u201d will always become \u201cRagu\u201d and \u201cMelissa\u201d will always become \u201cJasmine.\u201d The algorithm checks whether an input has already been mapped; if so, the algorithm changes the data to its designated output. You can use a mapping algorithm on any set of values, of any length, but you must know how many values you plan to mask.\n\n\nNOTE: When you use a mapping algorithm, you cannot mask more than one table at a time. You must mask tables serially.\n\n\n\n\n\n\nBinary Lookup\n\n\nReplaces objects that appear in object columns. For example, if a bank has an object column that stores images of checks, you can use binary lookup algorithm to mask those images. The Delphix Engine cannot change data within images themselves, such as the name on X-rays or driver\u2019s licenses. However, you can replace all such images with a new, fictional image. This fictional image is provided by the owner of the original data.\n\n\n\n\n\n\nTokenization\n\n\nThe only type of algorithm that allows you to reverse its masking. For example, you can use a tokenization algorithm to mask data before you send it to an external vendor for analysis. The vendor can then identify accounts that need attention without having any access to the original, sensitive data. Once you have the vendor\u2019s feedback, you can reverse the masking and take action on the appropriate accounts.\n\n\nLike mapping, a tokenization algorithm creates a unique token for each input such as \u201cDavid\u201d or \u201cMelissa.\u201d The Delphix Engine stores both the token and original so that you can reverse masking later.\n\n\n\n\n\n\nMin Max\n\n\nValues that are extremely high or low in certain categories allow viewers to infer someone\u2019s identity, even if their name has been masked. For example, a salary of $1 suggests a company\u2019s CEO, and some age ranges suggest higher insurance risk. You can use a min max algorithm to move all values of this kind into the midrange.\n\n\n\n\n\n\nData Cleaning\n\n\nDoes not perform any masking. Instead, it standardizes varied spellings, misspellings, and abbreviation for the same name. For example, \u201cAriz,\u201d \u201cAz,\u201d and \u201cArizona\u201d can all be cleaned to \u201cAZ.\u201d\n\n\n\n\n\n\nFree Text Redaction\n\n\nHelps you remove sensitive data that appears in free-text columns such as \u201cNotes.\u201d This type of algorithm requires some expertise to use, because you must set it to recognize sensitive data within a block of text.\n\n\nOne challenge is that individual words might not be sensitive on their own, but together they may be. This algorithm uses profiler sets to determine which information it needs to mask. You can decide which expressions the algorithm uses to search for material such as addresses. For example, you can set the algorithm to look for \u201cSt,\u201d \u201cCir,\u201d \u201cBlvd,\u201d and other words that suggest an address. You can also use pattern matching to identify potential sensitive information. For example, a number that takes the form 123-45-6789 is likely to be a Social Security Number.\n\n\nYou can use free text redaction algorithm to show or hide information by displaying either a \u201cblack list\u201d or a \u201cwhite list.\u201d\n\n\n\n\n\n\n\n\n\nProfile Job Concepts\n\u00b6\n\n\nThe following set of concepts are options available to the user for\nconfiguring a profiling job.\n\n\n\n\n\n\n\n\nTerm\n\n\nDefinition\n\n\n\n\n\n\n\n\n\n\nJob Name\n\n\nA free-form name for the job you are creating. Must be unique.\n\n\n\n\n\n\nMulti-Tenant\n\n\nCheck the box if the job is for a multi-tenant database. This option allows existing rulesets to be-reused to mask identical schemas via different connectors. The connector can be selected at job execution time.\n\n\n\n\n\n\nRule Set\n\n\nSelect a ruleset that this job will execute against.\n\n\n\n\n\n\n\n\nNo. of Streams\n\n\nThe number of parallel streams to use when running the jobs. For example, you can select two streams to run two tables in the ruleset concurrently in the job instead of one table at a time.\n\n\n\n\n\n\nMin Memory (MB) \noptional\n\n\nMinimum amount of memory to allocate for the job, in megabytes.\n\n\n\n\n\n\nMax Memory (MB) \noptional\n\n\nMaximum amount of memory to allocate for the job, in megabytes.\n\n\n\n\n\n\nFeedback Size \noptional\n\n\nThe number of rows to process before writing a message to the log. Set this parameter to the appropriate level of detail required for monitoring your job. For example, if you set this number significantly higher than the actual number of rows in a job, the progress for that job will only show 0 or 100%\n\n\n\n\n\n\nProfile Sets \noptional\n\n\nThe name of a profile set, which is a subset of expressions (for example, a subset of financial expressions).\n\n\n\n\n\n\n\n\nComments \noptional\n\n\nAdd comments related to this job.\n\n\n\n\n\n\nEmail \noptional\n\n\nAdd email address(es) to which to send status messages. Separate addresses with a comma (,).\n\n\n\n\n\n\n\n\n\nMasking Job Concepts\n\u00b6\n\n\nThese concepts are options available to the user for configuring a\nmasking job.\n\n\n\n\n\n\n\n\nTerm\n\n\nDefinition\n\n\n\n\n\n\n\n\n\n\nJob Name\n\n\nA free-form name for the job you are creating. Must be unique across the entire application.\n\n\n\n\n\n\nMasking Method\n\n\nSelect either In-Place or On-The-Fly.\n\n\n\n\n\n\nMulti-Tenant\n\n\nCheck box if the job is for a multi-tenant database.\n\n\n\n\n\n\nRule Set\n\n\nSelect a ruleset for this job to execute against.\n\n\n\n\n\n\nMasking Method\n\n\nSelect either In-place or On-the-fly.\n\n\n\n\n\n\nMin Memory (MB) optional\n\n\nMinimum amount of memory to allocate for the job, in megabytes.\n\n\n\n\n\n\nMax Memory (MB) optional\n\n\nMaximum amount of memory to allocate for the job, in megabytes.\n\n\n\n\n\n\nUpdate Threads\n\n\nThe number of update threads to run in parallel to update the target database.\n\n\nFor database using T-SQL, multiple update/insert threads can cause deadlock. If you see this type of error, reduce the number of threads that you specify in this box.\n\n\n\n\n\n\nCommit Size\n\n\nThe number of rows to process before issuing a commit to the database.\n\n\n\n\n\n\nFeedback Size\n\n\nThe number of rows to process before writing a message to the logs. Set this parameter to the appropriate level of detail required for monitoring your job. For example, if you set this number significantly higher than the actual number of rows in a job, the progress that job will show 0% or 100%.\n\n\n\n\n\n\nBulk Data \noptional\n\n\nFor In-Place masking only. The default is for this check box to be clear. If you are masking very large tables in-place and require performance improvements, check this box. Delphix will mask data to a flat file, and then use inserts instead of updates to bulk load the target table.\n\n\n\n\n\n\nDisable Trigger \noptional\n\n\nWhether to automatically disable database triggers. The default is for this check box to be clear and therefore not perform automatic disabling of triggers.\n\n\n\n\n\n\nDrop Index \noptional\n\n\nWhether to automatically drop indexes on columns which are being masked and automatically re-create the index when the masking job is completed. The default is for this check box to be clear and therefore not perform automatic dropping of indexes.\n\n\n\n\n\n\nPrescript \noptional\n\n\nSpecify the full pathname of a file that contains SQL statements to run before the job starts, or click Browse to specify a file. If you are editing the job and a pre script file is already specified, you can click the Delete button to remove the file. (The Delete button only appears if a prescript file was already specified.) For information about creating your own prescript files, see Create SQL Statements to Run Before and After Jobs.\n\n\n\n\n\n\nPostscript \noptional\n\n\nSpecify the full pathname of a file that contains SQL statements to be run after the job finishes, or click Browse to specify a file. If you are editing the job and a postscript file is already specified, you can click the Delete button to remove the file. (The Delete button only appears if a postscript file was already specified.) For information about creating your own postscript file, see Creating SQL Statement to Run Before and After Jobs.\n\n\n\n\n\n\nComments \noptional\n\n\nAdd comments related to this masking job.\n\n\n\n\n\n\nEmail \noptional\n\n\nAdd email address(es) to which to send status messages.",
            "title": "Delphix Masking Terminology"
        },
        {
            "location": "/Getting_Started/Definitions_-_Terms_and_Meanings/#delphix-masking-terminology",
            "text": "Before getting started with the Delphix Masking Engine, an overview of\nuniversal terms and concepts will build and unify how different masking\ncomponents come together. The following provides a brief overview of the\nkey concepts within the masking service.",
            "title": "Delphix Masking Terminology"
        },
        {
            "location": "/Getting_Started/Definitions_-_Terms_and_Meanings/#high-level-concepts",
            "text": "These concepts are the high level concepts users run into.     Term  Definition      Application  An Application is a tag that is assigned to one or more environments. We recommend using an application name that is the same as the application associated with the environments.    Connector  Connectors are any set of data (database, file, etc) that have been connected to the Delphix Data Platform. These data sources can be physical or virtualized data sources.    Domain  A domain represents a correlation between various sensitive data categories (social security numbers) and the way it should be secured.    Environment  An environment is a construct that can be used to describe a collection of masking jobs associated with a group of data sources.    In-place  In-place masking is 1 of 2 procedures that can be used to apply masking algorithms to a data source. By choosing the In-place option, Delphix will read data from the data source, secure the data in the Engine and then update the data source with the secure data.    On-the-fly  On-the-fly masking is the second procedure that can be used to apply masking algorithms to a data source. By choosing the On-the-fly option, Delphix will read data from the data source, secure the data in the Engine and then place the secure data in a target source (different from the location of the original data source).    Inventory  An inventory describes all of the data present in a particular data source and defines the methods which will be used to secure it. Inventories typically include the table name, column name, the data classification, and the chosen algorithm.    Profile  Profiling uses a variety of different methods to classify data in a data source into different categories. These categories are known as domains.  The profile process also assigns recommended algorithms for securing the data based on the the domain.    Ruleset  A rule set is group of tables or flat files within a particular data source that a user may choose to run profile, masking, or tokenization jobs on.",
            "title": "High Level Concepts"
        },
        {
            "location": "/Getting_Started/Definitions_-_Terms_and_Meanings/#masking-algorithms",
            "text": "The following terminology is around the different Algorithms that users\nmay use to secure their data.     Term  Definition      Secure Lookup  The most commonly used type of algorithm. It is easy to generate and works with different languages. When this algorithm replaces real, sensitive data with fictional data, it is possible that it will create repeating data patterns, known as \u201ccollisions.\u201d For example, the name \u201cTom\u201d and \u201cPeter\u201d could both be masked as \u201cMatt.\u201d Because names and addresses naturally recur in real data, this mimics an actual data set. However, if you want the masking engine to mask all data to unique outputs, you should use segmented mapping, described below.    Segment Mapping  Produces no overlap or repetition in the masked data. You can mask up to a maximum of 36 values using segmented mapping. You might use this method if you need columns with unique values, such as Social Security Numbers, primary key columns, or foreign key columns. You can set the algorithm to produce alphanumeric results (letters and numbers) or only numbers.    Mapping  Allows you to state what value will replace the original data. There will be no collisions in the masked data, because it always matches the same input to the same output. For example \u201cDavid\u201d will always become \u201cRagu\u201d and \u201cMelissa\u201d will always become \u201cJasmine.\u201d The algorithm checks whether an input has already been mapped; if so, the algorithm changes the data to its designated output. You can use a mapping algorithm on any set of values, of any length, but you must know how many values you plan to mask.  NOTE: When you use a mapping algorithm, you cannot mask more than one table at a time. You must mask tables serially.    Binary Lookup  Replaces objects that appear in object columns. For example, if a bank has an object column that stores images of checks, you can use binary lookup algorithm to mask those images. The Delphix Engine cannot change data within images themselves, such as the name on X-rays or driver\u2019s licenses. However, you can replace all such images with a new, fictional image. This fictional image is provided by the owner of the original data.    Tokenization  The only type of algorithm that allows you to reverse its masking. For example, you can use a tokenization algorithm to mask data before you send it to an external vendor for analysis. The vendor can then identify accounts that need attention without having any access to the original, sensitive data. Once you have the vendor\u2019s feedback, you can reverse the masking and take action on the appropriate accounts.  Like mapping, a tokenization algorithm creates a unique token for each input such as \u201cDavid\u201d or \u201cMelissa.\u201d The Delphix Engine stores both the token and original so that you can reverse masking later.    Min Max  Values that are extremely high or low in certain categories allow viewers to infer someone\u2019s identity, even if their name has been masked. For example, a salary of $1 suggests a company\u2019s CEO, and some age ranges suggest higher insurance risk. You can use a min max algorithm to move all values of this kind into the midrange.    Data Cleaning  Does not perform any masking. Instead, it standardizes varied spellings, misspellings, and abbreviation for the same name. For example, \u201cAriz,\u201d \u201cAz,\u201d and \u201cArizona\u201d can all be cleaned to \u201cAZ.\u201d    Free Text Redaction  Helps you remove sensitive data that appears in free-text columns such as \u201cNotes.\u201d This type of algorithm requires some expertise to use, because you must set it to recognize sensitive data within a block of text.  One challenge is that individual words might not be sensitive on their own, but together they may be. This algorithm uses profiler sets to determine which information it needs to mask. You can decide which expressions the algorithm uses to search for material such as addresses. For example, you can set the algorithm to look for \u201cSt,\u201d \u201cCir,\u201d \u201cBlvd,\u201d and other words that suggest an address. You can also use pattern matching to identify potential sensitive information. For example, a number that takes the form 123-45-6789 is likely to be a Social Security Number.  You can use free text redaction algorithm to show or hide information by displaying either a \u201cblack list\u201d or a \u201cwhite list.\u201d",
            "title": "Masking Algorithms"
        },
        {
            "location": "/Getting_Started/Definitions_-_Terms_and_Meanings/#profile-job-concepts",
            "text": "The following set of concepts are options available to the user for\nconfiguring a profiling job.     Term  Definition      Job Name  A free-form name for the job you are creating. Must be unique.    Multi-Tenant  Check the box if the job is for a multi-tenant database. This option allows existing rulesets to be-reused to mask identical schemas via different connectors. The connector can be selected at job execution time.    Rule Set  Select a ruleset that this job will execute against.     No. of Streams  The number of parallel streams to use when running the jobs. For example, you can select two streams to run two tables in the ruleset concurrently in the job instead of one table at a time.    Min Memory (MB)  optional  Minimum amount of memory to allocate for the job, in megabytes.    Max Memory (MB)  optional  Maximum amount of memory to allocate for the job, in megabytes.    Feedback Size  optional  The number of rows to process before writing a message to the log. Set this parameter to the appropriate level of detail required for monitoring your job. For example, if you set this number significantly higher than the actual number of rows in a job, the progress for that job will only show 0 or 100%    Profile Sets  optional  The name of a profile set, which is a subset of expressions (for example, a subset of financial expressions).     Comments  optional  Add comments related to this job.    Email  optional  Add email address(es) to which to send status messages. Separate addresses with a comma (,).",
            "title": "Profile Job Concepts"
        },
        {
            "location": "/Getting_Started/Definitions_-_Terms_and_Meanings/#masking-job-concepts",
            "text": "These concepts are options available to the user for configuring a\nmasking job.     Term  Definition      Job Name  A free-form name for the job you are creating. Must be unique across the entire application.    Masking Method  Select either In-Place or On-The-Fly.    Multi-Tenant  Check box if the job is for a multi-tenant database.    Rule Set  Select a ruleset for this job to execute against.    Masking Method  Select either In-place or On-the-fly.    Min Memory (MB) optional  Minimum amount of memory to allocate for the job, in megabytes.    Max Memory (MB) optional  Maximum amount of memory to allocate for the job, in megabytes.    Update Threads  The number of update threads to run in parallel to update the target database.  For database using T-SQL, multiple update/insert threads can cause deadlock. If you see this type of error, reduce the number of threads that you specify in this box.    Commit Size  The number of rows to process before issuing a commit to the database.    Feedback Size  The number of rows to process before writing a message to the logs. Set this parameter to the appropriate level of detail required for monitoring your job. For example, if you set this number significantly higher than the actual number of rows in a job, the progress that job will show 0% or 100%.    Bulk Data  optional  For In-Place masking only. The default is for this check box to be clear. If you are masking very large tables in-place and require performance improvements, check this box. Delphix will mask data to a flat file, and then use inserts instead of updates to bulk load the target table.    Disable Trigger  optional  Whether to automatically disable database triggers. The default is for this check box to be clear and therefore not perform automatic disabling of triggers.    Drop Index  optional  Whether to automatically drop indexes on columns which are being masked and automatically re-create the index when the masking job is completed. The default is for this check box to be clear and therefore not perform automatic dropping of indexes.    Prescript  optional  Specify the full pathname of a file that contains SQL statements to run before the job starts, or click Browse to specify a file. If you are editing the job and a pre script file is already specified, you can click the Delete button to remove the file. (The Delete button only appears if a prescript file was already specified.) For information about creating your own prescript files, see Create SQL Statements to Run Before and After Jobs.    Postscript  optional  Specify the full pathname of a file that contains SQL statements to be run after the job finishes, or click Browse to specify a file. If you are editing the job and a postscript file is already specified, you can click the Delete button to remove the file. (The Delete button only appears if a postscript file was already specified.) For information about creating your own postscript file, see Creating SQL Statement to Run Before and After Jobs.    Comments  optional  Add comments related to this masking job.    Email  optional  Add email address(es) to which to send status messages.",
            "title": "Masking Job Concepts"
        },
        {
            "location": "/Getting_Started/Kerberos_Configuration/",
            "text": "Kerberos Configuration\n\u00b6\n\n\nIntroduction\n\u00b6\n\n\nAs of 5.3.0.0, the Delphix Masking Engine supports Kerberos authentication for\nOracle, MS SQL Server, and Sybase connections. Utilizing this service requires\nthe presence of a Kerberos Key Distribution Center (KDC) server as well as\nadditional configuration actions to be done on both the the Masking Engine and\nthe database. This document presents configuration instructions for enabling\nand using Kerberos on the Delphix Masking Engine, as well as reference\nconfigurations for enabling Kerberos on the Databases. Although other\nconfigurations are possible, the configurations in this document have been\nvalidated by Delphix.\n\n\nTerminology\n\u00b6\n\n\nThroughout this document, the following example values are used. To recreate\nthese reference environments, these values must be replaced with real values\nappropriate for your network environment:\n\n\n\n\n.bar.com - the DNS domain of then network\n\n\nBAR.COM - the Kerberos domain\n\n\nme-host - the hostname of the masking engine\n\n\nfoo-kcd - the hostname KDC server\n\n\nkrbuser - the kerberos principal to be granted access to the database for masking\n\n\n\n\nConfiguring Kerberos on the Appliance\n\u00b6\n\n\nThis section details the steps required to configure Kerberos on your appliance.\n\n\nStep 1\n On the Delphix System Setup CLI, enable the Kerberos feature.\n\n\nNote\n\nYou may see a warning indicating that special permission is required to enable Kerberos.\nThis warning can be ignored when enabling Kerberos for use with Masking only.\nIn the following examples, \nme-hosts\n is the hostname of your masking engine.\n\n\n$ ssh sysadmin@me-host.bar.com\nme-host> system\nme-host system> enableFeatureFlag\nme-host system enableFeatureFlag *> set name=KERBEROS\nme-host system enableFeatureFlag *> commit\nme-host system> exit\n\n\n\n\n\nStep 2\n On the Delphix System Setup CLI, configure and enable Kerberos.\n\n\n$ ssh sysadmin@me-host.bar.com\nme-host service> kerberos\nme-host service kerberos> update\nme-host service kerberos update *> set name=Kerberos_Conf\nme-host service kerberos update *> edit kdcs\nme-host service kerberos update kdcs *> edit 0\nme-host service kerberos update kdcs 0 *> set hostname=foo-kcd.bar.com\nme-host service kerberos update kdcs *> back\nme-host service kerberos update *> set realm=BAR.COM\nme-host service kerberos update *> set principal=krbuser\nme-host service kerberos update *> set keytab=_krbuser_keytab_base64_\nme-host service kerberos update *> commit\n\n\n\n\n\nIn this case, \nkrbuser_keytab_base64\n is the base64 encoded contents of the\n keytab file for krbuser. The kerberos keytab for a user is typically available\nfrom your kerberos administrator.\n\n\nTo display a keytab file in base64 encoding use:\n\n\n$ base64 ~/krbuser.keytab\n\n\nStep 2\n Alternatively - On the Delphix Server Setup UI configure and enable\nKerberos:\n\n\na. From the Preferences menu select Kerberos Configuration.\n\n\n\n\nb. Add record(s) for your KDCs, and populate other fields appropriately for your\n   network environment. Upon pressing \nSave\n, your configuration will be tested.\n   If the engine is able to authenticate to the KDC with the supplied\n   configuration, the configuration is applied immediately.\n\n\n\n\nCreating Maskings Database Connectors using Kerberos\n\u00b6\n\n\nOnce the Delphix Appliance is configured for Kerberos, creating Connectors\nusing Kerberos authentication is simple:\n\n\n\n\nAssuming you are using the same user principal configured in Server Setup, the\nkeytab will be used and it is unnecessary to enter a password in the Connector\ndefinition.\n\n\nFor Sybase database Connectors, it is necessary to supply the service principal\nname as an additional configuration item. For Oracle DB, this value is\ndetermined automatically.  For MS SQL Server it is determined based on the\nreverse DNS mapping of the Server Name (refer to the section on MS SQL\nServer below).\n\n\nReference Database Configurations\n\u00b6\n\n\nThe following are a series of reference kerberos configuration procedures and\ntroubleshooting notes for the supported databases. These are meant to serve as\nexamples to be further customized according to the user's specific network\nenvironment and security needs.\n\n\nOracle Database\n\u00b6\n\n\nOverview\n\n\nThis document describes how to set up an Oracle DB instance for kerberized\nconnections. The following steps are described:\n\n\n\n\nCreating a service principal and adding it to the DB system\n\n\nConfiguring the database to use kerberos authentication\n\n\nCreating DB users identified via kerberos\n\n\nTroubleshooting tips\n\n\n\n\nPrerequisites\n\n\nThis document assumes you already have a kerberized network environment with\nan MIT Kerberos KDC. These procedures have been tested successfully with Oracle\ndatabase versions 11.2.0.2, 11.2.0.4 and 12.2.1. Oracle database version\n12.1.0.1 did not work in our testing.\n\n\nYou will need the following from your kerberos environment:\n\n\n\n\nThe krb5.conf file\n\n\nA user principal and associated password or keytab you'd like to use to log\n  into the database\n\n\nThe ability to create a service principal for the Oracle DB and retrieve the associated keytab\n\n\n\n\nThis section of the document uses these example values in addition to those\nmentioned above:\n\n\n\n\nThe oracle database is: ora-db.bar.com.\n\n\nThe oracle service name is: oracle\n\n\n\n\nCreating the Oracle Service Principal\n\n\nThe service principal will be named: \n/\n@\n\n\nGiven our default values above, this works out to: oracle/ora-db@bar.com\n\n\nNotice that the hostname is whatever the database system thinks its hostname\nis - that is, the output of \"uname -n\" on the database system, rather than the\nactual DNS name of the database system. Typically, these values would be the\nsame, but this is not always the case.\n\n\nOn the KDC, run:\n\n\n# kadmin.local\nkadmin.local: addprinc -randkey oracle/ora-db@bar.com\nkadmin.local: ktadd -norandkey -k /var/tmp/ora-db.keytab oracle/ora-db@bar.com\n\n\n\n\n\nCopy the resulting keytab file (/var/tmp/ora-db.keytab) to the Oracle DB system\nat this location:  /etc/v5srvtab\n\n\nAs root on the Oracle DB system, ensure that the keytab has the correct\npermissions:\n\n\n# chown root:oinstall /etc/v5srvtab\n# chmod 440 /etc/v5srvtab\n\n\n\n\n\nFinally, this is a good opportunity to copy /etc/krb5.conf from the KDC to\n/etc/krb5.conf on the Oracle DB system. This file should be readable by all users.\n\n\nConfiguring the Oracle Database for Kerberos\n\n\nLog into the Oracle DB system as the appropriate use for the database in question.\n\n\n$ cd $ORACLE_HOME\n$ vi network/admin/sqlnet.ora\n\n\n\n\n\nAdd the following for Oracle 11:\n\n\nSQLNET.KERBEROS5_CONF=/etc/krb5.conf\nSQLNET.AUTHENTICATION_SERVICES=(BEQ,KERBEROS5)\nSQLNET.KERBEROS5_CONF_MIT=true\nSQLNET.AUTHENTICATION_KERBEROS5_SERVICE=oracle\n\n\n\n\n\nOr the following for Oracle 12:\n\n\nNAMES.DIRECTORY_PATH=(TNSNAMES, EZCONNECT, HOSTNAME)\nSQLNET.KERBEROS5_CONF=/etc/krb5.conf\nSQLNET.AUTHENTICATION_SERVICES=(BEQ,KERBEROS5PRE,KERBEROS5)\nSQLNET.KERBEROS5_CONF_MIT=true\nSQLNET.AUTHENTICATION_KERBEROS5_SERVICE=oracle\n\n\n\n\n\nIf the database is Oracle 11 (not necessary on Oracle 12):\n\n\n$ vi dbs/init.ora\n\n\nAdd this line at the end: \nOS_AUTHENT_PREFIX=\"\"\n\n\nCreating a DB User Identified via Kerberos\n\n\nLog into the Oracle DB system as the appropriate database user and open a\ndatabase session as the DBA:\n\n\n$ sqlplus / as sysdba\n\n\nOn Oracle 12, you may wish to alter your session to create the user in one of\nthe PDBs:\n\n\nSQL> alter session set container=MYPDB;\n\n\nCreate the user that will connect to the DB using kerberos:\n\n\nSQL> create user krbdbuser identified externally as 'krbuser@BAR.COM';\n\n\nGrant the user privileges necessary for masking.\n\n\nThis example grants all privileges for the sake of simplicity:\n\n\nOracle 11:\n\n\nSQL> grant all privilege to krbdbuser;\n\n\nOracle 12: (Customize permissions as necessary for your environment).\n\n\nSQL> grant connect,resource to krbdbuser;\nSQL> grant create tablespace, drop tablespace to krbdbuser;\nSQL> grant create table to krbdbuser;\nSQL> grant create sequence to krbdbuser;\nSQL> grant select_catalog_role to krbdbuser;\nSQL> grant unlimited tablespace to krbdbuser;\nSQL> grant select_catalog_role to krbdbuser;\nSQL> grant alter system to krbdbuser;\nSQL> grant sysoper to krbdbuser;\nSQL> grant dba to krbdbuser;\n\n\n\n\n\nTroubleshooting Tips\n\n\n\n\nConnecting via JDBC with kerberos authentication from Delphix Masking involves two steps: a kerberos login, followed by JDBC connect. A failure stack with an error in the login function indicates a misconfiguration on either the engine or KDC - the engine hasn't even attempted to communicate with the database at that point. Failure stacks are saved in the debugging log for masking.\n\n\nLogin exceptions that mention a checksum error mean either the password or keytab supplied doesn't match the expected password/key on the KDC for the principal you're trying to use. Server Setup verifies that your keytab works at configuration time, but it could stop working if the key for your principal is updated on the KDC.\n\n\nPrior to version 12, Oracle databases instances assume they can create/write a particular temporary file to store kerberos credentials for the DB. This means if you attempt to run multiple kerberized instances of Oracle 11 on the same system or VM, and the databases run as different system users, the first Oracle instance that performs kerberos auth will create and own this file. Kerberos authentication will fail to function on all other instances.\n\n\n\n\nMS SQL Server\n\u00b6\n\n\nOverview\n\n\nThis is an overview of the step necessary to get your masking engine talking to\na MS SQL Server database using kerberos authentication. Since Active Directory\nalready uses Kerberos for authentication, little or no additional configuration\nis need on the MS SQL Database server.\n\n\nThe following steps are described in this section:\n\n\n\n\nCreate the necessary SPNs (Service Principal Names) for your MSSQL Database service in AD\n\n\nCreate the DB Connector on the masking engine\n\n\nCreating a keytab for an AD User\n\n\nTroubleshooting tips\n\n\n\n\nPrerequisites\n\n\nConfiguring cross-realm trust between Active Directory and an MIT KDC Server is\na complex topic, and will not be described here. In the absence of such a setup,\nit is possible to make the Delphix Appliance a kerberos client of the Active\nDirectory (AD) Server. In this configuration, no additional KDC in necessary.\nThe example below assumes this kind of configuration.\n\n\nThis section of the document uses these example values in addition to or instead\nof those mentioned above:\n\n\n\n\nThe MSSQL server database is named mssql-db.bar.com.\n\n\nThe AD user configured for masking access to the MSSQL database is aduser (rather than krbuser in other examples elsewhere in this document).\n\n\nThe AD user that start the MS SQL Server service on the DB Server is dbuser.\n\n\n\n\nCreating SPNs for the Database Service\n\n\nMS SQL Server service will typically register several SPNs with AD upon startup.\nHowever, there are several conditions which can cause these SPNs to not be\nregistered successfully, or to be registered with service names other than those\nthat are expected by the jTDS JDBC driver employed by Delphix Masking.\n\n\nThe service principal name for an MS SQL Server expected by Delphix Masking is: MSSQLSvc/\n:\n\n\nFor example, the SPN for our example MS SQL Server would be:\n\n\nMSSQLSvc/mssql-db.bar.com:1433\n\n\nIn addition, it is \nrequired\n that a reverse mapping exist in DNS from the IP\naddress of the MS SQL Server system to the FQDN registered.\n\n\nThe following commands may be run in powershell on the MS SQL Server to assist\nin debugging SPN related issues:\n\n\nList all SPNs for dbuser:\n\n\nsetspn -L -U dbuser\n\n\nDeleting an old SPN associated with dbuser:\n\n\nsetspn -U -D MSSQLSvc/other-server.ad.bar.com:SQL2008R2 dbuser\n\n\nHere's how to create the SPN describe above:\n\n\nsetspn -U -S MSSQLSvc/mssql-db.bar.com:1433 dbuser\n\n\nCreating the Database Connector on the Masking Engine\n\n\nOnce the above steps are complete, creating the database connector can be\nperformed using the procedure above. Enter the username and optionally, password\nof the AD user in the Connector definition. Be sure that the AD user has the\nsufficient access to the MS SQL Database for masking.\n\n\nThe password field can be left blank when creating the connector if the user\nis the same user configured in Server Setup for the appliance. Since keytabs\nare not typically used in an AD environment, it may be useful to create one\nmanually, to avoid having a password in the DB Connector.\n\n\nCreating a keytab file for an AD user\n\n\nOn a unix or MAC system with MIT kerberos CLI utilities installed:\n\n\n# ktutil\nktutil: addent -password -p krbuser -k 1 -e arcfour-hmac\n<type password for krbuser>\nktutil: addent -password -p krbuser -k 1 -e aes128-cts-hmac-sha1-96\n<type password for krbuser>\nktutil: addent -password -p krbuser -k 1 -e aes256-cts-hmac-sha1-96\n<type password for krbuser>\nktutil: write_kt /var/tmp/krbuser.keytab\nktutil: exit\n# base64 /var/tmp/krbuser.keytab ;# This is string to user for keytab in Server Setup kerberos configuration\n\n\n\n\n\n\n\nNote\n\n\nkvno doesn't matter when using kerberos keytabs with AD. The password must match the active password for the AD user in question.\n\n\n\n\nTroubleshooting Tips\n\n\nThe client uses the incorrect service name\nThis will typically manifest an exception mentioning cred, like:\n\n\nCaused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Fail to create credential. (63) - No service creds)\n  at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:770)\n  at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248)\n  at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)\n  at com.microsoft.sqlserver.jdbc.KerbAuthentication.intAuthHandShake(KerbAuthentication.java:163)\n        ... 101 common frames omitted\nCaused by: sun.security.krb5.internal.KrbApErrException: Fail to create credential. (63) - No service creds\n  at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:162)\n  at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:458)\n   at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:693)\n        ... 104 common frames omitted\n\n\n\n\n\nWhy might this happen:\n\n\n\n\n\n\nYou're using the JTDS JDBC driver, and your MSSQL Server's IP address doesn't\n have a reverse mapping in DNS. In this case, the driver may construct a service\n name like: MSSQLSvc/\n:\n and try to use that.  Either correct DNS to have a valid reverse mapping for the IP of your SQL server, or manually add an SPN to active directory for the name the JDBC client is trying to use:\n\n\n\n\nDetermine the user that starts MSSQL Server on your DB machine.\n\n\n\n\nFrom powershell, do: setspn -AU MSSQLSvc/\n:1433 \n\n\nExample: setspn -AU MSSQLSvc/10.43.100.101:1433 AD\\dbuser\n\n\n\n\n\n\n\n\n\n\nThe database server has multiple DNS names (FQDNs). In this case, SPNs may be\nregistered only for some of them. It may be necessary to add SPNs for the other\nFQDNs as above.\n\n\n\n\nThe MS SQL Server didn't automatically register an SPN. There is a limit (in\nthe thousands) to the number of SPNs that may be registered for a given AD user.\nIt is quite possible to hit this limit in an environment where many MS SQL Server\nVMs are actively created and destroyed with the same configuration.\n\n\n\n\n\n\nNote\n\n\nIn Active Directory, setspn isn't creating a service principal with distinct key as is typical for services on MIT KDCs - rather it's mapping the service principal to the key for the AD user in question.\n\n\n\n\nThe SPN for the SQL Server is registered to the incorrect AD account\n\n\nManifests as an exception with this text:  GSS failure: Defective token detected\n(Mechanism level: AP_REP token id does not match!)\n\n\nResolution: From powershell on the MS SQL Server:\n\n\nPS> setspn -Q <SPN>\n\n\nThis will show what user has the SPN registered.\n\n\nPS> setspn -U -D <SPN> <WRONG_ACCT>\n\n\nThis will unregister the SPN from that user\n\n\nPS> setspn -AU <SPN> <CORRECT_ACCT>\n\n\nSybase\n\u00b6\n\n\nCreating a principal and corresponding keytab on the KDC\n\n\n\n\nSSH into the KDC as the user with sufficient privileges to run kadmin.local\n\n\nRun the kerberos configuration CLI with kadmin.local\n\n\n\n\nAdd a new principal you want to authenticate as later with:\n   \nadd_principal <principalName>\n\n\nWe\u2019re going to continue to use \nkrbuser\n as our example kerberos principal.\n\n\n\n\n\n\nOnce you\u2019ve created the principal and provided it a password, we need to\ngenerate a keytab for it. Do so via the following command:\n\n\nktadd -norandkey -k v5srvtab krbuser\n\n\nIn this case, v5srvtab is the keytab filename, and it will be placed into whatever directory you\u2019ve invoked kadmin.local from. Presumably this will be the home directory of the machine.\n\n\n\n\n\n\nYou now have everything you need done on the KDC, but you will need your\nkeytab file later as well as the \nkrb5.conf\n file that is located in the home\ndirectory of the KDC, so consider moving them somewhere (probably your local\nmachine) that will be convenient for you to access later.\n\n\n\n\n\n\nConfiguring the Sybase image for Kerberos\n\n\n\n\nStart up a Sybase database.\n\n\nNote\n: Each sybase database machine may have multiple sybase instances running on it at a given point in time. In this case, I am configuring the ASE_1550_S5 instance, but these steps can be done on any instance so long as you change the $SYBASE_HOME directories accordingly.\n\n\n\n\n\n\n\n\nConnect to the particular sybase instance you are working on and invoke the\nfollowing sql statement:\n\n\nsp_configure \u2018use security services\u2019, 1\n\n\n\n\n\n\nContinue to create a user with the same name as the principal name you created previously on the KDC, in this case \nkrbuser\n:\n\n\nsp_addlogin krbuser, <password>\n\n\n\n\n\n\nChange your \n$SYBASE\n environment variable to point to the sybase directory\nfor whichever instance you are configuring. In this case, we want to do:\n\n\nexport SYBASE=/opt/sybase/15-5\n\n\n\n\n\n\nOpen the \n$SYBASE/interfaces file\n, and find the header for whichever Sybase instance you are configuring. In our case, it is \nASE_1550_S5\n. You should see something that looks like this:\n\n\nASE1550_S5\nmaster tcp ether 10.43.89.241 5500\nmaster tcp ether localhost 5500\nquery tcp ether 10.43.89.241 5500\nquery tcp ether localhost 5500\n\n\n\n\n\nYou want to add the following line to this:\n\n\nsecmech 1.3.6.1.4.1.897.4.6.6\n\n\nThis line is static, while the other lines in this section are dynamically generated for your instance. So, your final result should look something like this:\n\n\nASE1550_S5\nmaster tcp ether 10.43.89.241 5500 **< your numbers will vary**\nmaster tcp ether localhost 5500 **< your numbers will vary**\nquery tcp ether 10.43.89.241 5500 **< your numbers will vary**\nquery tcp ether localhost 5500 **< your numbers will vary**\n\n\n\n\n\n\n\n\n\nNavigate to \n$SYBASE/OCS-15_0/config\n. You should see \nlibtcl64.cfg\n and\n\nlibtcl.cfg\n\n\na. Change the contents of \nlibtcl64.cfg\n to be this:\n\n\n[DIRECTORY]\n;ldap=libsybdldap.so ldap://ldaphost/dc=sybase,dc=com\n[SECURITY]\ncsfkrb5=libsybskrb64.so secbase=@bar.com libgss=/lib64/libgssapi_krb5.so.2.2\n[FILTERS]\n;ssl=libsybfssl.so\n\n\n\n\n\nb. Change the contents of \nlibtcl.cfg\n to be this:\n\n\n[DIRECTORY]\n;ldap=libsybdldap.so ldap://ldaphost/dc=sybase,dc=com\n[SECURITY]\ncsfkrb5=libsybskrb.so secbase=@bar.com\nlibgss=/lib64/libgssapi_krb5.so.2.2\n[FILTERS]\n;ssl=libsybfssl.so\n\n\n\n\n\nc. \nNote\n that the @bar.com value is our realm name that is determined by the KDC. Realistically, you should never have to deal with this, and it should never change, but if for some reason it does, that value needs to be updated.\n\n\n\n\n\n\nCreate a directory for those Kerberos config files you created on the KDC in\nthe previous set of steps:\n\n\nsudo mkdir /krb\n\n\nCopy into /krb your keytab file \nv5srvtab\n and config file \nkrb5.conf\n that you took off of the KDC earlier.\n\n\n\n\n\n\nHead to \n$SYBASE/ASE-15_0/install\n and open the \nRUN_ASE1550_S5\n file.\nWe\u2019re going to add information so that Sybase knows where to find our\nkeytab and our krb5.conf file, so change the content to look like this:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n#!/bin/sh\n\n\n#\n\n\n# ASE page size (KB) :    4096\n\n\n# Master device path:   /opt/sybase/devices/data5/S5_master.dat\n\n\n# Error log path:       /opt/sybase/errorlogs/ASE1550_S5.log\n\n\n# Configuration file path:      /opt/sybase/15-5/ASE-15_0/ASE1550_S5.cfg\n\n\n# Directory for shared memory files:    /opt/sybase/15-5/ASE-15_0\n\n\n# Adaptive Server name: ASE1550_S5\n\n\n#\n\n\nexport\n **KRB5_KTNAME**\n=\n/krb/v5srvtab\n\nexport\n **KRB5_CONFIG**\n=\n/krb/krb5.conf\n/opt/sybase/15-5/ASE-15_0/bin/dataserver \n\\\n\n-kASE1550_S5@bar.com \n\\\n\n-d/opt/sybase/devices/data5/S5_master.dat \n\\\n\n-e/opt/sybase/errorlogs/ASE1550_S5.log \n\\\n\n-c/opt/sybase/15-5/ASE-15_0/ASE1550_S5.cfg \n\\\n\n-M/opt/sybase/15-5/ASE-15_0 \n\\\n\n-sASE1550_S5\n\n\n\n\n\n\n\n\n\n\nReboot the Sybase instance you\u2019re working so that it reads in all of these\nconfig changes.\n\n\n\n\n\n\nConnect to the Sybase instance as the \ndbo\n user so that you may give dbo\nprivileges to your kerberos authentication login on a particular database within\nthe instance. Below is an example of doing so with the database \npotatoes\n:\n\n\n>> sql5\n1> use potatoes\n2> go\n1> sp_addalias instructions, dbo\n2> go\nAlias user added.\n(return status = 0)\n\n\n\n\n\n\n\n\n\nNow, to access the Sybase instance via kerberos and confirm success, you\ncan do the following set of commands (I put these three lines into a script\ncalled \nconnect.sh\n for future convenience):\n\n\n1\n2\n3\n4\n#!/bin/sh\n\nkinit -k -t /krb/v5srvtab <yourPrincipalName>\n\nexport\n \nSYBASE\n=\n'/opt/sybase/15-5'\n\n/opt/sybase/15-5/OCS-15_0/bin/isql64 -V -SASE1550_S5\n\n\n\n\n\n\n\n\n\n\nTesting by creating a Kerberos Connector on the Delphix Engine\n\n\n\n\n\n\nStart by configuring your engine for kerberos. SSH into the engine as the\ndelphix user and run the following command:\n\n\n/opt/delphix/server/bin/jmxtool tunable set enabled_features KERBEROS true\n\n\n\n\n\n\nLog into the virtualization engine and proceed through first-time setup if\n  you need to.\n\n\n\n\n\n\nOnce first-time setup is complete, log into the Delphix Setup page, proceed\nto Preferences > Kerberos Configuration. Add the information for your KDC to\nconfigure it with the principal name you created earlier, \nkrbuser\n. You can get the keytab by running the following command on your keytab file:\n\n\nbase64 v5srvtab\n\n\nCopy the output as plaintext into the keytab field of the kerberos configuration box.\n\n\n\n\n\n\nFinally, create a Sybase connector with parameters that look like this, and if your \u201ctest connection\u201d attempt succeeds you\u2019re all set!",
            "title": "Kerberos Configuration"
        },
        {
            "location": "/Getting_Started/Kerberos_Configuration/#kerberos-configuration",
            "text": "",
            "title": "Kerberos Configuration"
        },
        {
            "location": "/Getting_Started/Kerberos_Configuration/#introduction",
            "text": "As of 5.3.0.0, the Delphix Masking Engine supports Kerberos authentication for\nOracle, MS SQL Server, and Sybase connections. Utilizing this service requires\nthe presence of a Kerberos Key Distribution Center (KDC) server as well as\nadditional configuration actions to be done on both the the Masking Engine and\nthe database. This document presents configuration instructions for enabling\nand using Kerberos on the Delphix Masking Engine, as well as reference\nconfigurations for enabling Kerberos on the Databases. Although other\nconfigurations are possible, the configurations in this document have been\nvalidated by Delphix.",
            "title": "Introduction"
        },
        {
            "location": "/Getting_Started/Kerberos_Configuration/#terminology",
            "text": "Throughout this document, the following example values are used. To recreate\nthese reference environments, these values must be replaced with real values\nappropriate for your network environment:   .bar.com - the DNS domain of then network  BAR.COM - the Kerberos domain  me-host - the hostname of the masking engine  foo-kcd - the hostname KDC server  krbuser - the kerberos principal to be granted access to the database for masking",
            "title": "Terminology"
        },
        {
            "location": "/Getting_Started/Kerberos_Configuration/#configuring-kerberos-on-the-appliance",
            "text": "This section details the steps required to configure Kerberos on your appliance.  Step 1  On the Delphix System Setup CLI, enable the Kerberos feature.  Note \nYou may see a warning indicating that special permission is required to enable Kerberos.\nThis warning can be ignored when enabling Kerberos for use with Masking only.\nIn the following examples,  me-hosts  is the hostname of your masking engine.  $ ssh sysadmin@me-host.bar.com\nme-host> system\nme-host system> enableFeatureFlag\nme-host system enableFeatureFlag *> set name=KERBEROS\nme-host system enableFeatureFlag *> commit\nme-host system> exit  Step 2  On the Delphix System Setup CLI, configure and enable Kerberos.  $ ssh sysadmin@me-host.bar.com\nme-host service> kerberos\nme-host service kerberos> update\nme-host service kerberos update *> set name=Kerberos_Conf\nme-host service kerberos update *> edit kdcs\nme-host service kerberos update kdcs *> edit 0\nme-host service kerberos update kdcs 0 *> set hostname=foo-kcd.bar.com\nme-host service kerberos update kdcs *> back\nme-host service kerberos update *> set realm=BAR.COM\nme-host service kerberos update *> set principal=krbuser\nme-host service kerberos update *> set keytab=_krbuser_keytab_base64_\nme-host service kerberos update *> commit  In this case,  krbuser_keytab_base64  is the base64 encoded contents of the\n keytab file for krbuser. The kerberos keytab for a user is typically available\nfrom your kerberos administrator.  To display a keytab file in base64 encoding use:  $ base64 ~/krbuser.keytab  Step 2  Alternatively - On the Delphix Server Setup UI configure and enable\nKerberos:  a. From the Preferences menu select Kerberos Configuration.   b. Add record(s) for your KDCs, and populate other fields appropriately for your\n   network environment. Upon pressing  Save , your configuration will be tested.\n   If the engine is able to authenticate to the KDC with the supplied\n   configuration, the configuration is applied immediately.",
            "title": "Configuring Kerberos on the Appliance"
        },
        {
            "location": "/Getting_Started/Kerberos_Configuration/#creating-maskings-database-connectors-using-kerberos",
            "text": "Once the Delphix Appliance is configured for Kerberos, creating Connectors\nusing Kerberos authentication is simple:   Assuming you are using the same user principal configured in Server Setup, the\nkeytab will be used and it is unnecessary to enter a password in the Connector\ndefinition.  For Sybase database Connectors, it is necessary to supply the service principal\nname as an additional configuration item. For Oracle DB, this value is\ndetermined automatically.  For MS SQL Server it is determined based on the\nreverse DNS mapping of the Server Name (refer to the section on MS SQL\nServer below).",
            "title": "Creating Maskings Database Connectors using Kerberos"
        },
        {
            "location": "/Getting_Started/Kerberos_Configuration/#reference-database-configurations",
            "text": "The following are a series of reference kerberos configuration procedures and\ntroubleshooting notes for the supported databases. These are meant to serve as\nexamples to be further customized according to the user's specific network\nenvironment and security needs.",
            "title": "Reference Database Configurations"
        },
        {
            "location": "/Getting_Started/Kerberos_Configuration/#oracle-database",
            "text": "Overview  This document describes how to set up an Oracle DB instance for kerberized\nconnections. The following steps are described:   Creating a service principal and adding it to the DB system  Configuring the database to use kerberos authentication  Creating DB users identified via kerberos  Troubleshooting tips   Prerequisites  This document assumes you already have a kerberized network environment with\nan MIT Kerberos KDC. These procedures have been tested successfully with Oracle\ndatabase versions 11.2.0.2, 11.2.0.4 and 12.2.1. Oracle database version\n12.1.0.1 did not work in our testing.  You will need the following from your kerberos environment:   The krb5.conf file  A user principal and associated password or keytab you'd like to use to log\n  into the database  The ability to create a service principal for the Oracle DB and retrieve the associated keytab   This section of the document uses these example values in addition to those\nmentioned above:   The oracle database is: ora-db.bar.com.  The oracle service name is: oracle   Creating the Oracle Service Principal  The service principal will be named:  / @  Given our default values above, this works out to: oracle/ora-db@bar.com  Notice that the hostname is whatever the database system thinks its hostname\nis - that is, the output of \"uname -n\" on the database system, rather than the\nactual DNS name of the database system. Typically, these values would be the\nsame, but this is not always the case.  On the KDC, run:  # kadmin.local\nkadmin.local: addprinc -randkey oracle/ora-db@bar.com\nkadmin.local: ktadd -norandkey -k /var/tmp/ora-db.keytab oracle/ora-db@bar.com  Copy the resulting keytab file (/var/tmp/ora-db.keytab) to the Oracle DB system\nat this location:  /etc/v5srvtab  As root on the Oracle DB system, ensure that the keytab has the correct\npermissions:  # chown root:oinstall /etc/v5srvtab\n# chmod 440 /etc/v5srvtab  Finally, this is a good opportunity to copy /etc/krb5.conf from the KDC to\n/etc/krb5.conf on the Oracle DB system. This file should be readable by all users.  Configuring the Oracle Database for Kerberos  Log into the Oracle DB system as the appropriate use for the database in question.  $ cd $ORACLE_HOME\n$ vi network/admin/sqlnet.ora  Add the following for Oracle 11:  SQLNET.KERBEROS5_CONF=/etc/krb5.conf\nSQLNET.AUTHENTICATION_SERVICES=(BEQ,KERBEROS5)\nSQLNET.KERBEROS5_CONF_MIT=true\nSQLNET.AUTHENTICATION_KERBEROS5_SERVICE=oracle  Or the following for Oracle 12:  NAMES.DIRECTORY_PATH=(TNSNAMES, EZCONNECT, HOSTNAME)\nSQLNET.KERBEROS5_CONF=/etc/krb5.conf\nSQLNET.AUTHENTICATION_SERVICES=(BEQ,KERBEROS5PRE,KERBEROS5)\nSQLNET.KERBEROS5_CONF_MIT=true\nSQLNET.AUTHENTICATION_KERBEROS5_SERVICE=oracle  If the database is Oracle 11 (not necessary on Oracle 12):  $ vi dbs/init.ora  Add this line at the end:  OS_AUTHENT_PREFIX=\"\"  Creating a DB User Identified via Kerberos  Log into the Oracle DB system as the appropriate database user and open a\ndatabase session as the DBA:  $ sqlplus / as sysdba  On Oracle 12, you may wish to alter your session to create the user in one of\nthe PDBs:  SQL> alter session set container=MYPDB;  Create the user that will connect to the DB using kerberos:  SQL> create user krbdbuser identified externally as 'krbuser@BAR.COM';  Grant the user privileges necessary for masking.  This example grants all privileges for the sake of simplicity:  Oracle 11:  SQL> grant all privilege to krbdbuser;  Oracle 12: (Customize permissions as necessary for your environment).  SQL> grant connect,resource to krbdbuser;\nSQL> grant create tablespace, drop tablespace to krbdbuser;\nSQL> grant create table to krbdbuser;\nSQL> grant create sequence to krbdbuser;\nSQL> grant select_catalog_role to krbdbuser;\nSQL> grant unlimited tablespace to krbdbuser;\nSQL> grant select_catalog_role to krbdbuser;\nSQL> grant alter system to krbdbuser;\nSQL> grant sysoper to krbdbuser;\nSQL> grant dba to krbdbuser;  Troubleshooting Tips   Connecting via JDBC with kerberos authentication from Delphix Masking involves two steps: a kerberos login, followed by JDBC connect. A failure stack with an error in the login function indicates a misconfiguration on either the engine or KDC - the engine hasn't even attempted to communicate with the database at that point. Failure stacks are saved in the debugging log for masking.  Login exceptions that mention a checksum error mean either the password or keytab supplied doesn't match the expected password/key on the KDC for the principal you're trying to use. Server Setup verifies that your keytab works at configuration time, but it could stop working if the key for your principal is updated on the KDC.  Prior to version 12, Oracle databases instances assume they can create/write a particular temporary file to store kerberos credentials for the DB. This means if you attempt to run multiple kerberized instances of Oracle 11 on the same system or VM, and the databases run as different system users, the first Oracle instance that performs kerberos auth will create and own this file. Kerberos authentication will fail to function on all other instances.",
            "title": "Oracle Database"
        },
        {
            "location": "/Getting_Started/Kerberos_Configuration/#ms-sql-server",
            "text": "Overview  This is an overview of the step necessary to get your masking engine talking to\na MS SQL Server database using kerberos authentication. Since Active Directory\nalready uses Kerberos for authentication, little or no additional configuration\nis need on the MS SQL Database server.  The following steps are described in this section:   Create the necessary SPNs (Service Principal Names) for your MSSQL Database service in AD  Create the DB Connector on the masking engine  Creating a keytab for an AD User  Troubleshooting tips   Prerequisites  Configuring cross-realm trust between Active Directory and an MIT KDC Server is\na complex topic, and will not be described here. In the absence of such a setup,\nit is possible to make the Delphix Appliance a kerberos client of the Active\nDirectory (AD) Server. In this configuration, no additional KDC in necessary.\nThe example below assumes this kind of configuration.  This section of the document uses these example values in addition to or instead\nof those mentioned above:   The MSSQL server database is named mssql-db.bar.com.  The AD user configured for masking access to the MSSQL database is aduser (rather than krbuser in other examples elsewhere in this document).  The AD user that start the MS SQL Server service on the DB Server is dbuser.   Creating SPNs for the Database Service  MS SQL Server service will typically register several SPNs with AD upon startup.\nHowever, there are several conditions which can cause these SPNs to not be\nregistered successfully, or to be registered with service names other than those\nthat are expected by the jTDS JDBC driver employed by Delphix Masking.  The service principal name for an MS SQL Server expected by Delphix Masking is: MSSQLSvc/ :  For example, the SPN for our example MS SQL Server would be:  MSSQLSvc/mssql-db.bar.com:1433  In addition, it is  required  that a reverse mapping exist in DNS from the IP\naddress of the MS SQL Server system to the FQDN registered.  The following commands may be run in powershell on the MS SQL Server to assist\nin debugging SPN related issues:  List all SPNs for dbuser:  setspn -L -U dbuser  Deleting an old SPN associated with dbuser:  setspn -U -D MSSQLSvc/other-server.ad.bar.com:SQL2008R2 dbuser  Here's how to create the SPN describe above:  setspn -U -S MSSQLSvc/mssql-db.bar.com:1433 dbuser  Creating the Database Connector on the Masking Engine  Once the above steps are complete, creating the database connector can be\nperformed using the procedure above. Enter the username and optionally, password\nof the AD user in the Connector definition. Be sure that the AD user has the\nsufficient access to the MS SQL Database for masking.  The password field can be left blank when creating the connector if the user\nis the same user configured in Server Setup for the appliance. Since keytabs\nare not typically used in an AD environment, it may be useful to create one\nmanually, to avoid having a password in the DB Connector.  Creating a keytab file for an AD user  On a unix or MAC system with MIT kerberos CLI utilities installed:  # ktutil\nktutil: addent -password -p krbuser -k 1 -e arcfour-hmac\n<type password for krbuser>\nktutil: addent -password -p krbuser -k 1 -e aes128-cts-hmac-sha1-96\n<type password for krbuser>\nktutil: addent -password -p krbuser -k 1 -e aes256-cts-hmac-sha1-96\n<type password for krbuser>\nktutil: write_kt /var/tmp/krbuser.keytab\nktutil: exit\n# base64 /var/tmp/krbuser.keytab ;# This is string to user for keytab in Server Setup kerberos configuration   Note  kvno doesn't matter when using kerberos keytabs with AD. The password must match the active password for the AD user in question.   Troubleshooting Tips  The client uses the incorrect service name\nThis will typically manifest an exception mentioning cred, like:  Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Fail to create credential. (63) - No service creds)\n  at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:770)\n  at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248)\n  at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)\n  at com.microsoft.sqlserver.jdbc.KerbAuthentication.intAuthHandShake(KerbAuthentication.java:163)\n        ... 101 common frames omitted\nCaused by: sun.security.krb5.internal.KrbApErrException: Fail to create credential. (63) - No service creds\n  at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:162)\n  at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:458)\n   at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:693)\n        ... 104 common frames omitted  Why might this happen:    You're using the JTDS JDBC driver, and your MSSQL Server's IP address doesn't\n have a reverse mapping in DNS. In this case, the driver may construct a service\n name like: MSSQLSvc/ :  and try to use that.  Either correct DNS to have a valid reverse mapping for the IP of your SQL server, or manually add an SPN to active directory for the name the JDBC client is trying to use:   Determine the user that starts MSSQL Server on your DB machine.   From powershell, do: setspn -AU MSSQLSvc/ :1433   Example: setspn -AU MSSQLSvc/10.43.100.101:1433 AD\\dbuser      The database server has multiple DNS names (FQDNs). In this case, SPNs may be\nregistered only for some of them. It may be necessary to add SPNs for the other\nFQDNs as above.   The MS SQL Server didn't automatically register an SPN. There is a limit (in\nthe thousands) to the number of SPNs that may be registered for a given AD user.\nIt is quite possible to hit this limit in an environment where many MS SQL Server\nVMs are actively created and destroyed with the same configuration.    Note  In Active Directory, setspn isn't creating a service principal with distinct key as is typical for services on MIT KDCs - rather it's mapping the service principal to the key for the AD user in question.   The SPN for the SQL Server is registered to the incorrect AD account  Manifests as an exception with this text:  GSS failure: Defective token detected\n(Mechanism level: AP_REP token id does not match!)  Resolution: From powershell on the MS SQL Server:  PS> setspn -Q <SPN>  This will show what user has the SPN registered.  PS> setspn -U -D <SPN> <WRONG_ACCT>  This will unregister the SPN from that user  PS> setspn -AU <SPN> <CORRECT_ACCT>",
            "title": "MS SQL Server"
        },
        {
            "location": "/Getting_Started/Kerberos_Configuration/#sybase",
            "text": "Creating a principal and corresponding keytab on the KDC   SSH into the KDC as the user with sufficient privileges to run kadmin.local  Run the kerberos configuration CLI with kadmin.local   Add a new principal you want to authenticate as later with:\n    add_principal <principalName>  We\u2019re going to continue to use  krbuser  as our example kerberos principal.    Once you\u2019ve created the principal and provided it a password, we need to\ngenerate a keytab for it. Do so via the following command:  ktadd -norandkey -k v5srvtab krbuser  In this case, v5srvtab is the keytab filename, and it will be placed into whatever directory you\u2019ve invoked kadmin.local from. Presumably this will be the home directory of the machine.    You now have everything you need done on the KDC, but you will need your\nkeytab file later as well as the  krb5.conf  file that is located in the home\ndirectory of the KDC, so consider moving them somewhere (probably your local\nmachine) that will be convenient for you to access later.    Configuring the Sybase image for Kerberos   Start up a Sybase database.  Note : Each sybase database machine may have multiple sybase instances running on it at a given point in time. In this case, I am configuring the ASE_1550_S5 instance, but these steps can be done on any instance so long as you change the $SYBASE_HOME directories accordingly.     Connect to the particular sybase instance you are working on and invoke the\nfollowing sql statement:  sp_configure \u2018use security services\u2019, 1    Continue to create a user with the same name as the principal name you created previously on the KDC, in this case  krbuser :  sp_addlogin krbuser, <password>    Change your  $SYBASE  environment variable to point to the sybase directory\nfor whichever instance you are configuring. In this case, we want to do:  export SYBASE=/opt/sybase/15-5    Open the  $SYBASE/interfaces file , and find the header for whichever Sybase instance you are configuring. In our case, it is  ASE_1550_S5 . You should see something that looks like this:  ASE1550_S5\nmaster tcp ether 10.43.89.241 5500\nmaster tcp ether localhost 5500\nquery tcp ether 10.43.89.241 5500\nquery tcp ether localhost 5500  You want to add the following line to this:  secmech 1.3.6.1.4.1.897.4.6.6  This line is static, while the other lines in this section are dynamically generated for your instance. So, your final result should look something like this:  ASE1550_S5\nmaster tcp ether 10.43.89.241 5500 **< your numbers will vary**\nmaster tcp ether localhost 5500 **< your numbers will vary**\nquery tcp ether 10.43.89.241 5500 **< your numbers will vary**\nquery tcp ether localhost 5500 **< your numbers will vary**    Navigate to  $SYBASE/OCS-15_0/config . You should see  libtcl64.cfg  and libtcl.cfg  a. Change the contents of  libtcl64.cfg  to be this:  [DIRECTORY]\n;ldap=libsybdldap.so ldap://ldaphost/dc=sybase,dc=com\n[SECURITY]\ncsfkrb5=libsybskrb64.so secbase=@bar.com libgss=/lib64/libgssapi_krb5.so.2.2\n[FILTERS]\n;ssl=libsybfssl.so  b. Change the contents of  libtcl.cfg  to be this:  [DIRECTORY]\n;ldap=libsybdldap.so ldap://ldaphost/dc=sybase,dc=com\n[SECURITY]\ncsfkrb5=libsybskrb.so secbase=@bar.com\nlibgss=/lib64/libgssapi_krb5.so.2.2\n[FILTERS]\n;ssl=libsybfssl.so  c.  Note  that the @bar.com value is our realm name that is determined by the KDC. Realistically, you should never have to deal with this, and it should never change, but if for some reason it does, that value needs to be updated.    Create a directory for those Kerberos config files you created on the KDC in\nthe previous set of steps:  sudo mkdir /krb  Copy into /krb your keytab file  v5srvtab  and config file  krb5.conf  that you took off of the KDC earlier.    Head to  $SYBASE/ASE-15_0/install  and open the  RUN_ASE1550_S5  file.\nWe\u2019re going to add information so that Sybase knows where to find our\nkeytab and our krb5.conf file, so change the content to look like this:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 #!/bin/sh  #  # ASE page size (KB) :    4096  # Master device path:   /opt/sybase/devices/data5/S5_master.dat  # Error log path:       /opt/sybase/errorlogs/ASE1550_S5.log  # Configuration file path:      /opt/sybase/15-5/ASE-15_0/ASE1550_S5.cfg  # Directory for shared memory files:    /opt/sybase/15-5/ASE-15_0  # Adaptive Server name: ASE1550_S5  #  export  **KRB5_KTNAME** = /krb/v5srvtab export  **KRB5_CONFIG** = /krb/krb5.conf\n/opt/sybase/15-5/ASE-15_0/bin/dataserver  \\ \n-kASE1550_S5@bar.com  \\ \n-d/opt/sybase/devices/data5/S5_master.dat  \\ \n-e/opt/sybase/errorlogs/ASE1550_S5.log  \\ \n-c/opt/sybase/15-5/ASE-15_0/ASE1550_S5.cfg  \\ \n-M/opt/sybase/15-5/ASE-15_0  \\ \n-sASE1550_S5     Reboot the Sybase instance you\u2019re working so that it reads in all of these\nconfig changes.    Connect to the Sybase instance as the  dbo  user so that you may give dbo\nprivileges to your kerberos authentication login on a particular database within\nthe instance. Below is an example of doing so with the database  potatoes :  >> sql5\n1> use potatoes\n2> go\n1> sp_addalias instructions, dbo\n2> go\nAlias user added.\n(return status = 0)    Now, to access the Sybase instance via kerberos and confirm success, you\ncan do the following set of commands (I put these three lines into a script\ncalled  connect.sh  for future convenience):  1\n2\n3\n4 #!/bin/sh \nkinit -k -t /krb/v5srvtab <yourPrincipalName> export   SYBASE = '/opt/sybase/15-5' \n/opt/sybase/15-5/OCS-15_0/bin/isql64 -V -SASE1550_S5     Testing by creating a Kerberos Connector on the Delphix Engine    Start by configuring your engine for kerberos. SSH into the engine as the\ndelphix user and run the following command:  /opt/delphix/server/bin/jmxtool tunable set enabled_features KERBEROS true    Log into the virtualization engine and proceed through first-time setup if\n  you need to.    Once first-time setup is complete, log into the Delphix Setup page, proceed\nto Preferences > Kerberos Configuration. Add the information for your KDC to\nconfigure it with the principal name you created earlier,  krbuser . You can get the keytab by running the following command on your keytab file:  base64 v5srvtab  Copy the output as plaintext into the keytab field of the kerberos configuration box.    Finally, create a Sybase connector with parameters that look like this, and if your \u201ctest connection\u201d attempt succeeds you\u2019re all set!",
            "title": "Sybase"
        },
        {
            "location": "/Getting_Started/Audit_Logs/",
            "text": "Audit Logs\n\u00b6\n\n\nDelphix helps you keep a record of user actions taken in the UI or directly through our REST APIs. You can access these audit logs directly from our UI or through our APIs.\n\n\nAudit Log UI Page\n\u00b6\n\n\nThe Audit Log page can be found in the UI under the Audit tab. This page contains information on what action occurred, the user that performed the action and the time at which the action occurred. It also provides the ability to filter based on:\n\n\n\n\nuser\n\n\ntime range\n\n\narbitrary search string\n\n\naction type or action target, or both (create, connector or create database\n     connector)\n\n\n\n\nAudit Log APIs\n\u00b6\n\n\nWith 5.3.2.0, Delphix introduced an endpoint to get all Audit Logs. This endpoint contains the user name, action type, target, status, start time, and end time.\nFor more information please refer to \nAPI documentation\n.\n\n\nWhat Gets Logged?\n\u00b6\n\n\nUser actions are categorized into the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCancel\n\n\nCreate\n\n\nDelete\n\n\nEdit\n\n\nExport\n\n\nGet\n\n\nGet All\n\n\n\n\n\n\nImport\n\n\nLock\n\n\nLogin\n\n\nLogout\n\n\nRun\n\n\nTest\n\n\nUnlock\n\n\n\n\n\n\n\n\nThe objects that user actions target are categorized into the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\n\n\nAnalytics\n\n\nApplication\n\n\nApplication Log\n\n\nAsync Task\n\n\nAudit Log\n\n\n\n\n\n\n\n\nColumn Metadata\n\n\nDatabase Connector\n\n\nRuleset Connector\n\n\nDatabase Ruleset\n\n\nDomain\n\n\nEncryption Key\n\n\nEnvironment\n\n\n\n\n\n\nExecution\n\n\nFile Connector\n\n\nFile Download\n\n\nFile Field Metadata\n\n\nFile Format\n\n\nFile Metadata\n\n\nFile Ruleset\n\n\n\n\n\n\nFile Upload\n\n\nLDAP\n\n\nMainframe Dataset Connector\n\n\nMainframe Dataset Field Metadata\n\n\nMainframe Dataset Format\n\n\nMainframe Dataset Metadata\n\n\nMainframe Dataset Ruleset\n\n\n\n\n\n\nMasking Job\n\n\nProfile Expression\n\n\nProfile Job\n\n\nProfile Set\n\n\nRe Identification Job\n\n\nRole\n\n\nSSH Key\n\n\n\n\n\n\nSSO\n\n\nSyncable Object\n\n\nSystem Information\n\n\nTable Metadata\n\n\nTokenization\n\n\nUser\n\n\n\n\n\n\n\n\n\n\nRetention Policy\n\u00b6\n\n\nThe default policy stores the last one million Audit Log entries. Any entries older than the most recent million are removed daily. Additionally, there is a fail-safe mechanism that prevents an attacker from forcing an unbounded number of actions to be logged to overload the system's disk space. In the event that such an attack occurs, Delphix also logs it to the application logs.\n\n\nRecommendation\n\u00b6\n\n\nIf a full record of all Audit Log entries is desired, Delphix recommends using the new API to periodically retrieve new entries from the Audit Logs.",
            "title": "Audit Logs"
        },
        {
            "location": "/Getting_Started/Audit_Logs/#audit-logs",
            "text": "Delphix helps you keep a record of user actions taken in the UI or directly through our REST APIs. You can access these audit logs directly from our UI or through our APIs.",
            "title": "Audit Logs"
        },
        {
            "location": "/Getting_Started/Audit_Logs/#audit-log-ui-page",
            "text": "The Audit Log page can be found in the UI under the Audit tab. This page contains information on what action occurred, the user that performed the action and the time at which the action occurred. It also provides the ability to filter based on:   user  time range  arbitrary search string  action type or action target, or both (create, connector or create database\n     connector)",
            "title": "Audit Log UI Page"
        },
        {
            "location": "/Getting_Started/Audit_Logs/#audit-log-apis",
            "text": "With 5.3.2.0, Delphix introduced an endpoint to get all Audit Logs. This endpoint contains the user name, action type, target, status, start time, and end time.\nFor more information please refer to  API documentation .",
            "title": "Audit Log APIs"
        },
        {
            "location": "/Getting_Started/Audit_Logs/#what-gets-logged",
            "text": "User actions are categorized into the following:                Cancel  Create  Delete  Edit  Export  Get  Get All    Import  Lock  Login  Logout  Run  Test  Unlock     The objects that user actions target are categorized into the following:                Algorithm  Analytics  Application  Application Log  Async Task  Audit Log     Column Metadata  Database Connector  Ruleset Connector  Database Ruleset  Domain  Encryption Key  Environment    Execution  File Connector  File Download  File Field Metadata  File Format  File Metadata  File Ruleset    File Upload  LDAP  Mainframe Dataset Connector  Mainframe Dataset Field Metadata  Mainframe Dataset Format  Mainframe Dataset Metadata  Mainframe Dataset Ruleset    Masking Job  Profile Expression  Profile Job  Profile Set  Re Identification Job  Role  SSH Key    SSO  Syncable Object  System Information  Table Metadata  Tokenization  User",
            "title": "What Gets Logged?"
        },
        {
            "location": "/Getting_Started/Audit_Logs/#retention-policy",
            "text": "The default policy stores the last one million Audit Log entries. Any entries older than the most recent million are removed daily. Additionally, there is a fail-safe mechanism that prevents an attacker from forcing an unbounded number of actions to be logged to overload the system's disk space. In the event that such an attack occurs, Delphix also logs it to the application logs.",
            "title": "Retention Policy"
        },
        {
            "location": "/Getting_Started/Audit_Logs/#recommendation",
            "text": "If a full record of all Audit Log entries is desired, Delphix recommends using the new API to periodically retrieve new entries from the Audit Logs.",
            "title": "Recommendation"
        },
        {
            "location": "/Getting_Started/PDF Versions of Masking Documentation/",
            "text": "PDF Versions of Masking Documentation\n\u00b6\n\n\nMasking Engine User Guide\n\n\nMasking API Guide",
            "title": "PDF Versions of Masking Documentation"
        },
        {
            "location": "/Getting_Started/PDF Versions of Masking Documentation/#pdf-versions-of-masking-documentation",
            "text": "Masking Engine User Guide  Masking API Guide",
            "title": "PDF Versions of Masking Documentation"
        },
        {
            "location": "/Preparing_Data/Preparing_Oracle_Database_For_Profiling_and_Masking/",
            "text": "Preparing Oracle Database for Profiling/Masking\n\u00b6\n\n\nBefore masking your data, it is important to prepare your database. This \nsection explains the required changes, reasons for the changes, and instructions\non how to make the changes.\n\n\nArchive Logging\n\u00b6\n\n\nWhat is Archive Logging?\n \n\n\nOracle Database lets you save filled groups of redo log files to one or more\noffline destinations, known collectively as the archived redo log, or more\nsimply the archive log. The process of turning redo log files into archived redo\nlog files is called archiving. This process is only possible if the database is\nrunning in ARCHIVELOG mode. You can choose automatic or manual archiving.\n\n\nWhy is it important to make this change?\n \n\n\nArchive logging will slow down masking processes and absorb CPU resources that\ncould be used by the masking process.  Furthermore, since masking will change\nevery row in every table being masked logs are only needed for short term\nrecovery and transaction backout.  \n\n\nThe choice of whether to enable the archiving of filled groups of redo log files\ndepends on the availability and reliability requirements of the application\nrunning on the database. If you cannot afford to lose any data in your database\nin the event of a disk failure, use ARCHIVELOG mode. The archiving of filled\nredo log files can require you to perform extra administrative operations.\n\n\nHow exactly do I make this change? (exact commands, etc).\n \n\n\nALTER DATABASE NOARCHIVELOG;\n\n\nDB/VDB Memory Allocation\n\u00b6\n\n\nWhat is SGA?\n \nA system global area (SGA) is a group of shared memory structures that contain\ndata and control information for one Oracle database instance. If multiple users\nare concurrently connected to the same instance, then the data in the instance's\nSGA is shared among the users. Consequently, the SGA is sometimes called the\nshared global area.\n\n\nAn SGA and Oracle processes constitute an Oracle instance. Oracle automatically\nallocates memory for an SGA when you start an instance, and the operating system\nreclaims the memory when you shut down the instance. Each instance has its own\nSGA.\n\n\nThe SGA is read/write. All users connected to a multiple-process database\ninstance can read information contained within the instance's SGA, and severa\nl processes write to the SGA during execution of Oracle.\nWhen automatic SGA memory management is enabled, the sizes of the different SGA\ncomponents are flexible and can adapt to the needs of a workload without\nrequiring any additional configuration. The database automatically distributes\nthe available memory among the various components as required, allowing the\nsystem to maximize the use of all available SGA memory. Make sure the DB/VDB\nmemory allocation is sufficient for the workload. Delphix\u2019s best practices for\nsizing a VDB will handle most masking requirements.  If you plan to run many\nconcurrent masking jobs a small memory allocation will negatively impact\nperformance of the masking jobs.  \n\n\nWhy is it important to make this change?\n\n\nTo assure that masking jobs will perform at an optimum level.  \n\n\nHow exactly do I make this change? (exact commands, etc).\n \nSet automatic SGA memory management to enabled. If not allowed set the SGA based\non the diagnosis from the AWR report generated during a masking job. The DBA is \nbest suited to make the appropriate tuning changes to the SGA parameters for the\nversion of oracle being masked.\n\n\nUndo Tablespace Size And Undo Retention Time:\n\u00b6\n\n\nWhat is tablespace?\n \nEvery Oracle Database must have a method of maintaining information that is used\nto roll back, or undo, changes to the database. Such information consists of \nrecords of the actions of transactions, primarily before they are committed. \nThese records are collectively referred to as undo.\n\n\nUndo records are used to:\n - Roll back transactions when a ROLLBACK statement is issued\n - Recover the database\n - Provide read consistency\n - Analyze data as of an earlier point in time by using Oracle Flashback Query\n - Recover from logical corruptions using Oracle Flashback features\n\n\nWhen a ROLLBACK statement is issued, undo records are used to undo changes that\nwere made to the database by the uncommitted transaction. During database\nrecovery, undo records are used to undo any uncommitted changes applied from the\nredo log to the datafiles. Undo records provide read consistency by maintaining\nthe before image of the data for users who are accessing the data at the same\ntime that another user is changing it.\n\n\nWhy is it important to make this change?\n \n\n\nThe masking Engine updates or inserts masked data in batches. In the case of an\ninsert it only requires the current transaction size for the commit of each table\nbeing masked. The default per table stream is 10k rows. However, with an update\nthe transaction is not complete until the entire table is masked. So, the more\ntables and more rows and the wider (size) each row is in each table, the more\nundo space is needed to complete the transaction. Large tables, such as DW\ntables or history and Audit tables, most often need an increase to the Undo\nspace and undo Retention time for updates. If the space or time is exceeded then\nthe masking job may fail with an ORA-01555, Snapshot too old error.\n\n\nHow exactly do I make this change? (exact commands, etc).\n \n\n\nIt is highly recommended to increase the Undo space and undo Retention time when\nrunning in-place jobs on large tables. A general rule of thumb is 2 or 3 times\nthe size of the larges table(s), or if there are multiple tables running at the\nsame time, then all tables combined. A DBA is best suited to make the necessary\nUDNO Space and the UNDO Retention changes.\n\n\nRedo Logs Are Optimally Sized\n\u00b6\n\n\nWhat is Redo Logs?\n \n\n\nThe most crucial structure for recovery operations is the redo log, which\nconsists of two or more preallocated files that store all changes made to the\ndatabase as they occur. Every instance of an Oracle Database has an associated\nredo log to protect the database in case of an instance failure.\n\n\nWhy is it important to make this change?\n \n\n\nThe most important reason to make this change is to keep performance optimal.\nIf redo logs are too small, then the log switching will occur too often, using\nup valuable Oracle resource.\n\n\nHow exactly do I make this change? (exact commands, etc).\n \n\n\nA DBA is best suited to make these changes appropriately.\n\n\nChange PCTFREE to 40-50:\n\u00b6\n\n\nWhat is PCTFREE?\n \n\n\nPCTFREE and PCTUSED are used together, but PCTFREE is critical for updates.\nThe larger the PCTFREE value the more updates can be done.\n\n\nWhy is it important to make this change?\n \n\n\nPCTFREE aids in performance increases for updating Oracle during masking. The\nMasking Engine does many updates at the same time in batch mode. The more that \ncan be done without DB overhead the faster the masking jobs run.\n\n\nHow exactly do I make this change? (exact commands, etc).\n\n\nA DBA is best suited to make these changes.\n\n\nChange Primary Key To ROWID:\n\u00b6\n\n\nWhat is ROWID?\n \n\n\nFor each row in the database, the ROWID pseudocolumn returns the address of the\nrow. Oracle Database rowid values contain information necessary to locate a row.\n\n\nWhy is it important to make this change?\n\n\nThis is especially important in masking for performance. IF ROWID is used then\nOracle will manage the updates for the rows it tracks using ROWID. This makes\nupdates much faster. On occasion there may be a key (PK/FK/UK) or ID column with\nan index that is faster, but generally ROWID is the fastest.\n\n\nHow exactly do I make this change? (exact commands, etc).\n\n\nAdd ROWID as the logical key on each table in the ruleset using the Masking\nEngine GUI. Also, in a script you should drop foreign keys, and if possible\nindices and disable triggers and recreate them after the masking job has been\nrun for any of these types of columns being masked.",
            "title": "Preparing Oracle Database for Profiling/Masking"
        },
        {
            "location": "/Preparing_Data/Preparing_Oracle_Database_For_Profiling_and_Masking/#preparing-oracle-database-for-profilingmasking",
            "text": "Before masking your data, it is important to prepare your database. This \nsection explains the required changes, reasons for the changes, and instructions\non how to make the changes.",
            "title": "Preparing Oracle Database for Profiling/Masking"
        },
        {
            "location": "/Preparing_Data/Preparing_Oracle_Database_For_Profiling_and_Masking/#archive-logging",
            "text": "What is Archive Logging?    Oracle Database lets you save filled groups of redo log files to one or more\noffline destinations, known collectively as the archived redo log, or more\nsimply the archive log. The process of turning redo log files into archived redo\nlog files is called archiving. This process is only possible if the database is\nrunning in ARCHIVELOG mode. You can choose automatic or manual archiving.  Why is it important to make this change?    Archive logging will slow down masking processes and absorb CPU resources that\ncould be used by the masking process.  Furthermore, since masking will change\nevery row in every table being masked logs are only needed for short term\nrecovery and transaction backout.    The choice of whether to enable the archiving of filled groups of redo log files\ndepends on the availability and reliability requirements of the application\nrunning on the database. If you cannot afford to lose any data in your database\nin the event of a disk failure, use ARCHIVELOG mode. The archiving of filled\nredo log files can require you to perform extra administrative operations.  How exactly do I make this change? (exact commands, etc).    ALTER DATABASE NOARCHIVELOG;",
            "title": "Archive Logging"
        },
        {
            "location": "/Preparing_Data/Preparing_Oracle_Database_For_Profiling_and_Masking/#dbvdb-memory-allocation",
            "text": "What is SGA?  \nA system global area (SGA) is a group of shared memory structures that contain\ndata and control information for one Oracle database instance. If multiple users\nare concurrently connected to the same instance, then the data in the instance's\nSGA is shared among the users. Consequently, the SGA is sometimes called the\nshared global area.  An SGA and Oracle processes constitute an Oracle instance. Oracle automatically\nallocates memory for an SGA when you start an instance, and the operating system\nreclaims the memory when you shut down the instance. Each instance has its own\nSGA.  The SGA is read/write. All users connected to a multiple-process database\ninstance can read information contained within the instance's SGA, and severa\nl processes write to the SGA during execution of Oracle.\nWhen automatic SGA memory management is enabled, the sizes of the different SGA\ncomponents are flexible and can adapt to the needs of a workload without\nrequiring any additional configuration. The database automatically distributes\nthe available memory among the various components as required, allowing the\nsystem to maximize the use of all available SGA memory. Make sure the DB/VDB\nmemory allocation is sufficient for the workload. Delphix\u2019s best practices for\nsizing a VDB will handle most masking requirements.  If you plan to run many\nconcurrent masking jobs a small memory allocation will negatively impact\nperformance of the masking jobs.    Why is it important to make this change?  To assure that masking jobs will perform at an optimum level.    How exactly do I make this change? (exact commands, etc).  \nSet automatic SGA memory management to enabled. If not allowed set the SGA based\non the diagnosis from the AWR report generated during a masking job. The DBA is \nbest suited to make the appropriate tuning changes to the SGA parameters for the\nversion of oracle being masked.",
            "title": "DB/VDB Memory Allocation"
        },
        {
            "location": "/Preparing_Data/Preparing_Oracle_Database_For_Profiling_and_Masking/#undo-tablespace-size-and-undo-retention-time",
            "text": "What is tablespace?  \nEvery Oracle Database must have a method of maintaining information that is used\nto roll back, or undo, changes to the database. Such information consists of \nrecords of the actions of transactions, primarily before they are committed. \nThese records are collectively referred to as undo.  Undo records are used to:\n - Roll back transactions when a ROLLBACK statement is issued\n - Recover the database\n - Provide read consistency\n - Analyze data as of an earlier point in time by using Oracle Flashback Query\n - Recover from logical corruptions using Oracle Flashback features  When a ROLLBACK statement is issued, undo records are used to undo changes that\nwere made to the database by the uncommitted transaction. During database\nrecovery, undo records are used to undo any uncommitted changes applied from the\nredo log to the datafiles. Undo records provide read consistency by maintaining\nthe before image of the data for users who are accessing the data at the same\ntime that another user is changing it.  Why is it important to make this change?    The masking Engine updates or inserts masked data in batches. In the case of an\ninsert it only requires the current transaction size for the commit of each table\nbeing masked. The default per table stream is 10k rows. However, with an update\nthe transaction is not complete until the entire table is masked. So, the more\ntables and more rows and the wider (size) each row is in each table, the more\nundo space is needed to complete the transaction. Large tables, such as DW\ntables or history and Audit tables, most often need an increase to the Undo\nspace and undo Retention time for updates. If the space or time is exceeded then\nthe masking job may fail with an ORA-01555, Snapshot too old error.  How exactly do I make this change? (exact commands, etc).    It is highly recommended to increase the Undo space and undo Retention time when\nrunning in-place jobs on large tables. A general rule of thumb is 2 or 3 times\nthe size of the larges table(s), or if there are multiple tables running at the\nsame time, then all tables combined. A DBA is best suited to make the necessary\nUDNO Space and the UNDO Retention changes.",
            "title": "Undo Tablespace Size And Undo Retention Time:"
        },
        {
            "location": "/Preparing_Data/Preparing_Oracle_Database_For_Profiling_and_Masking/#redo-logs-are-optimally-sized",
            "text": "What is Redo Logs?    The most crucial structure for recovery operations is the redo log, which\nconsists of two or more preallocated files that store all changes made to the\ndatabase as they occur. Every instance of an Oracle Database has an associated\nredo log to protect the database in case of an instance failure.  Why is it important to make this change?    The most important reason to make this change is to keep performance optimal.\nIf redo logs are too small, then the log switching will occur too often, using\nup valuable Oracle resource.  How exactly do I make this change? (exact commands, etc).    A DBA is best suited to make these changes appropriately.",
            "title": "Redo Logs Are Optimally Sized"
        },
        {
            "location": "/Preparing_Data/Preparing_Oracle_Database_For_Profiling_and_Masking/#change-pctfree-to-40-50",
            "text": "What is PCTFREE?    PCTFREE and PCTUSED are used together, but PCTFREE is critical for updates.\nThe larger the PCTFREE value the more updates can be done.  Why is it important to make this change?    PCTFREE aids in performance increases for updating Oracle during masking. The\nMasking Engine does many updates at the same time in batch mode. The more that \ncan be done without DB overhead the faster the masking jobs run.  How exactly do I make this change? (exact commands, etc).  A DBA is best suited to make these changes.",
            "title": "Change PCTFREE to 40-50:"
        },
        {
            "location": "/Preparing_Data/Preparing_Oracle_Database_For_Profiling_and_Masking/#change-primary-key-to-rowid",
            "text": "What is ROWID?    For each row in the database, the ROWID pseudocolumn returns the address of the\nrow. Oracle Database rowid values contain information necessary to locate a row.  Why is it important to make this change?  This is especially important in masking for performance. IF ROWID is used then\nOracle will manage the updates for the rows it tracks using ROWID. This makes\nupdates much faster. On occasion there may be a key (PK/FK/UK) or ID column with\nan index that is faster, but generally ROWID is the fastest.  How exactly do I make this change? (exact commands, etc).  Add ROWID as the logical key on each table in the ruleset using the Masking\nEngine GUI. Also, in a script you should drop foreign keys, and if possible\nindices and disable triggers and recreate them after the masking job has been\nrun for any of these types of columns being masked.",
            "title": "Change Primary Key To ROWID:"
        },
        {
            "location": "/Preparing_Data/Preparing_SQL_Server_Database_For_Profiling_and_Masking/",
            "text": "Before masking your data, it is important to prepare your database. This section\nexplains the required changes, reasons for the change, and the instructions to\nmake the change.\n\n\nLogging\n\u00b6\n\n\nWhat is Simple Recovery Model?\n\n\nSQL Database Simple Recovery model - Automatically reclaims log space to keep\nspace requirements small, essentially eliminating the need to manage the\ntransaction log space. Operations that require transaction log backups are not\nsupported by the simple recovery model. \n\n\nWhy is it important to make this change?\n\n\nReducing the overhead of the transaction logging and the size of the files\nbefore checkpoints increases the masking speed significantly.\n\n\nHow exactly do I make this change?\n\n\nUsing SQL Server Management Studio open the DB properties dialog box and select\nthe \u201csimple recovery model\u201d or from a SQL Query tool enter \u201cSET RECOVERY SIMPLE.\u201d\nPlease see \n_\n for more details. \n\n\nDB/VDB Memory Allocation\n\u00b6\n\n\nWhat is min/max memory in SQL Server?\n\n\nMemory is allocated at the SQL Server level, so all the DBs will share the\nentire load. Max memory should be close the maximum available on the server. \n\n\nWhy is it important to make this change?\n\n\nTo assure that masking jobs will perform at an optimum level.  \n\n\nHow exactly do I make this change?\n\n\nUse SQL Server Management Studio and change the max memory allocation for the\nserver.\n\n\nPrimary/Foreign/DMS_ROW_ID Keys\n\u00b6\n\n\nWhat is a key?\n\n\nA key is a unique, non-null value that identifies a row in the database.  \n\n\nWhy is it important to make this change?\n\n\nUsing a PK or Foreign key is critical for fast updates. When a table does not\nhave an identity column with an index or a PK/FK then the masking engine will\nalter the table to have an Identity column, DMS_ROW_ID to optimize performance.\n\n\nHow exactly do I make this change?\n\n\nA logical key can be added to a table in the Masking Engine Ruleset for each\ntable, if there is a specific column that would find the row to update faster\nthan the current PK/FK.",
            "title": "Preparing SQL Server Database For Profiling and Masking"
        },
        {
            "location": "/Preparing_Data/Preparing_SQL_Server_Database_For_Profiling_and_Masking/#logging",
            "text": "What is Simple Recovery Model?  SQL Database Simple Recovery model - Automatically reclaims log space to keep\nspace requirements small, essentially eliminating the need to manage the\ntransaction log space. Operations that require transaction log backups are not\nsupported by the simple recovery model.   Why is it important to make this change?  Reducing the overhead of the transaction logging and the size of the files\nbefore checkpoints increases the masking speed significantly.  How exactly do I make this change?  Using SQL Server Management Studio open the DB properties dialog box and select\nthe \u201csimple recovery model\u201d or from a SQL Query tool enter \u201cSET RECOVERY SIMPLE.\u201d\nPlease see  _  for more details.",
            "title": "Logging"
        },
        {
            "location": "/Preparing_Data/Preparing_SQL_Server_Database_For_Profiling_and_Masking/#dbvdb-memory-allocation",
            "text": "What is min/max memory in SQL Server?  Memory is allocated at the SQL Server level, so all the DBs will share the\nentire load. Max memory should be close the maximum available on the server.   Why is it important to make this change?  To assure that masking jobs will perform at an optimum level.    How exactly do I make this change?  Use SQL Server Management Studio and change the max memory allocation for the\nserver.",
            "title": "DB/VDB Memory Allocation"
        },
        {
            "location": "/Preparing_Data/Preparing_SQL_Server_Database_For_Profiling_and_Masking/#primaryforeigndms_row_id-keys",
            "text": "What is a key?  A key is a unique, non-null value that identifies a row in the database.    Why is it important to make this change?  Using a PK or Foreign key is critical for fast updates. When a table does not\nhave an identity column with an index or a PK/FK then the masking engine will\nalter the table to have an Identity column, DMS_ROW_ID to optimize performance.  How exactly do I make this change?  A logical key can be added to a table in the Masking Engine Ruleset for each\ntable, if there is a specific column that would find the row to update faster\nthan the current PK/FK.",
            "title": "Primary/Foreign/DMS_ROW_ID Keys"
        },
        {
            "location": "/Preparing_Data/Preparing_Sybase_Database_For_Profiling_and_Masking/",
            "text": "Before masking your data, it is important to prepare the database. This section\nexplains the required changes, reasons for the change, and instructions to make\nthe change.\n\n\nLogging Archive\n\u00b6\n\n\nWhat is Durability Level?\n \nSybase has 3 Durability levels, Full, at_shutdown and no_recovery. Databases\nwith a durability set to \nno_recovery\n or \nat_shutdown\n\u2014whether they are in-memory\nor disk-resident\u2014are referred to as low-durability databases. Data in low\ndurability databases survives after a commit (provided you do not restart the\nserver).\n\n\nUse \ncreate database with durability=durability_level\n to set a database\u2019s\ndurability level. Adaptive Server supports \nfull\n, \nno_recovery\n, and\n\nat_shutdown\n durability levels.\n\n\nWhy is it important to make this change?\n \n\n\nWe recommend using the no_recovery to minimize log size and increase\nperformance. This should be combined with setting the sp_dboption to \u2018trunc log\non chkpt\u2019 to true and to set the sp_dboption to 'select into/bulkcopy/pllsort'\nto true. It is also recommended at the table level to use DML_Logging set to\nminimal to reduce logging DML statements, such as updates. This is best for\nlarge tables.\n\n\nHow exactly do I make this change? (exact commands, etc).\n\n\nUse create database with \ndurability=durability_level\n to set a database\u2019s\ndurability level. Adaptive Server supports \nfull\n, \nno_recovery\n, and\n\nat_shutdown\n durability levels.\n\n\ncreate database database\n\n\n`on data_device = 'size of device'`\n\n`log on log_device = 'size of device'`\n\n`with durability = no_recovery;`\n\n\n\n\n\nsp_dboption database, 'trunc log on chkpt', true;\n\n\nsp_dboption database, 'select into/bulkcopy/pllsort', true;\n\n\nALTER TABLE tablename SET dml_logging = minimal;\n\n\nWhat is min/max memory in SQL Server?\n\u00b6\n\n\nDetermining the Amount of Memory SAP ASE Needs\n\u00b6\n\n\nThe total memory SAP ASE requires to start is the sum of all memory\nconfiguration parameters plus the size of the procedure cache plus the size of\nthe buffer cache, where the size of the procedure cache and the size of the\nbuffer cache are expressed in round numbers rather than in percentages.\nThe procedure cache size and buffer cache size do not depend on the total\nmemory you configure. You can configure the procedure cache size and buffer\ncache size independently. Use \nsp_cacheconfig\n to obtain information such as the\ntotal size of each cache, the number of pools for each cache, the size of each\npool, and so on.\n\n\nUse \nsp_configure\n to determine the total amount of memory SAP ASE is using at a\ngiven moment:\n\n\n1> sp_configure \"total logical memory\"\n\n\n|\nParameter Name\n|\nDefault\n|\nMemory Used\n|\nConfig Value\n|\nRun Value\n|\nUnit\n|\nType\n|\n|---|---|---|---|---|---|---|---|\n|total logical memory|33792|127550|63775|63775|memory pages(2k)|read-only|\n\n\nThe value for the Memory Used column is represented in kilobytes, while the\nvalue for the Config Value column is represented in 2K pages.\n\n\nThe Config Value column indicates the total logical memory SAP ASE uses while it\nis running. The Run Value column shows the total logical memory being consumed\nby the current SAP ASE configuration. Your output differs when you run this\ncommand, because no two SAP ASEs are configured exactly the same.\n\n\nDetermine the SAP ASE Memory Configuration\n\u00b6\n\n\nThe total memory allocated during system start-up is the sum of memory required\nfor all the configuration needs of SAP ASE. You can obtain this value from the \nread-only configuration parameter \ntotal logical memory\n\n.\nThis value is calculated by SAP ASE. The configuration parameter \nmax memory\n must\nbe greater than or equal to \ntotal logical memory\n. \nMax memory\n indicates\nthe amount of memory you will allow for SAP ASE needs.\n\n\nDuring server start-up, by default, SAP ASE allocates memory based on the value\nof \ntotal logical memory\n. However, if the configuration parameter \nallocate\nmax shared memory\n has been set, then the memory allocated will be based on \nthe value of \nmax memory\n. The configuration parameter \nallocate max shared \nmemory\n enables a system administrator to allocate the maximum memory that is\nallowed to be used by SAP ASE, during server start-up.\n\n\nThe key points for memory configuration are:\n\n\n\n\n\n\nThe system administrator should determine the size of shared memory available\n   to SAP ASE and set \nmax memory\n to this value.\n\n\n\n\n\n\nThe configuration parameter \nallocate max shared memory\n can be turned on \n   during start-up and runtime to allocate all the shared memory up to \nmax\n   memory\n with the least number of shared memory segments. A large number of\n   shared memory segments has the disadvantage of some performance degradation\n   on certain platforms. Check your operating system documentation to determine\n   the optimal number of shared memory segments. Once a shared memory segment is\n   allocated, it cannot be released until the server is restarted.\n\n\n\n\n\n\nThe difference between \nmax memory\n and \ntotal logical\n memory determines the\n   amount of memory available for the procedure and statement caches, data\n   caches, or other configuration parameters.\n\n\n\n\n\n\nThe amount of memory SAP ASE allocates during start-up is determined by\n   either \ntotal logical memory\n or \nmax memory\n. If you set \nalloc max\n\n   \nshared memory\n  to 1, SAP ASE uses the value for \nmax memory\n.\n\n\n\n\n\n\nIf either \ntotal logical memory\n or \nmax memory\n is too high:\n\n\n\n\nSAP ASE may not start if the physical resources on your machine are not\n  sufficient.\n\n\nIf it does start, the operating system page fault rates may rise\n  significantly and the operating system may need to be reconfigured to\n  compensate.\n\n\n\n\n\n\n\n\nWhy is it important to make this change?\n\n\nTo assure that masking jobs will perform at an optimum level.  \n\n\nPrimary/Foreign/DMS_ROW_ID keys to for masking Sybase:\n\u00b6\n\n\nWhat is a key?\n\n\nA key is a unique, non-null value that identifies a row in the database.  \n\n\nWhy is it important to make this change?\n \n\n\nUsing a PK or Foreign key is critical for fast updates. When a table does not\nhave an identity column with an index or a PK/FK then the masking engine will\nalter the table to have an Identity column, DMS_ROW_ID to optimize performance.\n\n\nHow exactly do I make this change? (exact commands, etc)\n. \n\n\nA logical key can be added to a table in the Masking Engine Ruleset for each\ntable, if there is a specific column that would find the row to update faster\nthan the current PK/FK.\n\n\nNote Sybase ASE will create unavoidable log entries when a table is altered \nand will increase the log size significantly. If needed, run the masking jobs\nusing the On-The-Fly method to avoid log file increases.\n\n\nCreating a Masking User and Privileges:\n\u00b6\n\n\nIt is highly recommended to create a database user, and possibly a role, to\nmask. This user should not be created in production but should be created in\nnon-Production. The following permissions are needed:\n\n\nSyntax to add user and give privileges:\n\n\nsp_adduser mask_user;\n \n\n\nCREATE user NEWUSER;\n\n\nCREATE LOGIN mask_user WITH PASSWORD Delphix_123; --THIS MUST BE DONE IN MASTER\n\n\nCREATE USER mask_user IDENTIFIED BY Delphix_123;\n\n\nGRANT SELECT ON PII_V2 TO mask_user;\n\n\nGRANT INSERT ON PII_V2 TO mask_user;\n\n\nGRANT DELETE ON PII_V2 TO mask_user;\n\n\nGRANT ALTER ON PII_V2 TO mask_user;\n\n\nGRANT UPDATE ON PII_V2 TO mask_user;\n\n\nGRANT ALTER ANY TABLE TO mask_user;\n\n\nAdaptive Server requires a two-step process to add a user: sp_addlogin followed\nby sp_adduser.\n\n\n\n\nCREATE LOGIN MASK_SUPER_USER WITH PASSWORD Delphix_123;\n\n\nsp_addlogin MASK_SUPER_USER, Delphix_123;\n\n\nGRANT ROLE sa_role TO MASK_SUPER_USER;",
            "title": "Preparing Sybase Database For Profiling and Masking"
        },
        {
            "location": "/Preparing_Data/Preparing_Sybase_Database_For_Profiling_and_Masking/#logging-archive",
            "text": "What is Durability Level?  \nSybase has 3 Durability levels, Full, at_shutdown and no_recovery. Databases\nwith a durability set to  no_recovery  or  at_shutdown \u2014whether they are in-memory\nor disk-resident\u2014are referred to as low-durability databases. Data in low\ndurability databases survives after a commit (provided you do not restart the\nserver).  Use  create database with durability=durability_level  to set a database\u2019s\ndurability level. Adaptive Server supports  full ,  no_recovery , and at_shutdown  durability levels.  Why is it important to make this change?    We recommend using the no_recovery to minimize log size and increase\nperformance. This should be combined with setting the sp_dboption to \u2018trunc log\non chkpt\u2019 to true and to set the sp_dboption to 'select into/bulkcopy/pllsort'\nto true. It is also recommended at the table level to use DML_Logging set to\nminimal to reduce logging DML statements, such as updates. This is best for\nlarge tables.  How exactly do I make this change? (exact commands, etc).  Use create database with  durability=durability_level  to set a database\u2019s\ndurability level. Adaptive Server supports  full ,  no_recovery , and at_shutdown  durability levels.  create database database  `on data_device = 'size of device'`\n\n`log on log_device = 'size of device'`\n\n`with durability = no_recovery;`  sp_dboption database, 'trunc log on chkpt', true;  sp_dboption database, 'select into/bulkcopy/pllsort', true;  ALTER TABLE tablename SET dml_logging = minimal;",
            "title": "Logging Archive"
        },
        {
            "location": "/Preparing_Data/Preparing_Sybase_Database_For_Profiling_and_Masking/#what-is-minmax-memory-in-sql-server",
            "text": "",
            "title": "What is min/max memory in SQL Server?"
        },
        {
            "location": "/Preparing_Data/Preparing_Sybase_Database_For_Profiling_and_Masking/#determining-the-amount-of-memory-sap-ase-needs",
            "text": "The total memory SAP ASE requires to start is the sum of all memory\nconfiguration parameters plus the size of the procedure cache plus the size of\nthe buffer cache, where the size of the procedure cache and the size of the\nbuffer cache are expressed in round numbers rather than in percentages.\nThe procedure cache size and buffer cache size do not depend on the total\nmemory you configure. You can configure the procedure cache size and buffer\ncache size independently. Use  sp_cacheconfig  to obtain information such as the\ntotal size of each cache, the number of pools for each cache, the size of each\npool, and so on.  Use  sp_configure  to determine the total amount of memory SAP ASE is using at a\ngiven moment:  1> sp_configure \"total logical memory\"  | Parameter Name | Default | Memory Used | Config Value | Run Value | Unit | Type |\n|---|---|---|---|---|---|---|---|\n|total logical memory|33792|127550|63775|63775|memory pages(2k)|read-only|  The value for the Memory Used column is represented in kilobytes, while the\nvalue for the Config Value column is represented in 2K pages.  The Config Value column indicates the total logical memory SAP ASE uses while it\nis running. The Run Value column shows the total logical memory being consumed\nby the current SAP ASE configuration. Your output differs when you run this\ncommand, because no two SAP ASEs are configured exactly the same.",
            "title": "Determining the Amount of Memory SAP ASE Needs"
        },
        {
            "location": "/Preparing_Data/Preparing_Sybase_Database_For_Profiling_and_Masking/#determine-the-sap-ase-memory-configuration",
            "text": "The total memory allocated during system start-up is the sum of memory required\nfor all the configuration needs of SAP ASE. You can obtain this value from the \nread-only configuration parameter  total logical memory \n.\nThis value is calculated by SAP ASE. The configuration parameter  max memory  must\nbe greater than or equal to  total logical memory .  Max memory  indicates\nthe amount of memory you will allow for SAP ASE needs.  During server start-up, by default, SAP ASE allocates memory based on the value\nof  total logical memory . However, if the configuration parameter  allocate\nmax shared memory  has been set, then the memory allocated will be based on \nthe value of  max memory . The configuration parameter  allocate max shared \nmemory  enables a system administrator to allocate the maximum memory that is\nallowed to be used by SAP ASE, during server start-up.  The key points for memory configuration are:    The system administrator should determine the size of shared memory available\n   to SAP ASE and set  max memory  to this value.    The configuration parameter  allocate max shared memory  can be turned on \n   during start-up and runtime to allocate all the shared memory up to  max\n   memory  with the least number of shared memory segments. A large number of\n   shared memory segments has the disadvantage of some performance degradation\n   on certain platforms. Check your operating system documentation to determine\n   the optimal number of shared memory segments. Once a shared memory segment is\n   allocated, it cannot be released until the server is restarted.    The difference between  max memory  and  total logical  memory determines the\n   amount of memory available for the procedure and statement caches, data\n   caches, or other configuration parameters.    The amount of memory SAP ASE allocates during start-up is determined by\n   either  total logical memory  or  max memory . If you set  alloc max \n    shared memory   to 1, SAP ASE uses the value for  max memory .    If either  total logical memory  or  max memory  is too high:   SAP ASE may not start if the physical resources on your machine are not\n  sufficient.  If it does start, the operating system page fault rates may rise\n  significantly and the operating system may need to be reconfigured to\n  compensate.     Why is it important to make this change?  To assure that masking jobs will perform at an optimum level.",
            "title": "Determine the SAP ASE Memory Configuration"
        },
        {
            "location": "/Preparing_Data/Preparing_Sybase_Database_For_Profiling_and_Masking/#primaryforeigndms_row_id-keys-to-for-masking-sybase",
            "text": "What is a key?  A key is a unique, non-null value that identifies a row in the database.    Why is it important to make this change?    Using a PK or Foreign key is critical for fast updates. When a table does not\nhave an identity column with an index or a PK/FK then the masking engine will\nalter the table to have an Identity column, DMS_ROW_ID to optimize performance.  How exactly do I make this change? (exact commands, etc) .   A logical key can be added to a table in the Masking Engine Ruleset for each\ntable, if there is a specific column that would find the row to update faster\nthan the current PK/FK.  Note Sybase ASE will create unavoidable log entries when a table is altered \nand will increase the log size significantly. If needed, run the masking jobs\nusing the On-The-Fly method to avoid log file increases.",
            "title": "Primary/Foreign/DMS_ROW_ID keys to for masking Sybase:"
        },
        {
            "location": "/Preparing_Data/Preparing_Sybase_Database_For_Profiling_and_Masking/#creating-a-masking-user-and-privileges",
            "text": "It is highly recommended to create a database user, and possibly a role, to\nmask. This user should not be created in production but should be created in\nnon-Production. The following permissions are needed:  Syntax to add user and give privileges:  sp_adduser mask_user;    CREATE user NEWUSER;  CREATE LOGIN mask_user WITH PASSWORD Delphix_123; --THIS MUST BE DONE IN MASTER  CREATE USER mask_user IDENTIFIED BY Delphix_123;  GRANT SELECT ON PII_V2 TO mask_user;  GRANT INSERT ON PII_V2 TO mask_user;  GRANT DELETE ON PII_V2 TO mask_user;  GRANT ALTER ON PII_V2 TO mask_user;  GRANT UPDATE ON PII_V2 TO mask_user;  GRANT ALTER ANY TABLE TO mask_user;  Adaptive Server requires a two-step process to add a user: sp_addlogin followed\nby sp_adduser.   CREATE LOGIN MASK_SUPER_USER WITH PASSWORD Delphix_123;  sp_addlogin MASK_SUPER_USER, Delphix_123;  GRANT ROLE sa_role TO MASK_SUPER_USER;",
            "title": "Creating a Masking User and Privileges:"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Environments/",
            "text": "Managing Environments\n\u00b6\n\n\nThis section describes how you can create and manage your environments\nin the masking service.\n\n\nAs a reminder, environments are used to group certain sets of objects\nwithin the masking engine. They can be thought of as folders/containers\nwhere a specified user can create manage connectors, rulesets and jobs.\n\n\nThe main environment screen lists all the environments the logged in\nuser has access to. It is the first screen that appears when a user logs\nin to Delphix.\n\n\n\n\nThe main \nenvironments\n screen contains the following information and\nactions:\n\n\n\n\n\n\nEnvironment ID\n \u2014 The numeric ID of the environment used to refer\n    to the environment from the Masking API.\n\n\n\n\n\n\nApplication\n \u2014 A way to indicate the name of the application\n    whose data will be managed within this environment.\n\n\n\n\n\n\nEnvironment\n \u2014 The name of the environment.\n\n\n\n\n\n\nPurpose\n \u2014 The purpose of the environment.\n\n\n\n\n\n\nJobs\n \u2014 The number of jobs contained within the environment.\n\n\n\n\n\n\nEdit\n \u2014 Edit the environment. See more details below.\n\n\n\n\n\n\nExport\n \u2014 Export the environment. See more details below.\n\n\n\n\n\n\nCopy\n \u2014 Copy the environment. See more details below.\n\n\n\n\n\n\nDelete\n \u2014 Delete the environment. See more details below.\n\n\n\n\n\n\nThe environments on the screen can be sorted by the various informational\nfields by clicking on the respective field. In addition, the environments\nlisted can be filtered using the \nSearch\n field. See more details below.\n\n\nAdding An Application\n\u00b6\n\n\nFor an environmment to be created, an application needs to be specified. Here\nare the steps to add an application:\n\n\n\n\n\n\nOn the main environments page, near the upper right-hand\ncorner of the screen, click \nAdd Application\n.\n\n\n\n\n\n\nThe screen prompts you for the following items:\n\n\na.  Application Name\n\n\n\n\n\n\nClick \nSave\n to return to the \nEnvironments\nList/Summary\n screen.\n\n\n\n\n\n\nCreating An Environment\n\u00b6\n\n\nHere are the steps you need to take to create an environment:\n\n\n\n\n\n\nOn the main environments page, in the upper right-hand\ncorner of the screen, click \nAdd Environment\n.\n\n\n\n\n\n\nThe screen prompts you for the following items:\n\n\n\n\n\n\nApplication Name\n \u2013 The name of the application to associate with\n    the environment, for informational purposes.\n\n\n\n\n\n\nEnvironment Name\n \u2013 The display name of the new environment.\n\n\n\n\n\n\nPurpose\n \u2013 The type of masking workflow for the environment: Mask\n    or Tokenize/Re-Identify.\n\n\n\n\n\n\nEnable Approval Workflow\n \u2013 Whether or not to require approvals\n    of inventories before masking jobs can be run in the environment.\n\n\n\n\n\n\nEither click \nSave\n to return to the \nEnvironments\nList/Summary\n screen, or click \nSave & View\n to display the\n\nEnvironment Overview\n screen.\n\n\n\n\n\n\nEditing an Environment\n\u00b6\n\n\nTo change the properties of an environment, do the following\n\n\n\n\n\n\nClick the \nEdit\n icon to the right of the environment status.\n\n\n\n\n\n\nThe popup prompts you for the following information:\n\n\na.  Environment Name\n\n\nb.  Purpose\n\n\nc.  Application Name\n\n\nd.  Enable Approval Workflow\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nExporting an Environment\n\u00b6\n\n\nFor a variety of different reasons (the main one being moving\nenvironments between masking engines), you may want to export all the\nobjects within an environment (connectors, rulesets, masking jobs, etc).\n\n\nTo export an environment, you have 2 different options. The first is to\nuse Delphix\u2019s open source \nMasking\nInitializer\n\ncommand line tool that can be used to backup and restore a masking\nengine using the APIv5 endpoints. This tool is recommended when you are\ntrying to backup/export all objects on the engine.\n\n\nThe second option, which will be outlined here, is to use the Export\nEnvironment option available in the Masking UI. To export an individual\nenvironment:\n\n\n\n\n\n\nClick the \nExport\n icon.\n\n\n\n\n\n\nThe popup fills in the following items:\n\n\na.  Environment Name\n\n\nb.  File Name.\n\n\n\n\n\n\nClick \nExport\n.\n\n\n\n\n\n\nAll the information for the specified environment (connectors, rule\nsets, inventory, jobs, and so on) is exported to an XML file.\n\n\nA status popup appears. When the export operation is complete, you can\nclick on the \nDownload file\n name to access the XML file.\n\n\nImporting An Environment\n\u00b6\n\n\nOnce you have exported your environment, you can easily import it into\nanother masking engine. To import an environment:\n\n\n\n\n\n\nIn the upper right-hand corner of the screen, click \nImport\nEnvironment\n.\n\n\n\n\n\n\nThe screen prompts you for the following items:\n\n\n\n\n\n\nApplication Name\n \u2013 The name of the application associated with\n    this environment, for informational purposes. (An integrated test\n    environment can have multiple applications.\n\n\n\n\n\n\nEnvironment Name\n \u2013 The name of the environment that you want\n    to import.\n\n\n\n\n\n\nPurpose\n \u2013 The way the environment is used in the development\n    process: Development, Gold Copy, QA, Training, and so on.\n\n\n\n\n\n\nEnable Approval Workflow\n \u2013 Whether or not to require approvals\n    of inventories before masking jobs can be run.\n\n\n\n\n\n\nSelect\u2026\n \u2013 Use to browse for the XML file that contains the\n    information you want to import. (This file must be a previously\n    exported Delphix Agile Data Masking environment.)\n\n\n\n\n\n\nEither click \nSave\n to return to the \nEnvironments\nList/Summary\n screen, or click \nSave & View\n to display the\n\nEnvironment Overview\n screen.\n\n\n\n\n\n\nCopying An Environment\n\u00b6\n\n\nA user can also easily create an exact copy of a certain environment.\nThis is a very powerful feature when wanting to have several similar but\nnot exact environments but don't want to start from scratch. To copy an\nenvironment do the following:\n\n\n\n\n\n\nClick the \nCopy\n icon to the right of the environment status.\n\n\n\n\n\n\nThe popup prompts you for the following information:\n\n\na.  Environment Name\n\n\nb.  Purpose\n\n\nc.  Application Name\n\n\nd.  Enable Approval Workflow\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nDeleting An Environments\n\u00b6\n\n\nTo delete an environment:\n\n\n\n\nClick the \nDelete\n icon to the right of the environment status\n    and copy\nicon.\n\n\n\n\n\n\nWarning\n\n\nClicking the \nDelete\n icon deletes EVERYTHING for that environment: connections, inventory, rule sets, and so on. It does not delete universal settings like algorithms, domains, etc.\n\n\n\n\nSearching For Environments\n\u00b6\n\n\nWhen a large number of environments have been created on a masking engine, it may be useful to filter\nthe \nEnvironments List/Summary\n screen. To filter the environment list, do the following:\n\n\n\n\n\n\nIn the \nSearch\n field in the upper left side of the screen, enter the characters to search by.\n\n\n\n\n\n\nClick the adjacent \nSearch\n button.\n\n\n\n\n\n\nThe screen will display only the environments whose name match the specified search characters.\n\n\n\n\n\n\nTo re-display the entire list of environments, clear the \nSearch\n field of characters and click the\n\nSearch\n button again.",
            "title": "Managing Environments"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Environments/#managing-environments",
            "text": "This section describes how you can create and manage your environments\nin the masking service.  As a reminder, environments are used to group certain sets of objects\nwithin the masking engine. They can be thought of as folders/containers\nwhere a specified user can create manage connectors, rulesets and jobs.  The main environment screen lists all the environments the logged in\nuser has access to. It is the first screen that appears when a user logs\nin to Delphix.   The main  environments  screen contains the following information and\nactions:    Environment ID  \u2014 The numeric ID of the environment used to refer\n    to the environment from the Masking API.    Application  \u2014 A way to indicate the name of the application\n    whose data will be managed within this environment.    Environment  \u2014 The name of the environment.    Purpose  \u2014 The purpose of the environment.    Jobs  \u2014 The number of jobs contained within the environment.    Edit  \u2014 Edit the environment. See more details below.    Export  \u2014 Export the environment. See more details below.    Copy  \u2014 Copy the environment. See more details below.    Delete  \u2014 Delete the environment. See more details below.    The environments on the screen can be sorted by the various informational\nfields by clicking on the respective field. In addition, the environments\nlisted can be filtered using the  Search  field. See more details below.",
            "title": "Managing Environments"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Environments/#adding-an-application",
            "text": "For an environmment to be created, an application needs to be specified. Here\nare the steps to add an application:    On the main environments page, near the upper right-hand\ncorner of the screen, click  Add Application .    The screen prompts you for the following items:  a.  Application Name    Click  Save  to return to the  Environments\nList/Summary  screen.",
            "title": "Adding An Application"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Environments/#creating-an-environment",
            "text": "Here are the steps you need to take to create an environment:    On the main environments page, in the upper right-hand\ncorner of the screen, click  Add Environment .    The screen prompts you for the following items:    Application Name  \u2013 The name of the application to associate with\n    the environment, for informational purposes.    Environment Name  \u2013 The display name of the new environment.    Purpose  \u2013 The type of masking workflow for the environment: Mask\n    or Tokenize/Re-Identify.    Enable Approval Workflow  \u2013 Whether or not to require approvals\n    of inventories before masking jobs can be run in the environment.    Either click  Save  to return to the  Environments\nList/Summary  screen, or click  Save & View  to display the Environment Overview  screen.",
            "title": "Creating An Environment"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Environments/#editing-an-environment",
            "text": "To change the properties of an environment, do the following    Click the  Edit  icon to the right of the environment status.    The popup prompts you for the following information:  a.  Environment Name  b.  Purpose  c.  Application Name  d.  Enable Approval Workflow    Click  Save .",
            "title": "Editing an Environment"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Environments/#exporting-an-environment",
            "text": "For a variety of different reasons (the main one being moving\nenvironments between masking engines), you may want to export all the\nobjects within an environment (connectors, rulesets, masking jobs, etc).  To export an environment, you have 2 different options. The first is to\nuse Delphix\u2019s open source  Masking\nInitializer \ncommand line tool that can be used to backup and restore a masking\nengine using the APIv5 endpoints. This tool is recommended when you are\ntrying to backup/export all objects on the engine.  The second option, which will be outlined here, is to use the Export\nEnvironment option available in the Masking UI. To export an individual\nenvironment:    Click the  Export  icon.    The popup fills in the following items:  a.  Environment Name  b.  File Name.    Click  Export .    All the information for the specified environment (connectors, rule\nsets, inventory, jobs, and so on) is exported to an XML file.  A status popup appears. When the export operation is complete, you can\nclick on the  Download file  name to access the XML file.",
            "title": "Exporting an Environment"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Environments/#importing-an-environment",
            "text": "Once you have exported your environment, you can easily import it into\nanother masking engine. To import an environment:    In the upper right-hand corner of the screen, click  Import\nEnvironment .    The screen prompts you for the following items:    Application Name  \u2013 The name of the application associated with\n    this environment, for informational purposes. (An integrated test\n    environment can have multiple applications.    Environment Name  \u2013 The name of the environment that you want\n    to import.    Purpose  \u2013 The way the environment is used in the development\n    process: Development, Gold Copy, QA, Training, and so on.    Enable Approval Workflow  \u2013 Whether or not to require approvals\n    of inventories before masking jobs can be run.    Select\u2026  \u2013 Use to browse for the XML file that contains the\n    information you want to import. (This file must be a previously\n    exported Delphix Agile Data Masking environment.)    Either click  Save  to return to the  Environments\nList/Summary  screen, or click  Save & View  to display the Environment Overview  screen.",
            "title": "Importing An Environment"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Environments/#copying-an-environment",
            "text": "A user can also easily create an exact copy of a certain environment.\nThis is a very powerful feature when wanting to have several similar but\nnot exact environments but don't want to start from scratch. To copy an\nenvironment do the following:    Click the  Copy  icon to the right of the environment status.    The popup prompts you for the following information:  a.  Environment Name  b.  Purpose  c.  Application Name  d.  Enable Approval Workflow    Click  Save .",
            "title": "Copying An Environment"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Environments/#deleting-an-environments",
            "text": "To delete an environment:   Click the  Delete  icon to the right of the environment status\n    and copy\nicon.    Warning  Clicking the  Delete  icon deletes EVERYTHING for that environment: connections, inventory, rule sets, and so on. It does not delete universal settings like algorithms, domains, etc.",
            "title": "Deleting An Environments"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Environments/#searching-for-environments",
            "text": "When a large number of environments have been created on a masking engine, it may be useful to filter\nthe  Environments List/Summary  screen. To filter the environment list, do the following:    In the  Search  field in the upper left side of the screen, enter the characters to search by.    Click the adjacent  Search  button.    The screen will display only the environments whose name match the specified search characters.    To re-display the entire list of environments, clear the  Search  field of characters and click the Search  button again.",
            "title": "Searching For Environments"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Connectors/",
            "text": "Managing Connectors\n\u00b6\n\n\nThis section describes how you can create and manage your connectors.\n\n\nAs a reminder, connectors are the way users define the data sources to which\nthe masking engine should connect. Connectors are grouped within\nenvironments. In order to navigate to the \nconnectors\n screen, click on an\nenvironment and then click the \nConnector\n tab.\n\n\n\n\nThe \nconnectors\n screen contains the following information and actions:\n\n\n\n\n\n\nConnector ID\n \u2014 The numeric ID of the connector used to refer\n    to the connector from the Masking API.\n\n\n\n\n\n\nConnector\n \u2014 The name of the connector.\n\n\n\n\n\n\nMeta Data Source\n \u2014 The type of connector. One of Database, File, or\n    Mainframe.\n\n\n\n\n\n\nType\n \u2014 The specific type of connector.\n\n\n\n\n\n\nEdit\n \u2014 Edit the connector. See more details below.\n\n\n\n\n\n\nDelete\n \u2014 Delete the connector. See more details below.\n\n\n\n\n\n\nThe connectors on the screen can be sorted by the various informational\nfields by clicking on the respective field.\n\n\nCreating a Connector\n\u00b6\n\n\nTo create a new connector:\n\n\n\n\n\n\nIn the upper right-hand corner of the \nConnector\n tab, click\n    \nCreate Connection\n. The \nCreate Connection\n window appears,\n    prompting you for connection information for the data source you\n    would like to connect to. The required information will change\n    depending on the \nType\n of data source you select. For more\n    details on what info is needed to connect to different types\n    (Oracle, AWS RDS, etc) see sections below.\n\n\n\n\n\n\nSeveral of our connector types offer two different modes of\n    connecting, \nBasic\n and \nAdvanced Mode\n. Advanced Mode gives you\n    the ability to specify the exact JDBC URL & add parameters that\n    may not be available in Basic Mode.\n\n\n\n\nThe fields that appear on the Connector screen are specific to the selected\nConnector Type (see Connector Types below).\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nEditing a Connector\n\u00b6\n\n\nTo edit a connector:\n\n\n\n\n\n\nIn the \nConnector\n tab, click the \nEdit\n icon for the\n    connector you want to edit.\n\n\n\n\n\n\nChange any information necessary. To change the password:\n\n\n\n\n\n\nSelect the checkbox next to \nChange Password\n.\n\n\n\n\n\n\nIn the field that appears, enter the new \npassword\n.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nDeleting a Connector\n\u00b6\n\n\nTo delete a connector, click the \nDelete\n icon to the far right of the\nconnector name.\n\n\nWarning:\n\n    When you delete a connector, you also delete its rule sets and inventory\n    data.\n\n\nConnector Types\n\u00b6\n\n\nDatabase Connectors\n\u00b6\n\n\nThe fields that appear are specific to the DBMS Type you select. If you\nneed assistance determining these values, please contact your database\nadministrator.\n\n\nYou can only create connectors for the databases and/or files listed. If your\ndatabase or file type is not listed here, you cannot create a connector for it.\n\n\n\n\n\n\nConnection Type\n \u2014 (Oracle, MS SQL Server, and Sybase only) Choose a\n    connection type:\n\n\n\n\n\n\nBasic\n \u2014 Basic connection information.\n\n\n\n\n\n\nAdvanced\n \u2014 The full JDBC connect string including any database parameters.\n\n\n\n\n\n\n\n\n\n\nConnection Name\n \u2014 The name of the database connector (specific\n    for your Delphix application).\n\n\n\n\n\n\nSchema Name\n \u2014 The schema that contains the tables that this\n    connector will access.\n\n\n\n\n\n\nDatabase Name\n \u2014 The name of the database to which you are\n    connecting.\n\n\n\n\n\n\nHost Name/ IP\n \u2014 The network host name or IP\n    address of the database server.\n\n\n\n\n\n\nUse Kerberos Authentication\n - (Oracle only, optional) Whether to use\n    kerberos to authenticate to the database. This box is clear by default.\n    Before Kerberos may be used, the appliance must be properly configured -\n    refer to these instructions (link to appliance kerberos configuration\n    instructions[1]). If this box is checked, the application authenticates with\n    the kerberos KDC before connecting to the database, then uses its kerberos\n    credentials to authenticate to the database instead of a login/password.\n    When kerberos is enabled, the \"Login ID\" field is treated as the kerberos\n    user principal name. The password, if supplied, is used to authenticate the\n    user principal with the KDC. The password field may be left blank if the\n    keytab set during appliance configuration contains keys for the user\n    principal.\n\n\n\n\n\n\nLogin ID\n \u2014 The user login this connector will use to connect to the database (not applicable\n    to Kerberos Authentication).\n\n\n\n\n\n\nPassword\n \u2014 The password associated with the Login ID or\n    Username. (This password is stored encrypted.)\n\n\n\n\n\n\nPrincipal Name\n - (Kerberos Authentication only) The name of the Kerberos user principal\n    to use when authenticating with the KDC. The realm portion of the principal may be omitted\n    if it matches the configured default realm.\n\n\n\n\n\n\nService Principal\n - (Sybase with Use Kerberos Authentication only) The name of the\n    Sybase service instance.\n\n\n\n\n\n\nPort\n \u2014 The TCP port of the server.\n\n\n\n\n\n\nSID\n \u2014 (Oracle only) Oracle System ID (SID).\n\n\n\n\n\n\nInstance Name\n \u2014 (MS SQL Server only) The name of the instance.\n    This is optional. If the instance name is specified, the connector\n    ignores the specified \"Port\" and attempts to connect to the \"SQL\n    Server Browser Service\" on port 1434 to retrieve the connection\n    information for the SQL Server instance. If the instance name is\n    provided, be sure to make exceptions in the firewall for port 1434\n    as well as the particular port that the SQL Server instance\n    listens to.\n\n\n\n\n\n\nCustom Driver Name\n \u2014 (Generic only) The name of the\n    JDBC driver class, including Java package name.\n\n\n\n\n\n\nJDBC URL\n \u2014 (Generic and Advanced connector mode for\n    Oracle, MS SQL Server, and Sybase only) The custom\n    JDBC URL, typically including hostname/IP and port number.\n\n\n\n\n\n\nAll database types have a \nTest Connection\n button at the bottom left\nof the New Connector window. We highly recommend that you test your\nconnection before you save it. Do so before you leave this window. When\nyou click \nTest Connection\n, Delphix uses the information in the form\nto attempt a database connection. When finished, a status message\nappears indicating success or failure.\n\n\nFile Connectors\n\u00b6\n\n\nThe values that appear correlate to the \nFile Type\n you select.\n\n\n\n\n\n\nConnector Name\n \u2014 The name of the file connector (specific to\n    your Delphix application and unrelated to the file itself).\n\n\n\n\n\n\nConnection Mode\n \u2014 SFTP, FTP\n\n\n\n\n\n\nPath\n \u2014 The path to the directory where the file(s) are\n    located.\n\n\n\n\n\n\nServer Name\n \u2014 The name of the server used to connect to the\n    file.\n\n\n\n\n\n\nPort\n \u2014 The port used to connect to the\nserver.\n\n\n\n\n\n\nUser Name\n \u2014 The user name to connect to the server.\n\n\n\n\n\n\nPassword\n \u2014 (non-Public Key Authentication only) The associated password\n    for the server.\n\n\n\n\n\n\nPublic Key Authentication\n \u2014 (Optional) (Only appears for\n    SFTP.) Check this box to specify a public key. When you check this box, the\n    \nAvailable Keys\n dropdown appears. Choose a key from the dropdown. See\n    Delphix Masking APIs for information on uploading public keys to the masking\n    engine.\n\n\nNote:\n\nIf you plan to do on-the-fly masking then you will need to create a separate environment and connector to be the source for the files to be masked. The masked files will get put into the directory pointed to by the connector you created previously (the target). However, the file path specified in the connector of the target rule set must point to an existing file the target directory. It does not have to be a copy of the file, just an entry in the directory with the same name. It will be replaced by the masked file.",
            "title": "Managing Connectors"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Connectors/#managing-connectors",
            "text": "This section describes how you can create and manage your connectors.  As a reminder, connectors are the way users define the data sources to which\nthe masking engine should connect. Connectors are grouped within\nenvironments. In order to navigate to the  connectors  screen, click on an\nenvironment and then click the  Connector  tab.   The  connectors  screen contains the following information and actions:    Connector ID  \u2014 The numeric ID of the connector used to refer\n    to the connector from the Masking API.    Connector  \u2014 The name of the connector.    Meta Data Source  \u2014 The type of connector. One of Database, File, or\n    Mainframe.    Type  \u2014 The specific type of connector.    Edit  \u2014 Edit the connector. See more details below.    Delete  \u2014 Delete the connector. See more details below.    The connectors on the screen can be sorted by the various informational\nfields by clicking on the respective field.",
            "title": "Managing Connectors"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Connectors/#creating-a-connector",
            "text": "To create a new connector:    In the upper right-hand corner of the  Connector  tab, click\n     Create Connection . The  Create Connection  window appears,\n    prompting you for connection information for the data source you\n    would like to connect to. The required information will change\n    depending on the  Type  of data source you select. For more\n    details on what info is needed to connect to different types\n    (Oracle, AWS RDS, etc) see sections below.    Several of our connector types offer two different modes of\n    connecting,  Basic  and  Advanced Mode . Advanced Mode gives you\n    the ability to specify the exact JDBC URL & add parameters that\n    may not be available in Basic Mode.   The fields that appear on the Connector screen are specific to the selected\nConnector Type (see Connector Types below).    Click  Save .",
            "title": "Creating a Connector"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Connectors/#editing-a-connector",
            "text": "To edit a connector:    In the  Connector  tab, click the  Edit  icon for the\n    connector you want to edit.    Change any information necessary. To change the password:    Select the checkbox next to  Change Password .    In the field that appears, enter the new  password .       Click  Save .",
            "title": "Editing a Connector"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Connectors/#deleting-a-connector",
            "text": "To delete a connector, click the  Delete  icon to the far right of the\nconnector name.  Warning: \n    When you delete a connector, you also delete its rule sets and inventory\n    data.",
            "title": "Deleting a Connector"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Connectors/#connector-types",
            "text": "",
            "title": "Connector Types"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Connectors/#database-connectors",
            "text": "The fields that appear are specific to the DBMS Type you select. If you\nneed assistance determining these values, please contact your database\nadministrator.  You can only create connectors for the databases and/or files listed. If your\ndatabase or file type is not listed here, you cannot create a connector for it.    Connection Type  \u2014 (Oracle, MS SQL Server, and Sybase only) Choose a\n    connection type:    Basic  \u2014 Basic connection information.    Advanced  \u2014 The full JDBC connect string including any database parameters.      Connection Name  \u2014 The name of the database connector (specific\n    for your Delphix application).    Schema Name  \u2014 The schema that contains the tables that this\n    connector will access.    Database Name  \u2014 The name of the database to which you are\n    connecting.    Host Name/ IP  \u2014 The network host name or IP\n    address of the database server.    Use Kerberos Authentication  - (Oracle only, optional) Whether to use\n    kerberos to authenticate to the database. This box is clear by default.\n    Before Kerberos may be used, the appliance must be properly configured -\n    refer to these instructions (link to appliance kerberos configuration\n    instructions[1]). If this box is checked, the application authenticates with\n    the kerberos KDC before connecting to the database, then uses its kerberos\n    credentials to authenticate to the database instead of a login/password.\n    When kerberos is enabled, the \"Login ID\" field is treated as the kerberos\n    user principal name. The password, if supplied, is used to authenticate the\n    user principal with the KDC. The password field may be left blank if the\n    keytab set during appliance configuration contains keys for the user\n    principal.    Login ID  \u2014 The user login this connector will use to connect to the database (not applicable\n    to Kerberos Authentication).    Password  \u2014 The password associated with the Login ID or\n    Username. (This password is stored encrypted.)    Principal Name  - (Kerberos Authentication only) The name of the Kerberos user principal\n    to use when authenticating with the KDC. The realm portion of the principal may be omitted\n    if it matches the configured default realm.    Service Principal  - (Sybase with Use Kerberos Authentication only) The name of the\n    Sybase service instance.    Port  \u2014 The TCP port of the server.    SID  \u2014 (Oracle only) Oracle System ID (SID).    Instance Name  \u2014 (MS SQL Server only) The name of the instance.\n    This is optional. If the instance name is specified, the connector\n    ignores the specified \"Port\" and attempts to connect to the \"SQL\n    Server Browser Service\" on port 1434 to retrieve the connection\n    information for the SQL Server instance. If the instance name is\n    provided, be sure to make exceptions in the firewall for port 1434\n    as well as the particular port that the SQL Server instance\n    listens to.    Custom Driver Name  \u2014 (Generic only) The name of the\n    JDBC driver class, including Java package name.    JDBC URL  \u2014 (Generic and Advanced connector mode for\n    Oracle, MS SQL Server, and Sybase only) The custom\n    JDBC URL, typically including hostname/IP and port number.    All database types have a  Test Connection  button at the bottom left\nof the New Connector window. We highly recommend that you test your\nconnection before you save it. Do so before you leave this window. When\nyou click  Test Connection , Delphix uses the information in the form\nto attempt a database connection. When finished, a status message\nappears indicating success or failure.",
            "title": "Database Connectors"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Connectors/#file-connectors",
            "text": "The values that appear correlate to the  File Type  you select.    Connector Name  \u2014 The name of the file connector (specific to\n    your Delphix application and unrelated to the file itself).    Connection Mode  \u2014 SFTP, FTP    Path  \u2014 The path to the directory where the file(s) are\n    located.    Server Name  \u2014 The name of the server used to connect to the\n    file.    Port  \u2014 The port used to connect to the\nserver.    User Name  \u2014 The user name to connect to the server.    Password  \u2014 (non-Public Key Authentication only) The associated password\n    for the server.    Public Key Authentication  \u2014 (Optional) (Only appears for\n    SFTP.) Check this box to specify a public key. When you check this box, the\n     Available Keys  dropdown appears. Choose a key from the dropdown. See\n    Delphix Masking APIs for information on uploading public keys to the masking\n    engine.  Note: \nIf you plan to do on-the-fly masking then you will need to create a separate environment and connector to be the source for the files to be masked. The masked files will get put into the directory pointed to by the connector you created previously (the target). However, the file path specified in the connector of the target rule set must point to an existing file the target directory. It does not have to be a copy of the file, just an entry in the directory with the same name. It will be replaced by the masked file.",
            "title": "File Connectors"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/",
            "text": "Managing Rule Sets\n\u00b6\n\n\nThis section describes how Rule Sets can be created, edited, and\nremoved.\n\n\nThe Rule Sets Screen\n\u00b6\n\n\nFrom anywhere within an Environment, click the \nRule Set\n tab to\ndisplay the Rule Sets associated with that environment. The \nRule Sets\n\nscreen appears. If you have not yet created any rule sets, the Rule Set\nlist is empty.\n\n\n\n\nThe \nRule Sets\n screen contains the following information and actions:\n\n\n\n\n\n\nRule Set ID\n \u2014 The numeric ID of the rule set used to refer\n    to the rule set from the Masking API.\n\n\n\n\n\n\nName\n \u2014 The name of the rule set.\n\n\n\n\n\n\nMeta Data Source\n \u2014 The type of rule set. One of Database, File, or\n    Mainframe.\n\n\n\n\n\n\nType\n \u2014 The specific type of rule set.\n\n\n\n\n\n\nEdit\n \u2014 Edit the rule set. See more details below.\n\n\n\n\n\n\nRefresh/Save\n \u2014 Refresh the rule set. Only applies to Database rule sets.\n    See more details below.\n\n\n\n\n\n\nCopy\n \u2014 Copy the rule set. See more details below.\n\n\n\n\n\n\nDelete\n \u2014 Delete the rule set. See more details below.\n\n\n\n\n\n\nThe rule sets on the screen can be sorted by the various informational\nfields by clicking on the respective field.\n\n\nThe Create/Edit Rule Set Window\n\u00b6\n\n\nIn the upper right-hand corner, click the \nCreate Rule Set\n button.\n\n\nThe \nCreate Rule Set\n window appears.\n\n\n\n\n\n\n\n\n\n\n1\n\n\nRule Set Name Input Field\n\n\nWhen editing an existing rule set, this field will be filled with the existing rule set name by default.\n\n\n\n\n\n\n2\n\n\nConnector List\n\n\nWhen creating a new rule set, all available connectors will be listed here. When editing an existing rule set, only the connector currently in use will appear.\n\n\n\n\n\n\n3\n\n\nTable or File List\n\n\nIf a database connector is selected in the connector list, all available tables in the database schema associated with the connector will appear in this list. If a file connector is selected, all available files in the directory associated with the connector will appear in this list.\n\n\n\n\n\n\n4\n\n\nSelected Table or File Number\n\n\nDisplays how many tables or files you have selected.\n\n\n\n\n\n\n5\n\n\nSearch Query Input Field\n\n\nYou can enter a search query here. After typing the search query, press \nENTER\n to execute the search query.\n\n\nINFO: search query\n\n\n\n\n\n\nUse * to match any characters in the names of tables or files.\n\n\n\n\n\n\nIf you have selected a table or file before searching and it is not in the search results, it will not be included in the rule set. You can add back the table or file by removing the search query.\n\n\n\n\n\n\n\n\n\n\n6\n\n\nClear Search Button\n\n\nClick to remove any search query.\n\n\n\n\n\n\n7\n\n\nSelect All Button\n\n\nClick to select all tables or files in the table or file list.\n\n\n\n\n\n\n8\n\n\nClear All Button\n\n\nClick to deselect all tables or files in the table or file list.\n\n\n\n\n\n\n9\n\n\nFile Name Patterns Editor\n\n\nThis editor will appear only when the selected connector is a file connector.\n\n\n\n\n\n\n10\n\n\nAdd File Pattern Button\n\n\nClick to add a new file pattern entry below.\n\n\n\n\n\n\n11\n\n\nFile Pattern Input Field\n\n\nEnter the file pattern here.\n\n\n\n\n\n\n12\n\n\nRemove File Pattern Button\n\n\nClick to remove a file pattern.\n\n\n\n\n\n\n\n\n\nCreating a Rule Set\n\u00b6\n\n\nTo create a new rule set:\n\n\n\n\n\n\nClick on the name of an Environment, and then click the \nRule\n    Set\n tab.\n\n\n\n\n\n\nIn the upper right-hand corner of the \nRule Set\n screen, click\n    \nCreate Rule Set\n.\n\n\n\n\n\n\nThe \nCreate Rule Set\n screen lets you specify which tables\n    belong in the rule set.\n\n\n\n\n\n\nEnter a \nname\n for the new Rule Set.\n\n\n\n\n\n\nSelect a \nConnector\n name from the drop-down menu.\n\n\n\n\n\n\nThe list of tables for that connector appears. If you have not yet\n    created any connectors, the list is empty. Click individual table\n    names to select them, or click \nSelect All\n to select all the\n    tables in the connector. See \"Create/Edit Rule Set Window\" for\n    a description of the screen and other options.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nYou may then need to define the Rule Set by modifying the table settings\nas described in \"Modifying Tables in a Rule Set\" below.\n\n\nFor example:\n\n\n\n\n\n\nFor a table in a database rule set, you may want to filter data from the table.\n\n\n\n\n\n\nFor a file in a file or mainframe rule set, you must select a File Format to use.\n\n\n\n\n\n\nRefreshing a Rule Set\n\u00b6\n\n\nRefreshing a rule set will result in the columns in the tables in the rule set\nbeing rescanned. As a result, the inventory associated with the rule set\nwill also be refreshed, but any pre-existing algorithm assignments will be\nretained.\n\n\nTo refresh a rule set:\n\n\n\n\n\n\nClick the \nRefresh/Save\n icon to the right of the rule set on the \nRule\n    Set\n screen.\n\n\n\n\n\n\nThe \nRefresh/Savet\n icon will turn to an hour glass as the the associated\n    tables are rescanned.\n\n\n\n\n\n\nAfter the refresh is complete, the \nRefresh/Savet\n icon will return to the\n    circular arrow.\n\n\n\n\n\n\nCopying a Rule Set\n\u00b6\n\n\nIf you copy a Rule Set, the inventory associated with that Rule Set\nwill also be copied. Also, any filter conditions defined for that\nRule Set will be copied.\n\n\nTo copy a rule set:\n\n\n\n\n\n\nClick the \nCopy\n icon to the right of the rule set on the \nRule\n    Set\n screen.\n\n\n\n\n\n\nThe \nCopy Rule Set\n window appears.\n\n\n\n\n\n\nEnter a \nName\n for the new rule set.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nModify the rule set as you want, using the procedures described\n    above.\n\n\n\n\n\n\nDeleting a Rule Set\n\u00b6\n\n\nIf you delete a Rule Set, the inventory associated with that Rule Set\nwill also be deleted. Also, any filter conditions defined for that Rule\nSet will be deleted.\n\n\nTo delete a rule set, click the \nDelete\n icon to the right of the rule\nset on the \nRule Set\n screen.\n\n\nThe Rule Set Screen\n\u00b6\n\n\nFrom the \nRule Set\n tab, click on a rule set to display the tables or\nfiles in the rule set. The \nRule Set\n screen appears.\n\n\n\n\nThe \nRule Set\n screen contains the following information and actions:\n\n\n\n\n\n\nTable\n or \nFile or Pattern\n  \u2014 The name of the table or file/file pattern\n    in the rule set.\n\n\n\n\n\n\nEdit\n \u2014 Edit the table or file in the rule set. See more details below.\n\n\n\n\n\n\nDelete\n \u2014 Delete the table or file from the rule set.\n\n\n\n\n\n\nFor rule sets with a large number of tables or files, the \nRule Set\n screen will\nbe displayed on pages which can be navigated by the controls at the bottom of the list\non the page. The tables or files displayed may also be filtered using the \nSearch\n\nfield and button.\n\n\nEditing/Modifying a Rule Set\n\u00b6\n\n\nTo edit a rule set:\n\n\n\n\n\n\nClick the \nEdit\n icon to the right of the rule set on the Rule\n    Set screen.\n\n\n\n\n\n\nClick the \nEdit Rule Set\n button towards the top.\n\n\n\n\n\n\nThe \nCreate Rule Set\n screen appears. This screen lets you\n    specify which tables belong in the rule set.\n\n\n\n\n\n\nModify the rule set as you want, using the preceding procedures.\n\n\n\n\n\n\nRemoving a Table or File\n\u00b6\n\n\nTo remove a table or file from a rule set:\n\n\n\n\n\n\nFrom the \nRule Set\n screen, click the \nname\n of the desired\n    rule set.\n\n\n\n\n\n\nClick the red \ndelete\n icon to the right of the table or file\n    you want to remove.\n\n\n\n\n\n\n\n\nINFO\n\n\nIf you remove a table/file from a rule set and that table/file has an inventory,\nthat inventory will also be removed.\n\n\n\n\nModifying Tables in a Rule Set\n\u00b6\n\n\nThe features in this section are disabled for file and mainframe\nrule sets.\n\n\nYou can modify tables in a rule set as follows:\n\n\nLogical Key\n\u00b6\n\n\nIf your table has no primary keys defined in the database, and you are\nusing an In-Place strategy, you must specify an existing column or\ncolumns to be a logical key. This logical key does not change the target\ndatabase; it only provides information to Delphix. For multiple columns,\nseparate each column using a comma. Note: If no primary key is defined\nand a logical key is not defined an identify column will be created.\n\n\nTo enter a logical key:\n\n\n\n\n\n\nFrom the \nRule Set\n screen, click the \nname\n of the desired\n    rule set.\n\n\n\n\n\n\nClick the green \nedit\n icon to the right of the table whose\n    filter you wish to edit.\n\n\n\n\n\n\nOn the left, select \nLogical Key\n.\n\n\n\n\n\n\nEdit the text for this property.\n\n\n\n\n\n\nTo remove any existing code, click \nDelete\n.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nEdit Filter\n\u00b6\n\n\nUse this function to specify a filter to run on the data before loading\nit to the target database.\n\n\nTo add a filter to a database rule set table or edit a filter:\n\n\n\n\n\n\nFrom the \nRule Set\n screen, click the \nname\n of the desired\n    rule set.\n\n\n\n\n\n\nClick the green \nedit\n icon to the right of the table you want.\n\n\n\n\n\n\nOn the left, select \nEdit Filter\n.\n\n\n\n\n\n\nEdit the properties of this filter by entering or changing values\n    in the \nWhere\n field.\n\n\n\n\n\n\nBe sure to specify column name with table name prefix (for example,\ncustomer.cust_id \\<1000).\n\n\n\n\n\n\nTo remove an existing filter, click \nDelete\n.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nCustom SQL\n\u00b6\n\n\nUse this function to use SQL statements to filter data for a table.\n\n\nTo add or edit SQL code:\n\n\n\n\n\n\nFrom the \nRule Set\n screen, click the \nname\n of the desired\n    rule set.\n\n\n\n\n\n\nClick the green \nedit\n icon to the right of the table you want.\n\n\n\n\n\n\nOn the left, select \nCustom SQL\n.\n\n\n\n\n\n\nEnter custom SQL code for this table.\n\n\n\n\n\n\nDelphix will run the query to subset the table based on the SQL you\nspecify.\n\n\n\n\n\n\nTo remove any existing code, click \nDelete\n.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nTable Suffix\n\u00b6\n\n\nIf you have tables with names that change monthly, for example tables\nthat are appended with the current date, you can set a table suffix for\na rule set.\n\n\nTo set a table suffix for a rule set:\n\n\n\n\n\n\nIn the \nRule Set\n screen, click the \nname\n of the desired rule\n    set.\n\n\n\n\n\n\nClick the green \nedit\n icon to the right of the table for which\n    you wish to set the suffix.\n\n\n\n\n\n\nOn the left, select \nTable Suffix\n.\n\n\n\n\n\n\nThe \nOriginal Table Name\n will already be filled in.\n\n\n\n\n\n\n(Optional) Enter a \nSuffix date\n \nPattern\n (for example,\n    mmyy).\n\n\n\n\n\n\n(Optional) Enter a \nSuffix Value\n, if you want to append a\n    specific value.\n\n\n\n\n\n\n(Optional) Enter a \nSeparator\n (for example, _). This value\n    will be inserted before the suffix value (for example,\n    tablename_0131).\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nAdd Column\n\u00b6\n\n\nUse this function to select a column or columns from a table when you\ndon't want to load data to all the columns in a table.\n\n\nTo add a column to a database rule set table or edit a column:\n\n\n\n\n\n\nFrom the \nRule Set\n screen, click the \nname\n of the desired\n    rule set.\n\n\n\n\n\n\nClick the green \nedit\n icon to the right of the table you want.\n\n\n\n\n\n\nOn the left, select \nAdd Column\n.\n\n\n\n\n\n\nSelect one or more \ncolumn names\n to include in the table. To\n    remove a column, deselect it.\n\n\n\n\n\n\nYou can also choose \nSelect All\n or \nSelect None\n.\n\n\n\n\n\n\nSelect \nSave\n.\n\n\n\n\n\n\nJoin Table\n\u00b6\n\n\nUse this function to specify a SQL join condition so that you can define\nprimary key/foreign key relationships between tables.\n\n\nTo define or edit the join condition for a table:\n\n\n\n\n\n\nFrom the \nRule Set\n screen, click the \nname\n of the desired\n    rule set.\n\n\n\n\n\n\nClick the green \nedit\n icon to the right of the table you want.\n\n\n\n\n\n\nOn the left, select \nJoin Table\n.\n\n\n\n\n\n\nEdit the properties for this join condition.\n\n\n\n\n\n\nTo remove an existing join condition, click \nDelete\n.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nList\n\u00b6\n\n\nUse this function to select a list to use for filtering data in a table.\n\n\nTo add or edit a list:\n\n\n\n\n\n\nFrom the \nRule Set\n screen, click the \nname\n of the desired\n    rule set.\n\n\n\n\n\n\nClick the green \nedit\n icon to the right of the table you want.\n\n\n\n\n\n\nOn the left, select \nList\n.\n\n\n\n\n\n\nEdit the text file properties for this list.\n\n\n\n\n\n\nSelect a \ncolumn\n.\n\n\n\n\n\n\nEnter or browse for a \nfilename\n.\n\n\n\n\n\n\nFiles that have already been specified appear next to\n    \nExisting File\n.\n\n\n\n\n\n\n\n\n\n\nTo remove an existing list file, click \nDelete\n.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nCreating a Ruleset For File Formats\n\u00b6\n\n\nOnce you create a ruleset with a file or set of files, you will need to assign those files to their appropriate file format.\n\n\nThis is accomplished by editing the ruleset. Click on the edit button for the file the Edit File window will appear with the file name. From the format drop-down select the proper format for the file.\n\n\n\n\nIf the file is a Mainframe data sets file with a copybook you will see a checkbox to signify if the file is variable length.\n\n\nFor all other file types, select the end-of-record to let Delphix know whether the file is in windows/dos format (CR+LF) or Linux format (LF).\n\n\nIf the file is a delimited file you will have a space to put in the delimiter.\n\n\nIf there are multiple files in the ruleset you will have to edit each one individually and assign it to the appropriate file format.",
            "title": "Managing Rule Sets"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#managing-rule-sets",
            "text": "This section describes how Rule Sets can be created, edited, and\nremoved.",
            "title": "Managing Rule Sets"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#the-rule-sets-screen",
            "text": "From anywhere within an Environment, click the  Rule Set  tab to\ndisplay the Rule Sets associated with that environment. The  Rule Sets \nscreen appears. If you have not yet created any rule sets, the Rule Set\nlist is empty.   The  Rule Sets  screen contains the following information and actions:    Rule Set ID  \u2014 The numeric ID of the rule set used to refer\n    to the rule set from the Masking API.    Name  \u2014 The name of the rule set.    Meta Data Source  \u2014 The type of rule set. One of Database, File, or\n    Mainframe.    Type  \u2014 The specific type of rule set.    Edit  \u2014 Edit the rule set. See more details below.    Refresh/Save  \u2014 Refresh the rule set. Only applies to Database rule sets.\n    See more details below.    Copy  \u2014 Copy the rule set. See more details below.    Delete  \u2014 Delete the rule set. See more details below.    The rule sets on the screen can be sorted by the various informational\nfields by clicking on the respective field.",
            "title": "The Rule Sets Screen"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#the-createedit-rule-set-window",
            "text": "In the upper right-hand corner, click the  Create Rule Set  button.  The  Create Rule Set  window appears.      1  Rule Set Name Input Field  When editing an existing rule set, this field will be filled with the existing rule set name by default.    2  Connector List  When creating a new rule set, all available connectors will be listed here. When editing an existing rule set, only the connector currently in use will appear.    3  Table or File List  If a database connector is selected in the connector list, all available tables in the database schema associated with the connector will appear in this list. If a file connector is selected, all available files in the directory associated with the connector will appear in this list.    4  Selected Table or File Number  Displays how many tables or files you have selected.    5  Search Query Input Field  You can enter a search query here. After typing the search query, press  ENTER  to execute the search query.  INFO: search query    Use * to match any characters in the names of tables or files.    If you have selected a table or file before searching and it is not in the search results, it will not be included in the rule set. You can add back the table or file by removing the search query.      6  Clear Search Button  Click to remove any search query.    7  Select All Button  Click to select all tables or files in the table or file list.    8  Clear All Button  Click to deselect all tables or files in the table or file list.    9  File Name Patterns Editor  This editor will appear only when the selected connector is a file connector.    10  Add File Pattern Button  Click to add a new file pattern entry below.    11  File Pattern Input Field  Enter the file pattern here.    12  Remove File Pattern Button  Click to remove a file pattern.",
            "title": "The Create/Edit Rule Set Window"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#creating-a-rule-set",
            "text": "To create a new rule set:    Click on the name of an Environment, and then click the  Rule\n    Set  tab.    In the upper right-hand corner of the  Rule Set  screen, click\n     Create Rule Set .    The  Create Rule Set  screen lets you specify which tables\n    belong in the rule set.    Enter a  name  for the new Rule Set.    Select a  Connector  name from the drop-down menu.    The list of tables for that connector appears. If you have not yet\n    created any connectors, the list is empty. Click individual table\n    names to select them, or click  Select All  to select all the\n    tables in the connector. See \"Create/Edit Rule Set Window\" for\n    a description of the screen and other options.    Click  Save .    You may then need to define the Rule Set by modifying the table settings\nas described in \"Modifying Tables in a Rule Set\" below.  For example:    For a table in a database rule set, you may want to filter data from the table.    For a file in a file or mainframe rule set, you must select a File Format to use.",
            "title": "Creating a Rule Set"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#refreshing-a-rule-set",
            "text": "Refreshing a rule set will result in the columns in the tables in the rule set\nbeing rescanned. As a result, the inventory associated with the rule set\nwill also be refreshed, but any pre-existing algorithm assignments will be\nretained.  To refresh a rule set:    Click the  Refresh/Save  icon to the right of the rule set on the  Rule\n    Set  screen.    The  Refresh/Savet  icon will turn to an hour glass as the the associated\n    tables are rescanned.    After the refresh is complete, the  Refresh/Savet  icon will return to the\n    circular arrow.",
            "title": "Refreshing a Rule Set"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#copying-a-rule-set",
            "text": "If you copy a Rule Set, the inventory associated with that Rule Set\nwill also be copied. Also, any filter conditions defined for that\nRule Set will be copied.  To copy a rule set:    Click the  Copy  icon to the right of the rule set on the  Rule\n    Set  screen.    The  Copy Rule Set  window appears.    Enter a  Name  for the new rule set.    Click  Save .    Modify the rule set as you want, using the procedures described\n    above.",
            "title": "Copying a Rule Set"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#deleting-a-rule-set",
            "text": "If you delete a Rule Set, the inventory associated with that Rule Set\nwill also be deleted. Also, any filter conditions defined for that Rule\nSet will be deleted.  To delete a rule set, click the  Delete  icon to the right of the rule\nset on the  Rule Set  screen.",
            "title": "Deleting a Rule Set"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#the-rule-set-screen",
            "text": "From the  Rule Set  tab, click on a rule set to display the tables or\nfiles in the rule set. The  Rule Set  screen appears.   The  Rule Set  screen contains the following information and actions:    Table  or  File or Pattern   \u2014 The name of the table or file/file pattern\n    in the rule set.    Edit  \u2014 Edit the table or file in the rule set. See more details below.    Delete  \u2014 Delete the table or file from the rule set.    For rule sets with a large number of tables or files, the  Rule Set  screen will\nbe displayed on pages which can be navigated by the controls at the bottom of the list\non the page. The tables or files displayed may also be filtered using the  Search \nfield and button.",
            "title": "The Rule Set Screen"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#editingmodifying-a-rule-set",
            "text": "To edit a rule set:    Click the  Edit  icon to the right of the rule set on the Rule\n    Set screen.    Click the  Edit Rule Set  button towards the top.    The  Create Rule Set  screen appears. This screen lets you\n    specify which tables belong in the rule set.    Modify the rule set as you want, using the preceding procedures.",
            "title": "Editing/Modifying a Rule Set"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#removing-a-table-or-file",
            "text": "To remove a table or file from a rule set:    From the  Rule Set  screen, click the  name  of the desired\n    rule set.    Click the red  delete  icon to the right of the table or file\n    you want to remove.     INFO  If you remove a table/file from a rule set and that table/file has an inventory,\nthat inventory will also be removed.",
            "title": "Removing a Table or File"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#modifying-tables-in-a-rule-set",
            "text": "The features in this section are disabled for file and mainframe\nrule sets.  You can modify tables in a rule set as follows:",
            "title": "Modifying Tables in a Rule Set"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#logical-key",
            "text": "If your table has no primary keys defined in the database, and you are\nusing an In-Place strategy, you must specify an existing column or\ncolumns to be a logical key. This logical key does not change the target\ndatabase; it only provides information to Delphix. For multiple columns,\nseparate each column using a comma. Note: If no primary key is defined\nand a logical key is not defined an identify column will be created.  To enter a logical key:    From the  Rule Set  screen, click the  name  of the desired\n    rule set.    Click the green  edit  icon to the right of the table whose\n    filter you wish to edit.    On the left, select  Logical Key .    Edit the text for this property.    To remove any existing code, click  Delete .    Click  Save .",
            "title": "Logical Key"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#edit-filter",
            "text": "Use this function to specify a filter to run on the data before loading\nit to the target database.  To add a filter to a database rule set table or edit a filter:    From the  Rule Set  screen, click the  name  of the desired\n    rule set.    Click the green  edit  icon to the right of the table you want.    On the left, select  Edit Filter .    Edit the properties of this filter by entering or changing values\n    in the  Where  field.    Be sure to specify column name with table name prefix (for example,\ncustomer.cust_id \\<1000).    To remove an existing filter, click  Delete .    Click  Save .",
            "title": "Edit Filter"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#custom-sql",
            "text": "Use this function to use SQL statements to filter data for a table.  To add or edit SQL code:    From the  Rule Set  screen, click the  name  of the desired\n    rule set.    Click the green  edit  icon to the right of the table you want.    On the left, select  Custom SQL .    Enter custom SQL code for this table.    Delphix will run the query to subset the table based on the SQL you\nspecify.    To remove any existing code, click  Delete .    Click  Save .",
            "title": "Custom SQL"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#table-suffix",
            "text": "If you have tables with names that change monthly, for example tables\nthat are appended with the current date, you can set a table suffix for\na rule set.  To set a table suffix for a rule set:    In the  Rule Set  screen, click the  name  of the desired rule\n    set.    Click the green  edit  icon to the right of the table for which\n    you wish to set the suffix.    On the left, select  Table Suffix .    The  Original Table Name  will already be filled in.    (Optional) Enter a  Suffix date   Pattern  (for example,\n    mmyy).    (Optional) Enter a  Suffix Value , if you want to append a\n    specific value.    (Optional) Enter a  Separator  (for example, _). This value\n    will be inserted before the suffix value (for example,\n    tablename_0131).    Click  Save .",
            "title": "Table Suffix"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#add-column",
            "text": "Use this function to select a column or columns from a table when you\ndon't want to load data to all the columns in a table.  To add a column to a database rule set table or edit a column:    From the  Rule Set  screen, click the  name  of the desired\n    rule set.    Click the green  edit  icon to the right of the table you want.    On the left, select  Add Column .    Select one or more  column names  to include in the table. To\n    remove a column, deselect it.    You can also choose  Select All  or  Select None .    Select  Save .",
            "title": "Add Column"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#join-table",
            "text": "Use this function to specify a SQL join condition so that you can define\nprimary key/foreign key relationships between tables.  To define or edit the join condition for a table:    From the  Rule Set  screen, click the  name  of the desired\n    rule set.    Click the green  edit  icon to the right of the table you want.    On the left, select  Join Table .    Edit the properties for this join condition.    To remove an existing join condition, click  Delete .    Click  Save .",
            "title": "Join Table"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#list",
            "text": "Use this function to select a list to use for filtering data in a table.  To add or edit a list:    From the  Rule Set  screen, click the  name  of the desired\n    rule set.    Click the green  edit  icon to the right of the table you want.    On the left, select  List .    Edit the text file properties for this list.    Select a  column .    Enter or browse for a  filename .    Files that have already been specified appear next to\n     Existing File .      To remove an existing list file, click  Delete .    Click  Save .",
            "title": "List"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Rule_Sets/#creating-a-ruleset-for-file-formats",
            "text": "Once you create a ruleset with a file or set of files, you will need to assign those files to their appropriate file format.  This is accomplished by editing the ruleset. Click on the edit button for the file the Edit File window will appear with the file name. From the format drop-down select the proper format for the file.   If the file is a Mainframe data sets file with a copybook you will see a checkbox to signify if the file is variable length.  For all other file types, select the end-of-record to let Delphix know whether the file is in windows/dos format (CR+LF) or Linux format (LF).  If the file is a delimited file you will have a space to put in the delimiter.  If there are multiple files in the ruleset you will have to edit each one individually and assign it to the appropriate file format.",
            "title": "Creating a Ruleset For File Formats"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_File_Formats/",
            "text": "Managing File Formats\n\u00b6\n\n\nFile formats\n\u00b6\n\n\nUnlike databases files for the most part do not have built in metadata to describe the format of the fields in the file. You must provide this to Delphix so it can update the file appropriately. This is done through the settings tab where you will see a menu item on the left for File Format. Select File Format and you will see options to create a file format or input a file format. This will depend on the type of file and how you want to let Delphix know the format of the file.\n\n\nMainframe and XML files\n\u00b6\n\n\nFor Mainframe data sets, you can specify the file format via Input Format option which will import the copybook directly into Delphix. You can input this file from SFTP or FTP. Please select Copybook as the Import Format Type.\nFor XML files you can also input the file format with the input format option. You can use the file you want to mask as the format. Delphix will input the format of the file directly. You can input this file from SFTP or FTP. Please select XML as the Import Format Type.\n\n\nDelimited, Excel, Fixed files\n\u00b6\n\n\nFor Delimited, Excel, and Fixed files you can either manually create the format of the file yourself, or you can input a text file which describes the structure of the file to Delphix. To input the file format for delimited or Excel files create a text document with the column names each on its own line. For example:\n\n\n\n\nName\n\n\nAddress\n\n\nCity\n\n\nState\n\n\n\n\nTo input the file format for fixed files create a text document with the column names and the length of each column on its own line. For example:\n\n\n\n\nName,25\n\n\nAddress,40\n\n\nCity,20\n\n\nState,2\n\n\n\n\nThen input this file as the file format. The name of the text file will be the name of the file format. To create a format manually, you can just click the create format button and give the format a name. We will input the details of the format a little later in this document.\n\n\n\n\n\n\nClick \nCreate Format\n in the upper right. The Create File Format window appears.\n\n\nEnter a \nFile Format Name\n.\n\n\n\n\nChoose a \nFile Format Type\n:\n\n\n\n\nDelimited File\n\n\nExcel Sheet\n\n\nFixed Width File\n\n\n\n\nNote\n:\n Creating a Copybook or XML file format is not supported. These formats must be imported instead.\n\n\n\n\n\n\nOptionally, enter a \nDescription\n.\n\n\n\n\n\n\nClick \nSubmit\n.\n\n\n\n\n\n\n\n\nTo Import a New File Format\n\u00b6\n\n\n\n\nClick \nImport Format\n at the upper right. The Import File Format window appears.\n\n\nSelect an \nImport File Type\n.\n\n\n\n\nFor a Format Type of Copybook or XML\n\u00b6\n\n\n\n\nSelect a \nConnection Mode\n.\n\n\nFill out the required fields of the selected \nConnection Mode\n.\n\n\nClick \nBrowse\n.\n\n\nClick the \nSelect\n button to the right of the desired import file format.\n\n\nEnter a \nLogical Name\n.\n\n\nClick \nSubmit\n.\n\n\n\n\nFor a Format Type of Delimited File, Excel sheet, or Fixed Width File\n\u00b6\n\n\n\n\nClick \nSelect\n.\n\n\nBrowse for the file from which to import fields.\n\n\nClick \nSave\n.\n\n\n\n\nNote:\n\n     - The file must have NO header.\n     - Make sure there are no spaces or returns at the end of the last line in the file.\n     - To be masked, the field names must be in the same order as they are in the file.\n\n\nRemoving a Selected File\n\u00b6\n\n\n\n\nIf you accidentally selected an incorrect file, simply click Remove button to the right of the file and repeat selection steps above.\n\n\nSamples\n\u00b6\n\n\nThe following is sample file content for Delimited or Excel file formats. With these formats just the field name is provided. Notice there is no header and only a list of values.\n\n\n\n\nThe following is sample file content for Fixed Width format. In this format the field name is followed by the length of the field, separated by a comma. Notice there is no header and only a list of values.\n\n\n\n\nTo Delete a File Format\n\u00b6\n\n\n\n\nClick the \nDelete\n icon to the right of the File Format name.\n\n\nFile inventory is based on file format. Therefore, if you make a change to a file inventory, that change applies to \nall\n files that use that format.\n\n\nYou can only add or delete a file format; you cannot edit one.\n\n\n\n\nAssigning a File Format to a files\n\u00b6\n\n\nOnce you create a ruleset with a file or set of files, you will need to assign those files to their appropriate file format. This is accomplished by editing the ruleset. When you click on the edit button for the file a popup screen called edit file will appear with the file name. There will be a dropdown for the format so you can select the proper format for the file. If the file is a Mainframe data sets file with a copybook you will see a checkbox to signify if the file is variable length. For all other file types, select the end-of-record to let Delphix know whether the file is in windows/dos format (CR+LF) or Linux format (LF). If the file is a delimited file you will have a space to put in the delimiter. If there are multiple files in the ruleset you will have to edit each one individually and assign it to the appropriate file format.",
            "title": "Managing File Formats"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_File_Formats/#managing-file-formats",
            "text": "",
            "title": "Managing File Formats"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_File_Formats/#file-formats",
            "text": "Unlike databases files for the most part do not have built in metadata to describe the format of the fields in the file. You must provide this to Delphix so it can update the file appropriately. This is done through the settings tab where you will see a menu item on the left for File Format. Select File Format and you will see options to create a file format or input a file format. This will depend on the type of file and how you want to let Delphix know the format of the file.",
            "title": "File formats"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_File_Formats/#mainframe-and-xml-files",
            "text": "For Mainframe data sets, you can specify the file format via Input Format option which will import the copybook directly into Delphix. You can input this file from SFTP or FTP. Please select Copybook as the Import Format Type.\nFor XML files you can also input the file format with the input format option. You can use the file you want to mask as the format. Delphix will input the format of the file directly. You can input this file from SFTP or FTP. Please select XML as the Import Format Type.",
            "title": "Mainframe and XML files"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_File_Formats/#delimited-excel-fixed-files",
            "text": "For Delimited, Excel, and Fixed files you can either manually create the format of the file yourself, or you can input a text file which describes the structure of the file to Delphix. To input the file format for delimited or Excel files create a text document with the column names each on its own line. For example:   Name  Address  City  State   To input the file format for fixed files create a text document with the column names and the length of each column on its own line. For example:   Name,25  Address,40  City,20  State,2   Then input this file as the file format. The name of the text file will be the name of the file format. To create a format manually, you can just click the create format button and give the format a name. We will input the details of the format a little later in this document.    Click  Create Format  in the upper right. The Create File Format window appears.  Enter a  File Format Name .   Choose a  File Format Type :   Delimited File  Excel Sheet  Fixed Width File   Note :\n Creating a Copybook or XML file format is not supported. These formats must be imported instead.    Optionally, enter a  Description .    Click  Submit .",
            "title": "Delimited, Excel, Fixed files"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_File_Formats/#to-import-a-new-file-format",
            "text": "Click  Import Format  at the upper right. The Import File Format window appears.  Select an  Import File Type .",
            "title": "To Import a New File Format"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_File_Formats/#for-a-format-type-of-copybook-or-xml",
            "text": "Select a  Connection Mode .  Fill out the required fields of the selected  Connection Mode .  Click  Browse .  Click the  Select  button to the right of the desired import file format.  Enter a  Logical Name .  Click  Submit .",
            "title": "For a Format Type of Copybook or XML"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_File_Formats/#for-a-format-type-of-delimited-file-excel-sheet-or-fixed-width-file",
            "text": "Click  Select .  Browse for the file from which to import fields.  Click  Save .   Note: \n     - The file must have NO header.\n     - Make sure there are no spaces or returns at the end of the last line in the file.\n     - To be masked, the field names must be in the same order as they are in the file.",
            "title": "For a Format Type of Delimited File, Excel sheet, or Fixed Width File"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_File_Formats/#removing-a-selected-file",
            "text": "If you accidentally selected an incorrect file, simply click Remove button to the right of the file and repeat selection steps above.",
            "title": "Removing a Selected File"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_File_Formats/#samples",
            "text": "The following is sample file content for Delimited or Excel file formats. With these formats just the field name is provided. Notice there is no header and only a list of values.   The following is sample file content for Fixed Width format. In this format the field name is followed by the length of the field, separated by a comma. Notice there is no header and only a list of values.",
            "title": "Samples"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_File_Formats/#to-delete-a-file-format",
            "text": "Click the  Delete  icon to the right of the File Format name.  File inventory is based on file format. Therefore, if you make a change to a file inventory, that change applies to  all  files that use that format.  You can only add or delete a file format; you cannot edit one.",
            "title": "To Delete a File Format"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_File_Formats/#assigning-a-file-format-to-a-files",
            "text": "Once you create a ruleset with a file or set of files, you will need to assign those files to their appropriate file format. This is accomplished by editing the ruleset. When you click on the edit button for the file a popup screen called edit file will appear with the file name. There will be a dropdown for the format so you can select the proper format for the file. If the file is a Mainframe data sets file with a copybook you will see a checkbox to signify if the file is variable length. For all other file types, select the end-of-record to let Delphix know whether the file is in windows/dos format (CR+LF) or Linux format (LF). If the file is a delimited file you will have a space to put in the delimiter. If there are multiple files in the ruleset you will have to edit each one individually and assign it to the appropriate file format.",
            "title": "Assigning a File Format to a files"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/",
            "text": "Managing Inventories\n\u00b6\n\n\nAn inventory describes all of the data present in a particular data source and defines the methods which will be used to secure it. Inventories typically include the table or file name, column/field name, the data classification, and the chosen algorithm.\n\n\nThe Inventory Screen\n\u00b6\n\n\nFrom anywhere within an environment, click the \nInventory\n tab to see the Inventory Screen. This displays the inventory for the environment's rule sets.\n\n\nInventory Settings\n\u00b6\n\n\nTo specify your inventory settings:\n\n\n\n\nOn the left-hand side of the screen, select a \nRule Set\n from the drop-down menu.\n\n\nBelow this, \nContents\n lists all the tables or files defined for the rule set.\n\n\n\n\n\n\n\n\n\n\nSelect a \ntable\n or \nfile\n for which you want to create or edit the inventory of sensitive data.\nThe \nColumns\n or \nFields\n for that specific table or file appear.\n\n\n\n\n\n\nIf a column is a primary key (PK), Foreign Key (FK), or index (IDX), an icon indicating this will appear to the Right of the column name. If there is a note for the column, a Note icon will appear. To read the note, click the icon.\n\n\n\n\nIf an algorithm associated with a column is a custom algorithm (formerly known as Mapplet) then (\ncustom\n) in red text will appear after the algorithm name.\n\n\nIf you selected a table, metadata for the column appears: \nData Type\n and \nLength\n (in parentheses). This information is read-only.\n\n\n\n\nChoose how you would like to view the inventory:\n\n\n\n\n\n\nAll Fields\n \u2014 Displays all columns in the table or all fields in the file (allowing you to mark new columns or fields to be masked).\n\n\n\n\n\n\nMasked Fields\n \u2014 Filters the list to just those columns or fields that are already marked for masking.\n\n\n\n\n\n\nChoose how to determine whether to mask/unmask a column:\n\n\n\n\n\n\nAuto\n \u2014 The default value. The profiling job can determine or update the algorithm assigned to a column and whether to mask the column.\n\n\n\n\nUser\n \u2014 The user's choice overrides the profiling job. The user manually updates the algorithm assignment, mask/unmask option of the column. The Profiler will ignore the column, so it will not be updated as part of the Profiling job. In order to use the Secure Lookup algorithm, the user would select it as a user-defined algorithm and assign it to the specific column. Secure Lookup automates the creation of a secure lookup algorithm by building a list of replacement values based on the existing unique values in the target column and creating a secure lookup using those values. In that respect, it is simply shuffling the values.\n\n\n\n\nManaging a Database inventory\n\u00b6\n\n\nThe following sections apply to databases.\n\n\nSetting Column Criteria for a Table\n\u00b6\n\n\nNote:\n\n    You must select a database from the Select Rule Set drop-down menu on the left, not a file system.\n\n\nTo set criteria for sensitive columns:\n1. Click the green edit icon to the right of a column's name.\n2. To mask the selected column, select the \nMask\n check box.\n\n\n\n\n\n\nIf you do not want to mask this column, clear this check box.\n\n\n\n\n\n\nFrom the Domain drop-down menu, select the appropriate sensitive data element type for the column.\n\n\n\n\n\n\nThe Delphix masking engine defaults to a \nMasking Algorithm\n as specified in the Settings screen. If necessary, you can override the default algorithm for a column.\n\n\n\n\n\n\nTo select a different masking algorithm, choose one from the Algorithm dropdown.\n\n\n\n\nTo identify a custom algorithm, a text as custom in parenthesis will appear with the algorithm name.\n\n\n\n\nFor detailed descriptions of these algorithms, please see [Securing Sensitive Data  - Configuring Your Own Algorithms].\n\n\nIf you select a DATESHIFT algorithm and you are not masking a datetime or timestamp column, you must specify a \nDate Format\n. (This field only appears if you select a DATESHIFT algorithm from the Masking Algorithm dropdown.) For a list of acceptable formats, click the \nHelp\n link for Date Format. The default format is yyyy-MM-dd.\n\n\n\n\nSelect the Row Type according to its purpose, using \"All Row\" as a convention for all rows.\n If you need to create a row type (for example, if filter conditions are required), see Row Types and Creating New Row Types for a Table next.\n\n\n\n\nSelect an ID Method:\n\n\n\n\n\n\nAuto\n \u2014 The default value. The profiling job can determine or update whether to mask a column.\n\n\n\n\n\n\nUser\n \u2014 The user decides whether to mask/unmask a column. The user's choice overrides the profiling job. (The user masking is done after the profiling job is finished.)\n\n\n\n\n\n\nYou can add/remove notes in the \nNotes\n text field.\n\n\n\n\nWhen you are finished, click \nSave\n.\n You must click Save for any edits to take effect.\n\n\n\n\nCreating a New Row Type\n\u00b6\n\n\n\n\nFrom an Environment's Inventory tab, click +Row Types in the upper right.\nThe \nRow Type\n window appears, listing existing row types.\n\n\nClick \n+ Add a Row Type\n. The \nAdd Row Type\n window appears.\n\n\nName the \nRow Type\n according to its purpose. For example, if you want to subset the rows to only take rows with addresses, you can name this row type \"Address Rows\".\n\n\nTo limit the masking to a subset of rows, specify an appropriate \nWhere Clause\n.\n\n\nClick \nSave\n.\n\n\n\n\nManaging a File Inventory\n\u00b6\n\n\nSetting Field Criteria for a file\n\u00b6\n\n\nTo set criteria for sensitive fields:\n\n\n\n\nFrom an Environment's Inventory tab, click the green edit icon to the right of the field you want.\n\n\nTo mask this field, check the \nMask\n check box (in the View Inventory pane).\n\n\nClear this check box if you do not want to mask this field.\n\n\nChoose the appropriate sensitive data element type for the field from the \nDomain\n drop-down.\n\n\nDelphix defaults to a masking \nAlgorithm\n as specified in the Settings screen. If necessary, you may override the default algorithm for a field.\n\n\nTo select a different masking algorithm, choose one from the \nAlgorithm\n drop-down.\n\n\nChoose a \nRecord Type\n from the drop-down.\n\n\nClick \nSave\n when you are finished.\n\n\n\n\nNote:\n\n    You must click Save for any edits to take effect.\n\n\nDefining fields\n\u00b6\n\n\nTo create new fields:\n1. From an Environment's Inventory tab, click \nDefine fields\n to the far right.\nThe Edit Fields window appears.\n\n\n\n\n\n\nEdit the fields as described in \nSetting Field Criteria for a File\n.\n\n\nWhen you are finished, click \nNew\n to create a new field, or click \nSave\n to update an existing field.\n\n\n\n\nAdding Record Types for files\n\u00b6\n\n\nTo add a new Record Format:\n1. In the upper right-hand corner of an environment's \nInventory\n tab, click \nRecord Types\n. The Record Type window appears.\n2. Click \n+Add a Record Type\n towards the bottom of the window. The Add Record Type window appears.\n3. Enter values for the following fields:\n\n\n\n\nRecord Type Name\n \u2014 A free-form name for this record format.\n\n\nHeader/Body/Trailer\n \u2014 If the file has header or trailer records, you will need to create file formats for them. Select the appropriate type. Delphix allows for masking of multiple headers, multiple trailers, and multiple types of body records.\n\n\nRecord Type ID\n \u2014 (optional) For body records, specify the value of the record type code or other identifier that allows Delphix to identify records that qualify as this record type.\n\n\nPosition #\n \u2014 (optional) Specify the field number (for delimited files) or the character position number (for fixed files) of the beginning of the Record Type Identifier within the data record.\n\n\n\n\nLength #\n \u2014 (optional) For fixed files, specify the length of the Record Type Identifier within the data record.\n\n\n\n\n\n\nClick \nSave\n when you are finished.\n\n\n\n\n\n\nRedefine Conditions\n\u00b6\n\n\nFor Mainframe files, the inventory also allows for the entry of Redefine Conditions, which are used to handle any occurrences of COBOL's REDEFINES construct that might appear in the Copybook. In COBOL, the REDEFINES keyword allows an area of a record to be interpreted in multiple different ways. In the example below, for instance, each record can hold either the details of a person (PERSON-DET) or the details of a company (COMP-DET).\n\n\n\n\nDepending on which group is present, different masking algorithms may need to be applied. Below is the inventory corresponding to this copybook, which allows algorithms to be selected separately for each group.\n\n\n\n\nIn order to do any masking however, the masking engine must be able to determine, for each record, which fields should be read, so that the correct algorithms can be applied. In order to do this, the masking engine uses Redefine Conditions, which are specified in the inventory. Redefine Conditions are boolean expressions which can reference any fields in the record when they are evaluated.\n\n\nIn the example copybook above, the field CUST-TYPE is used to indicate which group is present. If CUST-TYPE holds a 'P', a PERSON-DET group is present, and if it holds a 'C', COMP-DET is present. This can be expressed in the inventory by specifying a Redefine Condition with the value [CUST-TYPE]='P' . This expression indicates that, for each record read from the source file during the masking job, the value of the field CUST-TYPE should be read and compared against the string 'P'. If it is equal, the masking engine will read from the record the fields subordinate to PERSON-DET, and will apply any masking algorithms specified on those fields. Similarly, a Redefine Condition with the value [CUST-TYPE]='C' should be applied to the COMP-DET field.\nExactly one of the conditions should evaluate to 'true' for each group of redefined fields. For example, a copybook might have fields A, B REDEFINES A, and C REDEFINES A. Of the Redefine Conditions attached to A, B, and C, one and only one should evaluate to true for each record.\n\n\nEntering a Redefine Condition\n\u00b6\n\n\n\n\nClick on the orange \nREDEFINED\n or \nREDEF\n button next to the redefined or redefining field\n\n\nEnter a condition in the dialog box which appears. This is the expression, which, when it evaluates to true, causes the subordinate fields to be read and, if they have algorithms assigned, masked.\n\n\n\n\n\n\n\n\nClick \nSubmit\n.\n\n\n\n\nFormat of Redefine Conditions\n\u00b6\n\n\nRedefine Conditions allow fields to be compared against either number or string literals. Square brackets enclosing a field name indicate a variable, which takes on the value of the named field:\n\n\n[Field1] = 'An example String'\n\n\n\n\n\nString literals can be enclosed in either single or double quotes. For fields that are numeric (e.g. PIC S99V9), the operators <, <=, >, and >= can be used in addition to the =operator, e.g.\n\n\n[Field2] <= -10.5\n\n\n\n\n\nAlso, conditions can be joined using AND, OR, and NOT to form more complex conditions:\n\n\n([Field3] > 2.5 AND [Field3] < 10) OR NOT [FIELD4] = 'Z'\n\n\n\n\n\nImporting and Exporting an Inventory\n\u00b6\n\n\nTo export an inventory\n:\n1. Click the \nExport\n icon at the upper right. The Export Inventory popup appears with the name of the currently selected Rule Set as the Inventory Name and a corresponding .csv \nFile Name\n.\n2. Click \nSave\n.\n\n\nA status popup appears. When the export operation is complete, you can click on the \nDownload file\n name to access the inventory file\n\n\nTo import an inventory\n:\n\n\n\n\nIn the upper right-hand corner, click the \nImport\n icon. The Import Inventory popup appears.\n\n\nClick \nSelect\n to browse for the name of a comma-separated (.csv) file.\n\n\nClick \nSave\n.\n\n\n\n\nThe inventory you imported appears in the Rule Set list for this environment.\n\n\n\n\nInfo\n\n\n\n\nThe format of an imported.csv file must exactly match the format of the exported inventory. If you plan to import an inventory, before importing the inventory, you should export it and then update the exported file as needed before you import it.",
            "title": "Managing Inventories"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#managing-inventories",
            "text": "An inventory describes all of the data present in a particular data source and defines the methods which will be used to secure it. Inventories typically include the table or file name, column/field name, the data classification, and the chosen algorithm.",
            "title": "Managing Inventories"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#the-inventory-screen",
            "text": "From anywhere within an environment, click the  Inventory  tab to see the Inventory Screen. This displays the inventory for the environment's rule sets.",
            "title": "The Inventory Screen"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#inventory-settings",
            "text": "To specify your inventory settings:   On the left-hand side of the screen, select a  Rule Set  from the drop-down menu.  Below this,  Contents  lists all the tables or files defined for the rule set.      Select a  table  or  file  for which you want to create or edit the inventory of sensitive data.\nThe  Columns  or  Fields  for that specific table or file appear.    If a column is a primary key (PK), Foreign Key (FK), or index (IDX), an icon indicating this will appear to the Right of the column name. If there is a note for the column, a Note icon will appear. To read the note, click the icon.   If an algorithm associated with a column is a custom algorithm (formerly known as Mapplet) then ( custom ) in red text will appear after the algorithm name.  If you selected a table, metadata for the column appears:  Data Type  and  Length  (in parentheses). This information is read-only.   Choose how you would like to view the inventory:    All Fields  \u2014 Displays all columns in the table or all fields in the file (allowing you to mark new columns or fields to be masked).    Masked Fields  \u2014 Filters the list to just those columns or fields that are already marked for masking.    Choose how to determine whether to mask/unmask a column:    Auto  \u2014 The default value. The profiling job can determine or update the algorithm assigned to a column and whether to mask the column.   User  \u2014 The user's choice overrides the profiling job. The user manually updates the algorithm assignment, mask/unmask option of the column. The Profiler will ignore the column, so it will not be updated as part of the Profiling job. In order to use the Secure Lookup algorithm, the user would select it as a user-defined algorithm and assign it to the specific column. Secure Lookup automates the creation of a secure lookup algorithm by building a list of replacement values based on the existing unique values in the target column and creating a secure lookup using those values. In that respect, it is simply shuffling the values.",
            "title": "Inventory Settings"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#managing-a-database-inventory",
            "text": "The following sections apply to databases.",
            "title": "Managing a Database inventory"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#setting-column-criteria-for-a-table",
            "text": "Note: \n    You must select a database from the Select Rule Set drop-down menu on the left, not a file system.  To set criteria for sensitive columns:\n1. Click the green edit icon to the right of a column's name.\n2. To mask the selected column, select the  Mask  check box.    If you do not want to mask this column, clear this check box.    From the Domain drop-down menu, select the appropriate sensitive data element type for the column.    The Delphix masking engine defaults to a  Masking Algorithm  as specified in the Settings screen. If necessary, you can override the default algorithm for a column.    To select a different masking algorithm, choose one from the Algorithm dropdown.   To identify a custom algorithm, a text as custom in parenthesis will appear with the algorithm name.   For detailed descriptions of these algorithms, please see [Securing Sensitive Data  - Configuring Your Own Algorithms].  If you select a DATESHIFT algorithm and you are not masking a datetime or timestamp column, you must specify a  Date Format . (This field only appears if you select a DATESHIFT algorithm from the Masking Algorithm dropdown.) For a list of acceptable formats, click the  Help  link for Date Format. The default format is yyyy-MM-dd.   Select the Row Type according to its purpose, using \"All Row\" as a convention for all rows.\n If you need to create a row type (for example, if filter conditions are required), see Row Types and Creating New Row Types for a Table next.   Select an ID Method:    Auto  \u2014 The default value. The profiling job can determine or update whether to mask a column.    User  \u2014 The user decides whether to mask/unmask a column. The user's choice overrides the profiling job. (The user masking is done after the profiling job is finished.)    You can add/remove notes in the  Notes  text field.   When you are finished, click  Save .\n You must click Save for any edits to take effect.",
            "title": "Setting Column Criteria for a Table"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#creating-a-new-row-type",
            "text": "From an Environment's Inventory tab, click +Row Types in the upper right.\nThe  Row Type  window appears, listing existing row types.  Click  + Add a Row Type . The  Add Row Type  window appears.  Name the  Row Type  according to its purpose. For example, if you want to subset the rows to only take rows with addresses, you can name this row type \"Address Rows\".  To limit the masking to a subset of rows, specify an appropriate  Where Clause .  Click  Save .",
            "title": "Creating a New Row Type"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#managing-a-file-inventory",
            "text": "",
            "title": "Managing a File Inventory"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#setting-field-criteria-for-a-file",
            "text": "To set criteria for sensitive fields:   From an Environment's Inventory tab, click the green edit icon to the right of the field you want.  To mask this field, check the  Mask  check box (in the View Inventory pane).  Clear this check box if you do not want to mask this field.  Choose the appropriate sensitive data element type for the field from the  Domain  drop-down.  Delphix defaults to a masking  Algorithm  as specified in the Settings screen. If necessary, you may override the default algorithm for a field.  To select a different masking algorithm, choose one from the  Algorithm  drop-down.  Choose a  Record Type  from the drop-down.  Click  Save  when you are finished.   Note: \n    You must click Save for any edits to take effect.",
            "title": "Setting Field Criteria for a file"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#defining-fields",
            "text": "To create new fields:\n1. From an Environment's Inventory tab, click  Define fields  to the far right.\nThe Edit Fields window appears.    Edit the fields as described in  Setting Field Criteria for a File .  When you are finished, click  New  to create a new field, or click  Save  to update an existing field.",
            "title": "Defining fields"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#adding-record-types-for-files",
            "text": "To add a new Record Format:\n1. In the upper right-hand corner of an environment's  Inventory  tab, click  Record Types . The Record Type window appears.\n2. Click  +Add a Record Type  towards the bottom of the window. The Add Record Type window appears.\n3. Enter values for the following fields:   Record Type Name  \u2014 A free-form name for this record format.  Header/Body/Trailer  \u2014 If the file has header or trailer records, you will need to create file formats for them. Select the appropriate type. Delphix allows for masking of multiple headers, multiple trailers, and multiple types of body records.  Record Type ID  \u2014 (optional) For body records, specify the value of the record type code or other identifier that allows Delphix to identify records that qualify as this record type.  Position #  \u2014 (optional) Specify the field number (for delimited files) or the character position number (for fixed files) of the beginning of the Record Type Identifier within the data record.   Length #  \u2014 (optional) For fixed files, specify the length of the Record Type Identifier within the data record.    Click  Save  when you are finished.",
            "title": "Adding Record Types for files"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#redefine-conditions",
            "text": "For Mainframe files, the inventory also allows for the entry of Redefine Conditions, which are used to handle any occurrences of COBOL's REDEFINES construct that might appear in the Copybook. In COBOL, the REDEFINES keyword allows an area of a record to be interpreted in multiple different ways. In the example below, for instance, each record can hold either the details of a person (PERSON-DET) or the details of a company (COMP-DET).   Depending on which group is present, different masking algorithms may need to be applied. Below is the inventory corresponding to this copybook, which allows algorithms to be selected separately for each group.   In order to do any masking however, the masking engine must be able to determine, for each record, which fields should be read, so that the correct algorithms can be applied. In order to do this, the masking engine uses Redefine Conditions, which are specified in the inventory. Redefine Conditions are boolean expressions which can reference any fields in the record when they are evaluated.  In the example copybook above, the field CUST-TYPE is used to indicate which group is present. If CUST-TYPE holds a 'P', a PERSON-DET group is present, and if it holds a 'C', COMP-DET is present. This can be expressed in the inventory by specifying a Redefine Condition with the value [CUST-TYPE]='P' . This expression indicates that, for each record read from the source file during the masking job, the value of the field CUST-TYPE should be read and compared against the string 'P'. If it is equal, the masking engine will read from the record the fields subordinate to PERSON-DET, and will apply any masking algorithms specified on those fields. Similarly, a Redefine Condition with the value [CUST-TYPE]='C' should be applied to the COMP-DET field.\nExactly one of the conditions should evaluate to 'true' for each group of redefined fields. For example, a copybook might have fields A, B REDEFINES A, and C REDEFINES A. Of the Redefine Conditions attached to A, B, and C, one and only one should evaluate to true for each record.",
            "title": "Redefine Conditions"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#entering-a-redefine-condition",
            "text": "Click on the orange  REDEFINED  or  REDEF  button next to the redefined or redefining field  Enter a condition in the dialog box which appears. This is the expression, which, when it evaluates to true, causes the subordinate fields to be read and, if they have algorithms assigned, masked.     Click  Submit .",
            "title": "Entering a Redefine Condition"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#format-of-redefine-conditions",
            "text": "Redefine Conditions allow fields to be compared against either number or string literals. Square brackets enclosing a field name indicate a variable, which takes on the value of the named field:  [Field1] = 'An example String'  String literals can be enclosed in either single or double quotes. For fields that are numeric (e.g. PIC S99V9), the operators <, <=, >, and >= can be used in addition to the =operator, e.g.  [Field2] <= -10.5  Also, conditions can be joined using AND, OR, and NOT to form more complex conditions:  ([Field3] > 2.5 AND [Field3] < 10) OR NOT [FIELD4] = 'Z'",
            "title": "Format of Redefine Conditions"
        },
        {
            "location": "/Connecting_Data_to_Masking_Service/Managing_Inventories/#importing-and-exporting-an-inventory",
            "text": "To export an inventory :\n1. Click the  Export  icon at the upper right. The Export Inventory popup appears with the name of the currently selected Rule Set as the Inventory Name and a corresponding .csv  File Name .\n2. Click  Save .  A status popup appears. When the export operation is complete, you can click on the  Download file  name to access the inventory file  To import an inventory :   In the upper right-hand corner, click the  Import  icon. The Import Inventory popup appears.  Click  Select  to browse for the name of a comma-separated (.csv) file.  Click  Save .   The inventory you imported appears in the Rule Set list for this environment.   Info   The format of an imported.csv file must exactly match the format of the exported inventory. If you plan to import an inventory, before importing the inventory, you should export it and then update the exported file as needed before you import it.",
            "title": "Importing and Exporting an Inventory"
        },
        {
            "location": "/Identifying_Sensitive_Data/Discovering_Your_Sensitive_Data_-_Intro/",
            "text": "Discovering Your Sensitive Data\n\u00b6\n\n\nAfter connecting data to the masking service, the next step is to discover\nwhich of the data should be secured. This sensitive data discovery is\ndone using two different methods, column level profiling and data level\nprofiling.  \n\n\nColumn Level Profiling\n\nColumn level profiling uses regular expressions (regex) to scan the metadata (column \nnames) of the selected data sources. There are several dozen\npre-configured profile Expressions (like the one below) designed to\nidentify common sensitive data types (SSN, Name, Addresses, etc). You\nalso have the ability to write your own profile Expressions.  \n\n\nFirst Name Expression\n  <([A-Z][A-Z0-9]\n)\\b[^>]\n>(.*?)</\\1>\n\n\nData Level Profiling\n\nData level profiling also uses regex, but to scan the actual\ndata instead of the metadata. Similar to column level profiling, there\nare several dozen pre-configured Expressions (like the one below) and\nyou can add your own.  \n\n\nSocial Security Number Expression\n  <([A-Z][A-Z0-9]\n)\\b[^>]\n>(.*?)</\\1>\n\n\nFor both column and data level profiling, when a data item is identified as\nsensitive, Delphix recommends/assigns particular masking algorithms to be used\nwhen securing the data. The platform comes with several dozen\npre-configured algorithms which are recommended when the profiler finds\ncertain sensitive data.",
            "title": "Discovering Your Sensitive Data"
        },
        {
            "location": "/Identifying_Sensitive_Data/Discovering_Your_Sensitive_Data_-_Intro/#discovering-your-sensitive-data",
            "text": "After connecting data to the masking service, the next step is to discover\nwhich of the data should be secured. This sensitive data discovery is\ndone using two different methods, column level profiling and data level\nprofiling.    Column Level Profiling \nColumn level profiling uses regular expressions (regex) to scan the metadata (column \nnames) of the selected data sources. There are several dozen\npre-configured profile Expressions (like the one below) designed to\nidentify common sensitive data types (SSN, Name, Addresses, etc). You\nalso have the ability to write your own profile Expressions.    First Name Expression   <([A-Z][A-Z0-9] )\\b[^>] >(.*?)</\\1>  Data Level Profiling \nData level profiling also uses regex, but to scan the actual\ndata instead of the metadata. Similar to column level profiling, there\nare several dozen pre-configured Expressions (like the one below) and\nyou can add your own.    Social Security Number Expression   <([A-Z][A-Z0-9] )\\b[^>] >(.*?)</\\1>  For both column and data level profiling, when a data item is identified as\nsensitive, Delphix recommends/assigns particular masking algorithms to be used\nwhen securing the data. The platform comes with several dozen\npre-configured algorithms which are recommended when the profiler finds\ncertain sensitive data.",
            "title": "Discovering Your Sensitive Data"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/",
            "text": "Out of the Box Profiling Settings\n\u00b6\n\n\nThe Delphix Platform comes out of the box with over 50 profile\nExpressions to help you discover over 30 types (account numbers,\naddresses, etc.) of sensitive data.\n\n\nAccount Numbers\n\u00b6\n\n\nAn account number is the primary identifier for ownership of an account,\nwhether a vendor account, a checking or brokerage account, or a loan\naccount. An account number is used whether or not the identifier uses\nletters or numbers. Below are the profile Expressions Delphix uses to\nidentify account numbers:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nAccount Number\n\n\nACCOUNT_NO\n\n\nColumn\n\n\n(?>(acc(oun\\|n)?t)_?(num(ber)?\\|nbrjno)?)(?!\\w\\*(ID\\|type))\n\n\n\n\n\n\n\n\nPhysical Addresses\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify physical\naddresses:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nAddress\n\n\nADDRESS\n\n\nColumn\n\n\n^(?:(?!postalcode\\|city\\|state\\|country\\|email\\|(l\\|ln\\|lin\\|line)?_?2{1}\\|ID).)*addre?s?s?_?(?:(?!city\\|state\\|country\\|email|(l\\|ln\\|lin\\|line)?_?2{1}\\|ID).)*$\n\n\n\n\n\n\nStreet Address\n\n\nADDRESS\n\n\nColumn\n\n\n(?>(str(eet)?_?addre?s?s?\\|street))(?!\\w*(ID\\|type))\n\n\n\n\n\n\nData - Address\n\n\nADDRESS\n\n\nData\n\n\n(.*[\\s]+b(ou)?|(e)?v(ar)?d[\\d]*.*)\\|(.*[\\s]+st[.]?(reet)?[\\s]*.*)\\|(.*[\\s]+ave[.]?(nue)?[\\s]*.*)\\|(.*[\\s]+r(oa)?d[\\s]*.*)\\|(.*[\\s]+\\|(a)?n(e)?[\\s]*.*)\\|(.[\\s]+cir(cle)?[\\s]*.*1\n\n\n\n\n\n\nAddress Line2 - before\n\n\nADDRESS_LINE2\n\n\nColumn\n\n\n^(?:(?!email\\|ID).)*(l\\|ln\\|lin\\|line)?2{1}_?addre?s?s?(?:(?!email\\|ID).)*$\n\n\n\n\n\n\nAddress Line2 - after\n\n\nADDRESS_LINE2\n\n\nColumn\n\n\n^(?:(?!email\\|ID).)*addre?s?s?_?(l\\|ln\\|lin\\|line)?_?2{1}(?:(?!email\\|ID).)*$\n\n\n\n\n\n\nData - Address Line 2\n\n\nADDRESS_LINE2\n\n\nData\n\n\n(.*[\\s]*ap(ar)?t(ment)?[\\s]+.*)|(.*[\\s]*s(ui)?te[\\s]+.*)\\|(c(are)?[\\s]*[\\\\\\\\]?[/]?o(f)?[\\s]+.*)\n\n\n\n\n\n\n\n\nBeneficiary ID\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify beneficiary\nIDs:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nBeneficiary Number\n\n\nBENEFICIARY_NO\n\n\nColumn\n\n\n(?>(bene(ficiary)?)_?(num(ber)?|nbr\\|no))(?!\\w*ID)1\n\n\n\n\n\n\nBeneficiary ID\n\n\nBENEFICIARY_NO\n\n\nColumn\n\n\n(?>(bene(ficiary)?)_?id)\n\n\n\n\n\n\n\n\nBiometrics\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to biometric data:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nBiometric\n\n\nBIOMETRIC\n\n\nColumn\n\n\nbiometric\n\n\n\n\n\n\n\n\nCertificate ID\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify certificate\nIDs:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nCertificate Number\n\n\nCERTIFICATE_NO\n\n\nColumn\n\n\n(?>cert(ificate)?_?(num(ber)?\\|nbr\\|no\\|id))\n\n\n\n\n\n\nCertificate ID\n\n\nCERTIFICATE_NO\n\n\nColumn\n\n\n(?>cert(ificate)?_?id)\n\n\n\n\n\n\n\n\nCity\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify cities:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nCity\n\n\nCITY\n\n\nColumn\n\n\nci?ty(?!\\w*ID)\n\n\n\n\n\n\n\n\nCountry\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify countries:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nCountry\n\n\nCOUNTRY\n\n\nColumn\n\n\nc(ou)?nty(?!\\w*ID)\n\n\n\n\n\n\n\n\nCredit Card\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify credit cards:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nCard Number\n\n\nCREDIT CARD\n\n\nColumn\n\n\n(?>ca?rd_?(num(ber)?\\|nbr\\|no)?)(?!\\w*ID)\n\n\n\n\n\n\nCredit Card Number\n\n\nCREDIT CARD\n\n\nColumn\n\n\n(?>cre?di?t_?(ca?rd)?_?(num(ber)?\\|nbr\\|no)?)(?!\\w*ID)\n\n\n\n\n\n\nData - Credit Card\n\n\nCREDIT CARD\n\n\nData\n\n\n^(?:3[47][0-9]{13}|4[0-9]{12}(?:[0-9]{3})?(?:[0-9]{3})?\\|(?:5[1-5][0-9]{2}\\|222[1-9]\\|22[3-9][0-9]\\|2[3-6][0-9]{2}\\|27[01][0-9]\\|2720)[0-9]{12}\\|6(?:(011\\|5[0-9][0-9])[0-9]{2}\\|4[4-9][0-9]{3}\\|2212[6-9]\\|221[3-9][0-9]\\|22[2-8][0-9]{2}\\|229[0-1][0-9]|2292[0-5])[0-9]{10}?(?:[0-9]{3})?\\|3(?:0[0-5,9]\\|6[0-9])[0-9]{11}\\|3[89][0-9]{14}?(?:[0-9]{1,3})?)$\n\n\n\n\n\n\n\n\nCustomer Number\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify customer IDs:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nCustomer Number\n\n\nCUSTOMER_NUM\n\n\nColumn\n\n\n(?>(cu?st(omer\\|mr)?)_?(num(ber)?\\|nbr|no)?)(?!\\w*ID)\n\n\n\n\n\n\n\n\nDate of Birth\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify dates of\nbirth:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nBirth Date\n\n\nDOB\n\n\nColumn\n\n\n(?>(bi?rth)_?(date?\\|day\\|dt))(?!\\w*ID)\n\n\n\n\n\n\nBirth Date1\n\n\nDOB\n\n\nColumn\n\n\n(?>dob\\|dtofb\\|(day\\|date?\\|dt)_?(of)?_?(bi?rth))(?!\\w*ID)\n\n\n\n\n\n\nBirth Date2\n\n\nDOB\n\n\nColumn\n\n\n(?>b_?(date?\\|day))(?!\\w*ID)\n\n\n\n\n\n\nAdmission Date\n\n\nDOB\n\n\nColumn\n\n\n(?>(adm(it\\|ission)?)_?(date?\\|day\\|dt))(?!\\w*ID)\n\n\n\n\n\n\nTreatment Date\n\n\nDOB\n\n\nColumn\n\n\n(?>(tr(ea)?t(ment)?)_?(date?\\|day|dt))(?!\\w*ID)\n\n\n\n\n\n\nDischarge Date\n\n\nDOB\n\n\nColumn\n\n\n(?>(ds\\|disc(h\\|harge)?)_?(date?\\|day\\|dt))(?!\\w*ID)\n\n\n\n\n\n\n\n\nDriver License Number\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify driver\nlicense numbers:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nDrivers License Number\n\n\nDRIVING_LC\n\n\nColumn\n\n\n(?>(dri?v(e?rs?e?)?)_?(license|li?c)?_?(num(ber)?\\|nbr|no)?)(?!\\w*ID)\n\n\n\n\n\n\nDrivers License Number1\n\n\nDRIVING_LC\n\n\nColumn\n\n\n(^license$\\|(license\\|li?c)_?(num(ber)?\\|nbr\\|no))(?!\\w*ID)\n\n\n\n\n\n\n\n\nEmail\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify emails:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nEmail\n\n\nEMAIL\n\n\nColumn\n\n\n^(?:(?!invalid).)*email(?!\\w*ID)\n\n\n\n\n\n\nData - Email\n\n\nEMAIL\n\n\nColumn\n\n\n\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,6}\\b\n\n\n\n\n\n\n\n\nFirst Name\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify first names:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nFirst Name\n\n\nFIRST_NAME\n\n\nColumn\n\n\n(?>(fi?rst)_?(na?me?)\\|f_?name)(?!\\w*ID)\n\n\n\n\n\n\nMiddle Name\n\n\nFIRST_NAME\n\n\nColumn\n\n\n(?>(mid(dle)?)_?(na?me?)\\|m_?name)(?!\\w*ID)\n\n\n\n\n\n\n\n\nIP Address\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to IP addresses:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nIP Address\n\n\nIP ADDRESS\n\n\nColumn\n\n\n(?>(ip_?addre?s?s?))(?!\\w*(ID\\|type))\n\n\n\n\n\n\nData - IP Address\n\n\nIP ADDRESS\n\n\nData\n\n\n\\b(?:(?:25[0-5]\\|2[0-4][0-9]\\|1[0-9][0-9]\\|[1-9]?[0-9])\\.){3}(?:25[0-5]\\|2[0-4][0-9]\\|1[0-9][0-9]\\|[1-9]?[0-9])\\b\n\n\n\n\n\n\n\n\nLast Name\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify last names:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nLast Name\n\n\nLAST_NAME\n\n\nColumn\n\n\n^(?:(?!portal\\|ID).)*((la?st)_?(na?me?)\\|l_?name)(?:(?!portalname\\|ID).)*$\n\n\n\n\n\n\n\n\nPlate Number\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify plate\nnumbers:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nLicense Plate\n\n\nPLATE_NO\n\n\nColumn\n\n\n^(?:(?!template|ID|type).)*(license\\|li?c)?_?plate_?(num(ber)?\\|nbr\\|no)?(?:(?!template\\|ID\\|type).)*$\n\n\n\n\n\n\n\n\nPO Box Numbers\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify PO box\nnumbers:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nPO Box\n\n\nPO_BOX\n\n\nColumn\n\n\npo_?box\n\n\n\n\n\n\nData - PO Box\n\n\nPO_BOX\n\n\nData\n\n\npo box\\|p\\.o\\\n\n\n\n\n\n\n\n\nPrecinct\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify precincts:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nPrecinct\n\n\nPRECINCT\n\n\nColumn\n\n\n(>?precinct\\|prcnct)(?!\\w*ID)\n\n\n\n\n\n\n\n\nRecord Number\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify record\nnumbers:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nRecord Number\n\n\nRECORD_NO\n\n\nColumn\n\n\n(?>rec(ord)?_?(num(ber)?\\|nbr\\|no))(?!\\w*(ID\\|type))\n\n\n\n\n\n\n\n\nSchool Name\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify school names:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nSchool Name\n\n\nSCHOOL_NM\n\n\nColumn\n\n\n(?>school_?na?me?)(?!\\w*ID)\n\n\n\n\n\n\n\n\nSecurity Code\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify security\ncodes:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nSecurity Code\n\n\nSECURITY_CODE\n\n\nColumn\n\n\n(?>se?cu?r(i?ty?)?_?co?de?)(?!\\w*ID)\n\n\n\n\n\n\n\n\nSerial Number\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify serial\nnumbers:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nSerial Number\n\n\nSERIAL_NM\n\n\nColumn\n\n\n(?>(ser(ial)?)_?(num(ber)?\\|nbr|no))(?!\\w*ID)\n\n\n\n\n\n\n\n\nSignature\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify signatures:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nSignature\n\n\nSIGNATURE\n\n\nColumn\n\n\nsignature(?!\\w*(ID\\|type))\n\n\n\n\n\n\n\n\nSocial Security Number\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to social security\nnumbers:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nSocial Security Number\n\n\nSSN\n\n\nColumn\n\n\nssn(?!\\w*ID)\n\n\n\n\n\n\nData - SSN\n\n\nSSN\n\n\nData\n\n\n\\b(?!000)(?!666)[0-8]\\d{2}[- ](?!00)\\d{2}[- ](?!0000)\\d{4}\\b\n\n\n\n\n\n\n\n\nTax ID\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify tax IDs:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nTax ID Number\n\n\nTAX_ID\n\n\nColumn\n\n\ntin$\\|^tin\\|_tin\\|tin_\n\n\n\n\n\n\nTax ID Code or Number\n\n\nTAX_ID\n\n\nColumn\n\n\n(ta?x)_?(id(ent)?)?_?((co?de?)\\|(num(ber)?\\|nbr\\|no))?\n\n\n\n\n\n\n\n\nTelephone Number\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify telephone\nnumbers:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nTelphone or Contact Number\n\n\nTELEPHONE_NO\n\n\nColumn\n\n\n(?>((tele?)?phone)\\|(co?nta?ct\\|tel)_?(num(ber)?\\|nbr\\|no))(?!\\w*(ID\\|type))\n\n\n\n\n\n\nData - Phone Number\n\n\nTELEPHONE_NO\n\n\nData\n\n\n\\(?\\b[0-9]{3}\\)?[-. ]?[0-9]{3}[-. ]?[0-9]{4}\\b\n\n\n\n\n\n\nFax Number\n\n\nTELEPHONE_NO\n\n\nData\n\n\n(?>fax_?(num(ber)?\\|nbr\\|no)?)(?!\\w*(ID\\|type))\n\n\n\n\n\n\n\n\nVin Number\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify vin numbers:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nVehicle\n\n\nVIN_NO\n\n\nColumn\n\n\nvehicle\n\n\n\n\n\n\nVIN\n\n\nVIN_NO\n\n\nColumn\n\n\nvin$\\|^vin\\|_vin\\|vin_\n\n\n\n\n\n\n\n\nWeb Address\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify web\naddresses:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nWeb or URL Address\n\n\nWEB\n\n\nColumn\n\n\n(?>(url\\|web_?addre?s?s?))(?!\\w*(ID\\|type))\n\n\n\n\n\n\nData - Web Address\n\n\nWEB\n\n\nData\n\n\n\\b(?:(?:https?\\|ftp\\|file)://\\|www\\.\\|ftp\\.)[-A-Z0-9+&-@#/%=~_\\|$?!:,.]*[A-Z0-9+&-@#/%=~_\\|$]\n\n\n\n\n\n\n\n\nZIP Code\n\u00b6\n\n\nBelow are the profile Expressions Delphix uses to identify zip codes:\n\n\n\n\n\n\n\n\nExpression Name\n\n\nDomain\n\n\nExpression Level\n\n\nExpression\n\n\n\n\n\n\n\n\n\n\nzip or Postal Code\n\n\nZIP\n\n\nColumn\n\n\n(?>(zip\\|post(al)?)_?((co?de?)?4?))(?!\\w*ID)\n\n\n\n\n\n\nData - Zip Code\n\n\nZIP\n\n\nData\n\n\n1\\b([0-9]{5})-([0-9]{4})\\b",
            "title": "Out of the Box Profiling Settings"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#out-of-the-box-profiling-settings",
            "text": "The Delphix Platform comes out of the box with over 50 profile\nExpressions to help you discover over 30 types (account numbers,\naddresses, etc.) of sensitive data.",
            "title": "Out of the Box Profiling Settings"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#account-numbers",
            "text": "An account number is the primary identifier for ownership of an account,\nwhether a vendor account, a checking or brokerage account, or a loan\naccount. An account number is used whether or not the identifier uses\nletters or numbers. Below are the profile Expressions Delphix uses to\nidentify account numbers:     Expression Name  Domain  Expression Level  Expression      Account Number  ACCOUNT_NO  Column  (?>(acc(oun\\|n)?t)_?(num(ber)?\\|nbrjno)?)(?!\\w\\*(ID\\|type))",
            "title": "Account Numbers"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#physical-addresses",
            "text": "Below are the profile Expressions Delphix uses to identify physical\naddresses:     Expression Name  Domain  Expression Level  Expression      Address  ADDRESS  Column  ^(?:(?!postalcode\\|city\\|state\\|country\\|email\\|(l\\|ln\\|lin\\|line)?_?2{1}\\|ID).)*addre?s?s?_?(?:(?!city\\|state\\|country\\|email|(l\\|ln\\|lin\\|line)?_?2{1}\\|ID).)*$    Street Address  ADDRESS  Column  (?>(str(eet)?_?addre?s?s?\\|street))(?!\\w*(ID\\|type))    Data - Address  ADDRESS  Data  (.*[\\s]+b(ou)?|(e)?v(ar)?d[\\d]*.*)\\|(.*[\\s]+st[.]?(reet)?[\\s]*.*)\\|(.*[\\s]+ave[.]?(nue)?[\\s]*.*)\\|(.*[\\s]+r(oa)?d[\\s]*.*)\\|(.*[\\s]+\\|(a)?n(e)?[\\s]*.*)\\|(.[\\s]+cir(cle)?[\\s]*.*1    Address Line2 - before  ADDRESS_LINE2  Column  ^(?:(?!email\\|ID).)*(l\\|ln\\|lin\\|line)?2{1}_?addre?s?s?(?:(?!email\\|ID).)*$    Address Line2 - after  ADDRESS_LINE2  Column  ^(?:(?!email\\|ID).)*addre?s?s?_?(l\\|ln\\|lin\\|line)?_?2{1}(?:(?!email\\|ID).)*$    Data - Address Line 2  ADDRESS_LINE2  Data  (.*[\\s]*ap(ar)?t(ment)?[\\s]+.*)|(.*[\\s]*s(ui)?te[\\s]+.*)\\|(c(are)?[\\s]*[\\\\\\\\]?[/]?o(f)?[\\s]+.*)",
            "title": "Physical Addresses"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#beneficiary-id",
            "text": "Below are the profile Expressions Delphix uses to identify beneficiary\nIDs:     Expression Name  Domain  Expression Level  Expression      Beneficiary Number  BENEFICIARY_NO  Column  (?>(bene(ficiary)?)_?(num(ber)?|nbr\\|no))(?!\\w*ID)1    Beneficiary ID  BENEFICIARY_NO  Column  (?>(bene(ficiary)?)_?id)",
            "title": "Beneficiary ID"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#biometrics",
            "text": "Below are the profile Expressions Delphix uses to biometric data:     Expression Name  Domain  Expression Level  Expression      Biometric  BIOMETRIC  Column  biometric",
            "title": "Biometrics"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#certificate-id",
            "text": "Below are the profile Expressions Delphix uses to identify certificate\nIDs:     Expression Name  Domain  Expression Level  Expression      Certificate Number  CERTIFICATE_NO  Column  (?>cert(ificate)?_?(num(ber)?\\|nbr\\|no\\|id))    Certificate ID  CERTIFICATE_NO  Column  (?>cert(ificate)?_?id)",
            "title": "Certificate ID"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#city",
            "text": "Below are the profile Expressions Delphix uses to identify cities:     Expression Name  Domain  Expression Level  Expression      City  CITY  Column  ci?ty(?!\\w*ID)",
            "title": "City"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#country",
            "text": "Below are the profile Expressions Delphix uses to identify countries:     Expression Name  Domain  Expression Level  Expression      Country  COUNTRY  Column  c(ou)?nty(?!\\w*ID)",
            "title": "Country"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#credit-card",
            "text": "Below are the profile Expressions Delphix uses to identify credit cards:     Expression Name  Domain  Expression Level  Expression      Card Number  CREDIT CARD  Column  (?>ca?rd_?(num(ber)?\\|nbr\\|no)?)(?!\\w*ID)    Credit Card Number  CREDIT CARD  Column  (?>cre?di?t_?(ca?rd)?_?(num(ber)?\\|nbr\\|no)?)(?!\\w*ID)    Data - Credit Card  CREDIT CARD  Data  ^(?:3[47][0-9]{13}|4[0-9]{12}(?:[0-9]{3})?(?:[0-9]{3})?\\|(?:5[1-5][0-9]{2}\\|222[1-9]\\|22[3-9][0-9]\\|2[3-6][0-9]{2}\\|27[01][0-9]\\|2720)[0-9]{12}\\|6(?:(011\\|5[0-9][0-9])[0-9]{2}\\|4[4-9][0-9]{3}\\|2212[6-9]\\|221[3-9][0-9]\\|22[2-8][0-9]{2}\\|229[0-1][0-9]|2292[0-5])[0-9]{10}?(?:[0-9]{3})?\\|3(?:0[0-5,9]\\|6[0-9])[0-9]{11}\\|3[89][0-9]{14}?(?:[0-9]{1,3})?)$",
            "title": "Credit Card"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#customer-number",
            "text": "Below are the profile Expressions Delphix uses to identify customer IDs:     Expression Name  Domain  Expression Level  Expression      Customer Number  CUSTOMER_NUM  Column  (?>(cu?st(omer\\|mr)?)_?(num(ber)?\\|nbr|no)?)(?!\\w*ID)",
            "title": "Customer Number"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#date-of-birth",
            "text": "Below are the profile Expressions Delphix uses to identify dates of\nbirth:     Expression Name  Domain  Expression Level  Expression      Birth Date  DOB  Column  (?>(bi?rth)_?(date?\\|day\\|dt))(?!\\w*ID)    Birth Date1  DOB  Column  (?>dob\\|dtofb\\|(day\\|date?\\|dt)_?(of)?_?(bi?rth))(?!\\w*ID)    Birth Date2  DOB  Column  (?>b_?(date?\\|day))(?!\\w*ID)    Admission Date  DOB  Column  (?>(adm(it\\|ission)?)_?(date?\\|day\\|dt))(?!\\w*ID)    Treatment Date  DOB  Column  (?>(tr(ea)?t(ment)?)_?(date?\\|day|dt))(?!\\w*ID)    Discharge Date  DOB  Column  (?>(ds\\|disc(h\\|harge)?)_?(date?\\|day\\|dt))(?!\\w*ID)",
            "title": "Date of Birth"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#driver-license-number",
            "text": "Below are the profile Expressions Delphix uses to identify driver\nlicense numbers:     Expression Name  Domain  Expression Level  Expression      Drivers License Number  DRIVING_LC  Column  (?>(dri?v(e?rs?e?)?)_?(license|li?c)?_?(num(ber)?\\|nbr|no)?)(?!\\w*ID)    Drivers License Number1  DRIVING_LC  Column  (^license$\\|(license\\|li?c)_?(num(ber)?\\|nbr\\|no))(?!\\w*ID)",
            "title": "Driver License Number"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#email",
            "text": "Below are the profile Expressions Delphix uses to identify emails:     Expression Name  Domain  Expression Level  Expression      Email  EMAIL  Column  ^(?:(?!invalid).)*email(?!\\w*ID)    Data - Email  EMAIL  Column  \\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,6}\\b",
            "title": "Email"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#first-name",
            "text": "Below are the profile Expressions Delphix uses to identify first names:     Expression Name  Domain  Expression Level  Expression      First Name  FIRST_NAME  Column  (?>(fi?rst)_?(na?me?)\\|f_?name)(?!\\w*ID)    Middle Name  FIRST_NAME  Column  (?>(mid(dle)?)_?(na?me?)\\|m_?name)(?!\\w*ID)",
            "title": "First Name"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#ip-address",
            "text": "Below are the profile Expressions Delphix uses to IP addresses:     Expression Name  Domain  Expression Level  Expression      IP Address  IP ADDRESS  Column  (?>(ip_?addre?s?s?))(?!\\w*(ID\\|type))    Data - IP Address  IP ADDRESS  Data  \\b(?:(?:25[0-5]\\|2[0-4][0-9]\\|1[0-9][0-9]\\|[1-9]?[0-9])\\.){3}(?:25[0-5]\\|2[0-4][0-9]\\|1[0-9][0-9]\\|[1-9]?[0-9])\\b",
            "title": "IP Address"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#last-name",
            "text": "Below are the profile Expressions Delphix uses to identify last names:     Expression Name  Domain  Expression Level  Expression      Last Name  LAST_NAME  Column  ^(?:(?!portal\\|ID).)*((la?st)_?(na?me?)\\|l_?name)(?:(?!portalname\\|ID).)*$",
            "title": "Last Name"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#plate-number",
            "text": "Below are the profile Expressions Delphix uses to identify plate\nnumbers:     Expression Name  Domain  Expression Level  Expression      License Plate  PLATE_NO  Column  ^(?:(?!template|ID|type).)*(license\\|li?c)?_?plate_?(num(ber)?\\|nbr\\|no)?(?:(?!template\\|ID\\|type).)*$",
            "title": "Plate Number"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#po-box-numbers",
            "text": "Below are the profile Expressions Delphix uses to identify PO box\nnumbers:     Expression Name  Domain  Expression Level  Expression      PO Box  PO_BOX  Column  po_?box    Data - PO Box  PO_BOX  Data  po box\\|p\\.o\\",
            "title": "PO Box Numbers"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#precinct",
            "text": "Below are the profile Expressions Delphix uses to identify precincts:     Expression Name  Domain  Expression Level  Expression      Precinct  PRECINCT  Column  (>?precinct\\|prcnct)(?!\\w*ID)",
            "title": "Precinct"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#record-number",
            "text": "Below are the profile Expressions Delphix uses to identify record\nnumbers:     Expression Name  Domain  Expression Level  Expression      Record Number  RECORD_NO  Column  (?>rec(ord)?_?(num(ber)?\\|nbr\\|no))(?!\\w*(ID\\|type))",
            "title": "Record Number"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#school-name",
            "text": "Below are the profile Expressions Delphix uses to identify school names:     Expression Name  Domain  Expression Level  Expression      School Name  SCHOOL_NM  Column  (?>school_?na?me?)(?!\\w*ID)",
            "title": "School Name"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#security-code",
            "text": "Below are the profile Expressions Delphix uses to identify security\ncodes:     Expression Name  Domain  Expression Level  Expression      Security Code  SECURITY_CODE  Column  (?>se?cu?r(i?ty?)?_?co?de?)(?!\\w*ID)",
            "title": "Security Code"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#serial-number",
            "text": "Below are the profile Expressions Delphix uses to identify serial\nnumbers:     Expression Name  Domain  Expression Level  Expression      Serial Number  SERIAL_NM  Column  (?>(ser(ial)?)_?(num(ber)?\\|nbr|no))(?!\\w*ID)",
            "title": "Serial Number"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#signature",
            "text": "Below are the profile Expressions Delphix uses to identify signatures:     Expression Name  Domain  Expression Level  Expression      Signature  SIGNATURE  Column  signature(?!\\w*(ID\\|type))",
            "title": "Signature"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#social-security-number",
            "text": "Below are the profile Expressions Delphix uses to social security\nnumbers:     Expression Name  Domain  Expression Level  Expression      Social Security Number  SSN  Column  ssn(?!\\w*ID)    Data - SSN  SSN  Data  \\b(?!000)(?!666)[0-8]\\d{2}[- ](?!00)\\d{2}[- ](?!0000)\\d{4}\\b",
            "title": "Social Security Number"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#tax-id",
            "text": "Below are the profile Expressions Delphix uses to identify tax IDs:     Expression Name  Domain  Expression Level  Expression      Tax ID Number  TAX_ID  Column  tin$\\|^tin\\|_tin\\|tin_    Tax ID Code or Number  TAX_ID  Column  (ta?x)_?(id(ent)?)?_?((co?de?)\\|(num(ber)?\\|nbr\\|no))?",
            "title": "Tax ID"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#telephone-number",
            "text": "Below are the profile Expressions Delphix uses to identify telephone\nnumbers:     Expression Name  Domain  Expression Level  Expression      Telphone or Contact Number  TELEPHONE_NO  Column  (?>((tele?)?phone)\\|(co?nta?ct\\|tel)_?(num(ber)?\\|nbr\\|no))(?!\\w*(ID\\|type))    Data - Phone Number  TELEPHONE_NO  Data  \\(?\\b[0-9]{3}\\)?[-. ]?[0-9]{3}[-. ]?[0-9]{4}\\b    Fax Number  TELEPHONE_NO  Data  (?>fax_?(num(ber)?\\|nbr\\|no)?)(?!\\w*(ID\\|type))",
            "title": "Telephone Number"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#vin-number",
            "text": "Below are the profile Expressions Delphix uses to identify vin numbers:     Expression Name  Domain  Expression Level  Expression      Vehicle  VIN_NO  Column  vehicle    VIN  VIN_NO  Column  vin$\\|^vin\\|_vin\\|vin_",
            "title": "Vin Number"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#web-address",
            "text": "Below are the profile Expressions Delphix uses to identify web\naddresses:     Expression Name  Domain  Expression Level  Expression      Web or URL Address  WEB  Column  (?>(url\\|web_?addre?s?s?))(?!\\w*(ID\\|type))    Data - Web Address  WEB  Data  \\b(?:(?:https?\\|ftp\\|file)://\\|www\\.\\|ftp\\.)[-A-Z0-9+&-@#/%=~_\\|$?!:,.]*[A-Z0-9+&-@#/%=~_\\|$]",
            "title": "Web Address"
        },
        {
            "location": "/Identifying_Sensitive_Data/Out_of_the_Box_Profiling_Settings/#zip-code",
            "text": "Below are the profile Expressions Delphix uses to identify zip codes:     Expression Name  Domain  Expression Level  Expression      zip or Postal Code  ZIP  Column  (?>(zip\\|post(al)?)_?((co?de?)?4?))(?!\\w*ID)    Data - Zip Code  ZIP  Data  1\\b([0-9]{5})-([0-9]{4})\\b",
            "title": "ZIP Code"
        },
        {
            "location": "/Identifying_Sensitive_Data/Managing_Domains/",
            "text": "Managing Domains\n\u00b6\n\n\nThis section describes how you can create and manage your domains.\n\n\nDomains specify certain data to be masked with a certain algorithm. From the \nSettings\n tab, if you click \nDomains\n to the left, the list of domains will be displayed. From here, you can add, edit, or delete domains.\n\n\nDelphix Agile Data Masking includes several default domains and algorithms. These appear the first time you display the Masking Settings tab. Each domain has a classification and masking method assigned to it. You might choose to assign a different algorithm to a domain, but each domain name is unique and can only be associated with one algorithm. If you create additional algorithms, they will appear in the \nAlgorithms\n drop-down menu. Because each algorithm you use must have a unique domain, you must add a domain (or reassign an existing domain) to use any other algorithms.\n\n\nThe \nDomains\n tab is where you define domains, along with their classification and the default Masking Algorithm.\n\n\n\n\nAdding a New Domain\n\u00b6\n\n\n\n\nAt the top of the \nDomains\n tab, click \nAdd Domain\n.\n\n\nEnter the new \nDomain Name\n. The domain name you specify will appear as a menu option on the \nInventory\n screen elsewhere in the Delphix Masking Engine. Domain names must be unique.\n\n\nSelect the \nClassification\n (informational only). For example, customer-facing data, employee data, or company data.\n\n\nSelect a default \nMasking Algorithm\n for the new domain. \n\n\nClick \nSave\n.\nTo delete any domain, click the Delete icon to the far right of the domain name.",
            "title": "Managing Domains"
        },
        {
            "location": "/Identifying_Sensitive_Data/Managing_Domains/#managing-domains",
            "text": "This section describes how you can create and manage your domains.  Domains specify certain data to be masked with a certain algorithm. From the  Settings  tab, if you click  Domains  to the left, the list of domains will be displayed. From here, you can add, edit, or delete domains.  Delphix Agile Data Masking includes several default domains and algorithms. These appear the first time you display the Masking Settings tab. Each domain has a classification and masking method assigned to it. You might choose to assign a different algorithm to a domain, but each domain name is unique and can only be associated with one algorithm. If you create additional algorithms, they will appear in the  Algorithms  drop-down menu. Because each algorithm you use must have a unique domain, you must add a domain (or reassign an existing domain) to use any other algorithms.  The  Domains  tab is where you define domains, along with their classification and the default Masking Algorithm.",
            "title": "Managing Domains"
        },
        {
            "location": "/Identifying_Sensitive_Data/Managing_Domains/#adding-a-new-domain",
            "text": "At the top of the  Domains  tab, click  Add Domain .  Enter the new  Domain Name . The domain name you specify will appear as a menu option on the  Inventory  screen elsewhere in the Delphix Masking Engine. Domain names must be unique.  Select the  Classification  (informational only). For example, customer-facing data, employee data, or company data.  Select a default  Masking Algorithm  for the new domain.   Click  Save .\nTo delete any domain, click the Delete icon to the far right of the domain name.",
            "title": "Adding a New Domain"
        },
        {
            "location": "/Identifying_Sensitive_Data/Configuring_Profiling_Settings/",
            "text": "Configuring Profiling Settings\n\u00b6\n\n\nIn addition to using your Rule Set to determine the\ninventory of what to profile, a Profiling job uses Expressions\nto identify your sensitive data.  You can add regular expressions\nto be used by Profiler Sets to the Profiler Settings.\n\n\nTo display the Profiler Settings, click on the \nSettings\n tab and\nselect \nProfiler\n on the left-hand side of the page. \n\n\n\n\nThe \nProfiler Settings\n screen displays Expressions along with their \nDomain\n,\n\nExpression\n text, Expression \nName\n, \nOwner\n, and Expression profiling \nLevel\n.\n\n\nTo add an Expression\n\u00b6\n\n\n\n\nClick \nAdd Expression\n at the top of the Profiler screen.\n\n\n\n\n\n\n\n\n\n\nSelect a Domain from the \nDomain\n dropdown. \n\n\n\n\nDomains are used by Profiling jobs to determine the masking Algorithm to apply\n    to your sensitive data.  When an Expression is matched, the Profiling job will\n    associate the specified Domain to the sensitive data. The Masking Engine comes \n    out of the box with over 30 pre-defined Domains. Domains can be added, edited,\n    and deleted from the \nSettings Domains\n screen. \n\n\n\n\n\n\n\n\nEnter the following information for the Expression:\n\n\n\n\n\n\nExpression Name\n\u2014 The name used to select this\n    expression as part of a Profiler Set.\n\n\n\n\n\n\nExpression Text\n\u2014 The regular expression used to identify\n    the sensitive data.\n\n\n\n\n\n\n\n\n\n\nSelect an \nExpression Level\n for the Expression:\n\n\n\n\n\n\nColumn Level\n\u2014 To identify sensitive data based on column\n    names.\n\n\n\n\n\n\nData Level\n\u2014 To identify sensitive data based on data\n    values, not column names.\n\n\n\n\n\n\n\n\n\n\nWhen you are finished, click \nSave\n.\n\n\n\n\n\n\nTo edit a saved Expression, click the \nEdit\n icon to the right of the \nExpression.\n\n\nTo delete an Expression\n\u00b6\n\n\nClick the \nDelete\n icon to the far right of the name.\n\n\nProfiler Sets\n\u00b6\n\n\nProfiling jobs use Profiler Sets to determine the set of Expressions\nto use in identifying sensitive data in an Inventory. A Profiler Set\nis a grouping of Expressions for a particular purpose. For instance,\nFirst Name, Last Name, Address, Credit Card, SSN, and Bank Account\nNumber Expressions could constitute a Financial Profiler Set.\n\n\nThe Masking Engine comes with two predefined Profiler Sets: Financial and\nHealthcare vertical. A Delphix Masking Engine administrator (a user with\nthe appropriate role privileges) can create/add/update/delete these\nProfiler Sets.\n\n\nIf you want to edit or add a Profiler set, click \nProfiler Set\n at the\ntop of the \nProfiler Settings\n screen. The Profiler Set dialog appears, \nlisting the Profiler Sets along with their Purpose, Owner, and Date Created.\n\n\n\n\nTo add a Profiler Set\n\u00b6\n\n\n\n\nClick \nAdd Set\n at the top of dialog.\n\n\nEnter a Profiler \nSet Name\n.\n\n\nOptionally, enter a \nPurpose\n for this Profiler Set.\n\n\nEnter or select which \nExpressions\n to include in this set.\n\n\nWhen you are finished, cick \nSubmit\n.\n\n\n\n\nTo edit an existing Profiler Set, click the \nEdit\n icon to the right of the\nProfiler Set name.\n\n\nTo delete a Profiler Set\n\u00b6\n\n\nClick the \nDelete\n icon to the right of the Profiler Set name.",
            "title": "Configuring Profiling Settings"
        },
        {
            "location": "/Identifying_Sensitive_Data/Configuring_Profiling_Settings/#configuring-profiling-settings",
            "text": "In addition to using your Rule Set to determine the\ninventory of what to profile, a Profiling job uses Expressions\nto identify your sensitive data.  You can add regular expressions\nto be used by Profiler Sets to the Profiler Settings.  To display the Profiler Settings, click on the  Settings  tab and\nselect  Profiler  on the left-hand side of the page.    The  Profiler Settings  screen displays Expressions along with their  Domain , Expression  text, Expression  Name ,  Owner , and Expression profiling  Level .",
            "title": "Configuring Profiling Settings"
        },
        {
            "location": "/Identifying_Sensitive_Data/Configuring_Profiling_Settings/#to-add-an-expression",
            "text": "Click  Add Expression  at the top of the Profiler screen.      Select a Domain from the  Domain  dropdown.    Domains are used by Profiling jobs to determine the masking Algorithm to apply\n    to your sensitive data.  When an Expression is matched, the Profiling job will\n    associate the specified Domain to the sensitive data. The Masking Engine comes \n    out of the box with over 30 pre-defined Domains. Domains can be added, edited,\n    and deleted from the  Settings Domains  screen.      Enter the following information for the Expression:    Expression Name \u2014 The name used to select this\n    expression as part of a Profiler Set.    Expression Text \u2014 The regular expression used to identify\n    the sensitive data.      Select an  Expression Level  for the Expression:    Column Level \u2014 To identify sensitive data based on column\n    names.    Data Level \u2014 To identify sensitive data based on data\n    values, not column names.      When you are finished, click  Save .    To edit a saved Expression, click the  Edit  icon to the right of the \nExpression.",
            "title": "To add an Expression"
        },
        {
            "location": "/Identifying_Sensitive_Data/Configuring_Profiling_Settings/#to-delete-an-expression",
            "text": "Click the  Delete  icon to the far right of the name.",
            "title": "To delete an Expression"
        },
        {
            "location": "/Identifying_Sensitive_Data/Configuring_Profiling_Settings/#profiler-sets",
            "text": "Profiling jobs use Profiler Sets to determine the set of Expressions\nto use in identifying sensitive data in an Inventory. A Profiler Set\nis a grouping of Expressions for a particular purpose. For instance,\nFirst Name, Last Name, Address, Credit Card, SSN, and Bank Account\nNumber Expressions could constitute a Financial Profiler Set.  The Masking Engine comes with two predefined Profiler Sets: Financial and\nHealthcare vertical. A Delphix Masking Engine administrator (a user with\nthe appropriate role privileges) can create/add/update/delete these\nProfiler Sets.  If you want to edit or add a Profiler set, click  Profiler Set  at the\ntop of the  Profiler Settings  screen. The Profiler Set dialog appears, \nlisting the Profiler Sets along with their Purpose, Owner, and Date Created.",
            "title": "Profiler Sets"
        },
        {
            "location": "/Identifying_Sensitive_Data/Configuring_Profiling_Settings/#to-add-a-profiler-set",
            "text": "Click  Add Set  at the top of dialog.  Enter a Profiler  Set Name .  Optionally, enter a  Purpose  for this Profiler Set.  Enter or select which  Expressions  to include in this set.  When you are finished, cick  Submit .   To edit an existing Profiler Set, click the  Edit  icon to the right of the\nProfiler Set name.",
            "title": "To add a Profiler Set"
        },
        {
            "location": "/Identifying_Sensitive_Data/Configuring_Profiling_Settings/#to-delete-a-profiler-set",
            "text": "Click the  Delete  icon to the right of the Profiler Set name.",
            "title": "To delete a Profiler Set"
        },
        {
            "location": "/Identifying_Sensitive_Data/Creating_A_Profiling_Job/",
            "text": "Creating A Profiling Job\n\u00b6\n\n\nThis section describes how users can create a Profiling job. You can \ncreate Profiling jobs for databases, copybooks, delimited files, fixed-width, \nand Excel files.\n\n\nThe Profiler assigns each sensitive data element to a domain, with each domain\nhaving a default masking algorithm. Then, in the inventory, masking\nalgorithms can be manually updated as needed to establish the\nmasking rulesets for your data sources.\n\n\nProfiling Jobs are grouped within environments on the \nEnvironment Overview\n page \nalong with all masking jobs. In order to navigate to the \nOverview\n screen, click \non an environment and the \nOverview\n tab should automatically display. \n\n\n\n\nCreating a New Profiling Job\n\u00b6\n\n\nTo create a new Profiling job:\n\n\n\n\n\n\nClick the \nProfile\n button on the upper side of the page.\n\n\n\n\n\n\nThe \nCreate Profiling Job\n window appears.\n\n\n\n\n\n\n\n\nYou will be prompted for the following information:\n\n\n\n\n\n\nJob Name\n \u2014 A free-form name for the job you are creating.\n    Must be unique.\n\n\n\n\n\n\nMulti Tenant\n \u2014 Check the box if the job is for a\n    multi-tenant database. This option allows existing rulesets to\n    be re-used to mask identical schemas via different connectors.\n    The connector is selected at job execution time.\n\n\n\n\n\n\nRule Set\n \u2014 Select the rule set that this job will profile.\n\n\n\n\n\n\nNo. of Streams\n \u2014 The number of parallel streams to use\n    when running the jobs. For example, you can select two streams\n    to profile two tables in the ruleset concurrently in the job\n    instead of one table at a time.\n\n\n\n\n\n\nMin Memory (MB)\n \u2014 (optional) Minimum amount of memory to\n    allocate for the job, in megabytes.\n\n\n\n\n\n\nMax Memory (MB)\n \u2014 (optional) Maximum amount of memory to\n    allocate for the job, in megabytes.\n\n\n\n\n\n\nFeedback Size\n \u2014 (optional) The number of rows to process\n    before writing a message to the logs. Set this parameter to\n    the appropriate level of detail required for monitoring your\n    job. For example, if you set this number significantly higher\n    than the actual number of rows in a job, the progress for that\n    job will only show 0 or 100%.\n\n\n\n\n\n\nMultiple PHI\n - Check the box if the job should run all Profile\n     Expressions against the result set instead of finding the first\n     matching Profile Expression. With this option, the Profiler report\n     will indicate all matching Profile expressions, and if multiple Profile\n     Expressions match, will assign the default Multiple PHI masking\n     algorithm.\n\n\n\n\n\n\nProfile Sets\n \u2014 The name of the Profile Set to use.\n    A Profile Set is a set of Profile Expressions (for example, a set of\n    financial expressions). (See Delphix Administrator's Guide.)\n\n\n\n\n\n\nComments\n \u2014 (optional) Add comments related to this job.\n\n\n\n\n\n\nEmail\n \u2014 (optional) Add e-mail address(es) to which to send\n    status messages. Separate addresses with a comma (,).\n\n\n\n\n\n\n\n\n\n\nWhen you are finished, click \nSave\n.",
            "title": "Creating A Profiling Job"
        },
        {
            "location": "/Identifying_Sensitive_Data/Creating_A_Profiling_Job/#creating-a-profiling-job",
            "text": "This section describes how users can create a Profiling job. You can \ncreate Profiling jobs for databases, copybooks, delimited files, fixed-width, \nand Excel files.  The Profiler assigns each sensitive data element to a domain, with each domain\nhaving a default masking algorithm. Then, in the inventory, masking\nalgorithms can be manually updated as needed to establish the\nmasking rulesets for your data sources.  Profiling Jobs are grouped within environments on the  Environment Overview  page \nalong with all masking jobs. In order to navigate to the  Overview  screen, click \non an environment and the  Overview  tab should automatically display.",
            "title": "Creating A Profiling Job"
        },
        {
            "location": "/Identifying_Sensitive_Data/Creating_A_Profiling_Job/#creating-a-new-profiling-job",
            "text": "To create a new Profiling job:    Click the  Profile  button on the upper side of the page.    The  Create Profiling Job  window appears.     You will be prompted for the following information:    Job Name  \u2014 A free-form name for the job you are creating.\n    Must be unique.    Multi Tenant  \u2014 Check the box if the job is for a\n    multi-tenant database. This option allows existing rulesets to\n    be re-used to mask identical schemas via different connectors.\n    The connector is selected at job execution time.    Rule Set  \u2014 Select the rule set that this job will profile.    No. of Streams  \u2014 The number of parallel streams to use\n    when running the jobs. For example, you can select two streams\n    to profile two tables in the ruleset concurrently in the job\n    instead of one table at a time.    Min Memory (MB)  \u2014 (optional) Minimum amount of memory to\n    allocate for the job, in megabytes.    Max Memory (MB)  \u2014 (optional) Maximum amount of memory to\n    allocate for the job, in megabytes.    Feedback Size  \u2014 (optional) The number of rows to process\n    before writing a message to the logs. Set this parameter to\n    the appropriate level of detail required for monitoring your\n    job. For example, if you set this number significantly higher\n    than the actual number of rows in a job, the progress for that\n    job will only show 0 or 100%.    Multiple PHI  - Check the box if the job should run all Profile\n     Expressions against the result set instead of finding the first\n     matching Profile Expression. With this option, the Profiler report\n     will indicate all matching Profile expressions, and if multiple Profile\n     Expressions match, will assign the default Multiple PHI masking\n     algorithm.    Profile Sets  \u2014 The name of the Profile Set to use.\n    A Profile Set is a set of Profile Expressions (for example, a set of\n    financial expressions). (See Delphix Administrator's Guide.)    Comments  \u2014 (optional) Add comments related to this job.    Email  \u2014 (optional) Add e-mail address(es) to which to send\n    status messages. Separate addresses with a comma (,).      When you are finished, click  Save .",
            "title": "Creating a New Profiling Job"
        },
        {
            "location": "/Identifying_Sensitive_Data/Running_A_Profiling_Job/",
            "text": "Running A Profiling Job\n\u00b6\n\n\nThis section describes how users can run a profiling job from the\n\nEnvironment Overview\n screen.\n\n\n\n\nTo run or rerun a job from the \nEnvironment Overview\n screen:\n\n\n\n\n\n\nClick the \nRun\n icon (play icon) in the Action column for the\n    desired job.\n\n\n\n\n\n\nThe \nRun\n icon changes to a \nStop\n icon while the job is\n    running.\n\n\n\n\n\n\nWhen the job is complete, the \nStatus\n changes.\n\n\n\n\n\n\nTo stop a running job from the \nEnvironment Overview\n screen:\n\n\n\n\n\n\nLocate the job you want to stop.\n\n\n\n\n\n\nIn the job's \nAction\n column, click the \nStop\n icon.\n\n\n\n\n\n\nA popup appears asking, \"Are you sure you want to stop job?\" Click\n    \nOK\n.\n\n\nWhen the job has been stopped, its status changes.",
            "title": "Running A Profiling Job"
        },
        {
            "location": "/Identifying_Sensitive_Data/Running_A_Profiling_Job/#running-a-profiling-job",
            "text": "This section describes how users can run a profiling job from the Environment Overview  screen.   To run or rerun a job from the  Environment Overview  screen:    Click the  Run  icon (play icon) in the Action column for the\n    desired job.    The  Run  icon changes to a  Stop  icon while the job is\n    running.    When the job is complete, the  Status  changes.    To stop a running job from the  Environment Overview  screen:    Locate the job you want to stop.    In the job's  Action  column, click the  Stop  icon.    A popup appears asking, \"Are you sure you want to stop job?\" Click\n     OK .  When the job has been stopped, its status changes.",
            "title": "Running A Profiling Job"
        },
        {
            "location": "/Identifying_Sensitive_Data/Reporting_Profiling_Results/",
            "text": "Reporting Profiling Results\n\u00b6\n\n\nThis section describes the different ways of sharing/exploring the\nresults of a Profiling job.\n\n\nAfter a Job has been started from the Environment \nOverview\n screen,\nclicking on the Job Name will result in the display of the Profiling\njob from the \nMonitor\n tab. Clicking on the \nResults\n tab in the\nmiddle of the screen after the job has completed will display\nthe sensitive data findings on a table-column by table-column or\nfile-field by file-field basis.\n\n\n\n\nTo retrieve a PDF report of the \nResults\n tab, click on the\n\nProfiling Report\n link near the top of the page.\n\n\n\n\nAlternatively, after a job completes successfully, the profiling results\ncan be displayed through the \nInventory\n screen by examining the assigned\n\nDomain\n and masking algorithm \nMethod\ns for tables/files in\nthe Rule Set.\n\n\n\n\nTo get a spreadsheet capturing the Profiling results for the inventory, click\non \nExport\n near the top of the page and a CSV file will be created.\n\n\n\n\nThe spreadsheet can then be shared and manually modified to correct\nthe sensitive data findings by:\n\n\n\n\nChanging the \nIs Masked\n, \nAlgorithm\n, and/or \nDomains\n fields for\n   the respective Table/Column or File/Field in the CSV file accordingly.\n\n\nImporting the modified spreadsheet by clicking on \nImport\n near the\n   top of the \nInventory\n screen and specifying the modified CSV file\n   name.",
            "title": "Reporting Profiling Results"
        },
        {
            "location": "/Identifying_Sensitive_Data/Reporting_Profiling_Results/#reporting-profiling-results",
            "text": "This section describes the different ways of sharing/exploring the\nresults of a Profiling job.  After a Job has been started from the Environment  Overview  screen,\nclicking on the Job Name will result in the display of the Profiling\njob from the  Monitor  tab. Clicking on the  Results  tab in the\nmiddle of the screen after the job has completed will display\nthe sensitive data findings on a table-column by table-column or\nfile-field by file-field basis.   To retrieve a PDF report of the  Results  tab, click on the Profiling Report  link near the top of the page.   Alternatively, after a job completes successfully, the profiling results\ncan be displayed through the  Inventory  screen by examining the assigned Domain  and masking algorithm  Method s for tables/files in\nthe Rule Set.   To get a spreadsheet capturing the Profiling results for the inventory, click\non  Export  near the top of the page and a CSV file will be created.   The spreadsheet can then be shared and manually modified to correct\nthe sensitive data findings by:   Changing the  Is Masked ,  Algorithm , and/or  Domains  fields for\n   the respective Table/Column or File/Field in the CSV file accordingly.  Importing the modified spreadsheet by clicking on  Import  near the\n   top of the  Inventory  screen and specifying the modified CSV file\n   name.",
            "title": "Reporting Profiling Results"
        },
        {
            "location": "/Securing_Sensitive_Data/Out_Of_The_Box_Algorithm_Frameworks/",
            "text": "Out Of The Box Algorithm Frameworks\n\u00b6\n\n\nThis section describes the different algorithm frameworks (Secure Lookup, Segment Mapping, etc) that are available.\n\n\nSecure Lookup Algorithm Framework\n\u00b6\n\n\nSecure lookup is the most commonly used type of algorithm. It is easy to\ngenerate and works with different languages. When this algorithm\nreplaces real, sensitive data with fictional data, it is possible that\nit will create repeating data patterns, known as \u201ccollisions.\u201d For\nexample, the names \u201cTom\u201d and \u201cPeter\u201d could both be masked as \u201cMatt.\u201d\nBecause names and addresses naturally recur in real data, this mimics an\nactual data set. However, if you want the masking engine to mask all\ndata into unique outputs, you should use segment mapping.\n\n\nSement Mapping Algorithm Framework\n\u00b6\n\n\nSegment mapping algorithms produce no overlaps or repetitions in the\nmasked data. They let you create unique masked values by dividing a\ntarget value into separate segments and masking each segment\nindividually.\n\n\nYou can mask up to a maximum of 36 values using segment mapping. You\nmight use this method if you need columns with unique values, such as\nSocial Security Numbers, primary key columns, or foreign key columns.\nWhen using segment mapping algorithms for primary and foreign keys, in\norder to make sure they match, you must use the same segment mapping\nalgorithm for each. You can set the algorithm to produce alphanumeric\nresults (letters and numbers) or only numbers.\n\n\nWith segment mapping, you can set the algorithm to ignore specific\ncharacters. For example, you can choose to ignore dashes [-] so that\nthe same Social Security Number will be identified no matter how it is\nformatted. You can also preserve certain values. For example, to\nincrease the randomness of masked values, you can preserve a single\nnumber such as 5 wherever it occurs. Or if you want to leave some\ninformation unmasked, such as the last four digits of Social Security\nnumbers, you can preserve that information.\n\n\nSegment Mapping Example\n\u00b6\n\n\nPerhaps you have an account number for which you need to create a\nsegment mapping algorithm. You can separate the account number into\nsegments, preserving the first two-character segment, replacing a\nsegment with a specific value, and preserving a hyphen. The following is\na sample value for this account number:\n\n\nNM831026-04\n\n\nWhere:\n\n\n\n\n\n\nNM\n is a plan code number that you want to preserve, always a\n    two-character alphanumeric code.\n\n\n\n\n\n\n831026\n is the uniquely identifiable account number. To ensure\n    that you do not inadvertently create actual account numbers, you\n    can replace the first two digits with a sequence that never\n    appears in your account numbers in that location. (For example,\n    you can replace the first two digits with 98 because 98 is never\n    used as the first two digits of an account number.) To do that,\n    you want to split these six digits into two segments.\n\n\n\n\n\n\n-04\n is a location code. You want to preserve the hyphen and\n    you can replace the two digits with a number within a range (in\n    this case, a range of 1 to 77).\n\n\n\n\n\n\nMapping Algorithm Framework\n\u00b6\n\n\nA mapping algorithm allows you to state what values will replace the\noriginal data. It sequentially maps original data values to masked\nvalues that are pre-populated to a lookup table through the Masking\nEngine user interface. There will be no collisions in the masked data,\nbecause it always matches the same input to the same output. For example\n\u201cDavid\u201d will always become \u201cRagu,\u201d and \u201cMelissa\u201d will always become\n\u201cJasmine.\u201d The algorithm checks whether an input has already been\nmapped; if so, the algorithm changes the data to its designated output.\n\n\nYou can use a mapping algorithm on any set of values, of any length, but\nyou must know how many values you plan to mask. You must supply AT\nMINIMUM the same number of values as the number of unique values you are\nmasking; more is acceptable. For example, if there are 10,000 unique\nvalues in the column you are masking you must give the mapping algorithm\nAT LEAST 10,000\nvalues.\n\n\n\n\nInfo\n\n\nWhen you use a mapping algorithm, you cannot mask more than one table at a time. You must mask tables serially.\n\n\n\n\nBinary Lookup Algorithm Framework\n\u00b6\n\n\nA Binary Lookup Algorithm is much like the Secure Lookup Algorithm, but\nis used when entire files are stored in a specific column. This\nalgorithm replaces objects that appear in object columns. For example,\nif a bank has an object column that stores images of checks, you can use\na binary lookup algorithm to mask those images. The Delphix Engine\ncannot change data within images themselves, such as the names on X-rays\nor driver\u2019s licenses. However, you can replace all such images with a\nnew, fictional image. This fictional image is provided by the owner of\nthe original data.\n\n\nTokenization Algorithm Framework\n\u00b6\n\n\nA tokenization algorithm is the only type of algorithm that allows you\nto reverse its masking. For example, you can use a tokenization\nalgorithm to mask data before you send it to an external vendor for\nanalysis. The vendor can then identify accounts that need attention\nwithout having any access to the original, sensitive data. Once you have\nthe vendor\u2019s feedback, you can reverse the masking and take action on\nthe appropriate accounts.\n\n\nLike mapping, a tokenization algorithm creates a unique token for each\ninput such as \u201cDavid\u201d or \u201cMelissa.\u201d The actual data (for example, names\nand addresses) are converted into tokens that have similar properties to\nthe original data \u2013 such as text and length \u2013 but no longer convey any\nmeaning. The Delphix Masking Engine stores both the token and the\noriginal so that you can reverse masking later.\n\n\nMin Max Algorithm Framework\n\u00b6\n\n\nThe Delphix Masking Engine provides a \"Min Max Algorithm\" to normalize\ndata within a range \u2013 for example, 10 to 400. Values that are extremely\nhigh or low in certain categories allow viewers to infer someone\u2019s\nidentity, even if their name has been masked. For example, a salary of\n$1 suggests a company\u2019s CEO, and some age ranges suggest higher\ninsurance risk. You can use a min max algorithm to move all values of\nthis kind into the midrange. This algorithm allows you to make sure that\nall the values in the database are within a specified range.\n\n\nIf the \nOut of range Replacement Values\n checkbox is selected, a\ndefault value is used when the input cannot be evaluated.\n\n\nData Cleansing Algorithm Framework\n\u00b6\n\n\nA data cleansing algorithm does not perform any masking. Instead, it\nstandardizes varied spellings, misspellings, and abbreviations for the\nsame name. For example, \u201cAriz,\u201d \u201cAz,\u201d and \u201cArizona\u201d can all be cleansed\nto \u201cAZ.\u201d Use this algorithm if the target data needs to be in a standard\nformat prior to masking.\n\n\nFree Text Algorithm Framework\n\u00b6\n\n\nA free text redaction algorithm helps you remove sensitive data that\nappears in free-text columns such as \u201cNotes.\u201d This type of algorithm\nrequires some expertise to use, because you must set it to recognize\nsensitive data within a block of text.\n\n\nOne challenge is that individual words might not be sensitive on their\nown, but together they can be. The algorithm uses profiler sets to\ndetermine what information it needs to mask. You can decide which\nexpressions the algorithm uses to search for material such as addresses.\nFor example, you can set the algorithm to look for \u201cSt,\u201d \u201cCir,\u201d \u201cBlvd,\u201d\nand other words that suggest an address. You can also use pattern\nmatching to identify potentially sensitive information. For example, a\nnumber that takes the form 123-45-6789 is likely to be a Social Security\nNumber.\n\n\nYou can use a free text redaction algorithm to show or hide information\nby displaying either a \u201cblack list\u201d or a \u201cwhite list.\u201d\n\n\nBlacklist\n \u2013 Designated material will be redacted (removed). For\nexample, you can set a blacklist to hide patient names and addresses.\nThe blacklist feature will match the data in the lookup file to the\ninput file.\n\n\nWhitelist\n \u2013 ONLY designated material will be visible. For example,\nif a drug company wants to assess how often a particular drug is being\nprescribed, you can use a white list so that only the name of the drug\nwill appear in the notes. The whitelist feature enables you to mask data\nusing both the lookup file and a profile set.\n\n\nFor either option, a list of words can be imported from an external text\nfile or alternatively, you can use Profiler Sets to match words based on\nregular expressions, defined within Profiler Expressions. You can also\nspecify the redaction value that will replace the masked words. Regular\nexpressions defined using Profiler Sets will match individual words\nwithin the input text, rather than phrases.",
            "title": "Out Of The Box Algorithm Frameworks"
        },
        {
            "location": "/Securing_Sensitive_Data/Out_Of_The_Box_Algorithm_Frameworks/#out-of-the-box-algorithm-frameworks",
            "text": "This section describes the different algorithm frameworks (Secure Lookup, Segment Mapping, etc) that are available.",
            "title": "Out Of The Box Algorithm Frameworks"
        },
        {
            "location": "/Securing_Sensitive_Data/Out_Of_The_Box_Algorithm_Frameworks/#secure-lookup-algorithm-framework",
            "text": "Secure lookup is the most commonly used type of algorithm. It is easy to\ngenerate and works with different languages. When this algorithm\nreplaces real, sensitive data with fictional data, it is possible that\nit will create repeating data patterns, known as \u201ccollisions.\u201d For\nexample, the names \u201cTom\u201d and \u201cPeter\u201d could both be masked as \u201cMatt.\u201d\nBecause names and addresses naturally recur in real data, this mimics an\nactual data set. However, if you want the masking engine to mask all\ndata into unique outputs, you should use segment mapping.",
            "title": "Secure Lookup Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Out_Of_The_Box_Algorithm_Frameworks/#sement-mapping-algorithm-framework",
            "text": "Segment mapping algorithms produce no overlaps or repetitions in the\nmasked data. They let you create unique masked values by dividing a\ntarget value into separate segments and masking each segment\nindividually.  You can mask up to a maximum of 36 values using segment mapping. You\nmight use this method if you need columns with unique values, such as\nSocial Security Numbers, primary key columns, or foreign key columns.\nWhen using segment mapping algorithms for primary and foreign keys, in\norder to make sure they match, you must use the same segment mapping\nalgorithm for each. You can set the algorithm to produce alphanumeric\nresults (letters and numbers) or only numbers.  With segment mapping, you can set the algorithm to ignore specific\ncharacters. For example, you can choose to ignore dashes [-] so that\nthe same Social Security Number will be identified no matter how it is\nformatted. You can also preserve certain values. For example, to\nincrease the randomness of masked values, you can preserve a single\nnumber such as 5 wherever it occurs. Or if you want to leave some\ninformation unmasked, such as the last four digits of Social Security\nnumbers, you can preserve that information.",
            "title": "Sement Mapping Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Out_Of_The_Box_Algorithm_Frameworks/#segment-mapping-example",
            "text": "Perhaps you have an account number for which you need to create a\nsegment mapping algorithm. You can separate the account number into\nsegments, preserving the first two-character segment, replacing a\nsegment with a specific value, and preserving a hyphen. The following is\na sample value for this account number:  NM831026-04  Where:    NM  is a plan code number that you want to preserve, always a\n    two-character alphanumeric code.    831026  is the uniquely identifiable account number. To ensure\n    that you do not inadvertently create actual account numbers, you\n    can replace the first two digits with a sequence that never\n    appears in your account numbers in that location. (For example,\n    you can replace the first two digits with 98 because 98 is never\n    used as the first two digits of an account number.) To do that,\n    you want to split these six digits into two segments.    -04  is a location code. You want to preserve the hyphen and\n    you can replace the two digits with a number within a range (in\n    this case, a range of 1 to 77).",
            "title": "Segment Mapping Example"
        },
        {
            "location": "/Securing_Sensitive_Data/Out_Of_The_Box_Algorithm_Frameworks/#mapping-algorithm-framework",
            "text": "A mapping algorithm allows you to state what values will replace the\noriginal data. It sequentially maps original data values to masked\nvalues that are pre-populated to a lookup table through the Masking\nEngine user interface. There will be no collisions in the masked data,\nbecause it always matches the same input to the same output. For example\n\u201cDavid\u201d will always become \u201cRagu,\u201d and \u201cMelissa\u201d will always become\n\u201cJasmine.\u201d The algorithm checks whether an input has already been\nmapped; if so, the algorithm changes the data to its designated output.  You can use a mapping algorithm on any set of values, of any length, but\nyou must know how many values you plan to mask. You must supply AT\nMINIMUM the same number of values as the number of unique values you are\nmasking; more is acceptable. For example, if there are 10,000 unique\nvalues in the column you are masking you must give the mapping algorithm\nAT LEAST 10,000\nvalues.   Info  When you use a mapping algorithm, you cannot mask more than one table at a time. You must mask tables serially.",
            "title": "Mapping Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Out_Of_The_Box_Algorithm_Frameworks/#binary-lookup-algorithm-framework",
            "text": "A Binary Lookup Algorithm is much like the Secure Lookup Algorithm, but\nis used when entire files are stored in a specific column. This\nalgorithm replaces objects that appear in object columns. For example,\nif a bank has an object column that stores images of checks, you can use\na binary lookup algorithm to mask those images. The Delphix Engine\ncannot change data within images themselves, such as the names on X-rays\nor driver\u2019s licenses. However, you can replace all such images with a\nnew, fictional image. This fictional image is provided by the owner of\nthe original data.",
            "title": "Binary Lookup Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Out_Of_The_Box_Algorithm_Frameworks/#tokenization-algorithm-framework",
            "text": "A tokenization algorithm is the only type of algorithm that allows you\nto reverse its masking. For example, you can use a tokenization\nalgorithm to mask data before you send it to an external vendor for\nanalysis. The vendor can then identify accounts that need attention\nwithout having any access to the original, sensitive data. Once you have\nthe vendor\u2019s feedback, you can reverse the masking and take action on\nthe appropriate accounts.  Like mapping, a tokenization algorithm creates a unique token for each\ninput such as \u201cDavid\u201d or \u201cMelissa.\u201d The actual data (for example, names\nand addresses) are converted into tokens that have similar properties to\nthe original data \u2013 such as text and length \u2013 but no longer convey any\nmeaning. The Delphix Masking Engine stores both the token and the\noriginal so that you can reverse masking later.",
            "title": "Tokenization Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Out_Of_The_Box_Algorithm_Frameworks/#min-max-algorithm-framework",
            "text": "The Delphix Masking Engine provides a \"Min Max Algorithm\" to normalize\ndata within a range \u2013 for example, 10 to 400. Values that are extremely\nhigh or low in certain categories allow viewers to infer someone\u2019s\nidentity, even if their name has been masked. For example, a salary of\n$1 suggests a company\u2019s CEO, and some age ranges suggest higher\ninsurance risk. You can use a min max algorithm to move all values of\nthis kind into the midrange. This algorithm allows you to make sure that\nall the values in the database are within a specified range.  If the  Out of range Replacement Values  checkbox is selected, a\ndefault value is used when the input cannot be evaluated.",
            "title": "Min Max Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Out_Of_The_Box_Algorithm_Frameworks/#data-cleansing-algorithm-framework",
            "text": "A data cleansing algorithm does not perform any masking. Instead, it\nstandardizes varied spellings, misspellings, and abbreviations for the\nsame name. For example, \u201cAriz,\u201d \u201cAz,\u201d and \u201cArizona\u201d can all be cleansed\nto \u201cAZ.\u201d Use this algorithm if the target data needs to be in a standard\nformat prior to masking.",
            "title": "Data Cleansing Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Out_Of_The_Box_Algorithm_Frameworks/#free-text-algorithm-framework",
            "text": "A free text redaction algorithm helps you remove sensitive data that\nappears in free-text columns such as \u201cNotes.\u201d This type of algorithm\nrequires some expertise to use, because you must set it to recognize\nsensitive data within a block of text.  One challenge is that individual words might not be sensitive on their\nown, but together they can be. The algorithm uses profiler sets to\ndetermine what information it needs to mask. You can decide which\nexpressions the algorithm uses to search for material such as addresses.\nFor example, you can set the algorithm to look for \u201cSt,\u201d \u201cCir,\u201d \u201cBlvd,\u201d\nand other words that suggest an address. You can also use pattern\nmatching to identify potentially sensitive information. For example, a\nnumber that takes the form 123-45-6789 is likely to be a Social Security\nNumber.  You can use a free text redaction algorithm to show or hide information\nby displaying either a \u201cblack list\u201d or a \u201cwhite list.\u201d  Blacklist  \u2013 Designated material will be redacted (removed). For\nexample, you can set a blacklist to hide patient names and addresses.\nThe blacklist feature will match the data in the lookup file to the\ninput file.  Whitelist  \u2013 ONLY designated material will be visible. For example,\nif a drug company wants to assess how often a particular drug is being\nprescribed, you can use a white list so that only the name of the drug\nwill appear in the notes. The whitelist feature enables you to mask data\nusing both the lookup file and a profile set.  For either option, a list of words can be imported from an external text\nfile or alternatively, you can use Profiler Sets to match words based on\nregular expressions, defined within Profiler Expressions. You can also\nspecify the redaction value that will replace the masked words. Regular\nexpressions defined using Profiler Sets will match individual words\nwithin the input text, rather than phrases.",
            "title": "Free Text Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/",
            "text": "Configuring Your Own Algorithms\n\u00b6\n\n\nThis section describes how users can configure their own algorithms\nusing Delphix\u2019s built in algorithm frameworks.\n\n\nAlgorithm Settings\n\u00b6\n\n\nThe \nAlgorithm\n tab displays algorithm Names along with Type and\nDescription. This is where you add (or create) new algorithms. The\ndefault algorithms and any algorithms you have defined appear on this tab.\nAll algorithm values are stored encrypted. These values are only\ndecrypted during the masking process.\n\n\n\n\nCreating New Algorithms\n\u00b6\n\n\nIf none of the default algorithms meet your needs, you might want to\ncreate a new algorithm.\n\n\nAlgorithm Frameworks give you the ability to quickly and easily define the\nalgorithms you want, directly on the Settings page. Then, you can immediately\npropagate them. Anyone in your organization who has the Delphix Masking Engine\ncan then access the information.\n\n\nAdministrators can update \nsystem\n-defined algorithms. User-defined\nalgorithms can be accessed by all users and updated by the owner/user\nwho created the algorithm.\n\n\nTo add an algorithm:\n\n\n\n\n\n\nIn the upper right-hand corner of the \nAlgorithm\n settings tab,\n    click \nAdd Algorithm\n.\n\n\n\n\n\n\n\n\nSelect an algorithm type.\n\n\n\n\n\n\nComplete the form to the right to name and describe your new\nalgorithm.\n\n\n\n\n\n\nClick \nSave.\n\n\n\n\n\n\nChoosing an Algorithm Framework\n\u00b6\n\n\nSee Out Of The Box Secure Methods/Algorithms for detailed description on each\nAlgorithm Framework. The algorithm framework you choose will depend on the\nformat of the data & your internal data security guidelines.\n\n\nSecure Lookup Algorithm Framework\n\u00b6\n\n\nTo add a secure lookup algorithm:\n\n\n\n\n\n\nIn the upper right-hand corner of the \nAlgorithm\n tab, click\n    \nAdd Algorithm\n.\n\n\n\n\n\n\nChoose \nSecure Lookup Algorithm\n. The Create SL Algorithm pane\n    appears.\n\n\n\n\n\n\n\n\nEnter a \nAlgorithm Name\n.\n\n\n\n\nInfo\n\n\nThis MUST be unique.\n\n\n\n\n\n\n\n\nEnter a \nDescription\n.\n\n\n\n\n\n\nSpecify a \nLookup File\n.\n\n\nThis file is a single list of values. It does not require a header.\nMake sure there are no spaces or returns at the end of the last line\nin the file. The following is sample file content:\n\n\nSmallville\nClarkville\nFarmville\nTownville\nCityname\nCitytown\nTowneaster\n\n\n\n\n\n\n\n\n\nWhen you are finished, click \nSave\n.\n\n\n\n\n\n\nBefore you can use the algorithm in a profiling job, you must add it to a domain.\n\n\n\n\n\n\n\n\nInfo\n\n\nThe masking engine supports lookup files saved in ASCII or UTF-8 format only. If the lookup file contains foreign alphabet characters, the file must be saved in UTF-8 format with no BOM (Byte Order Marker) for Masking Engine to read the Unicode text correctly. Some applications, e.g. Notepad on Windows, write a BOM (Byte Order Marker) at the beginning of Unicode files which irritates the masking engine and will lead to SQL update or insert errors when trying to run a masking job that applies a Secure Lookup algorithm that has been created based on a UTF-8 file that included a BOM.\n\n\n\n\nSegmented Mapping Algorithm Framework\n\u00b6\n\n\n\n\n\n\nIn the upper right-hand region of the \nAlgorithm\n tab, click\n    \nAdd Algorithm\n.\n\n\n\n\n\n\nSelect \nSegment Mapping Algorithm\n. The Create Segment Mapping\n    Algorithm pane appears.\n\n\n\n\n\n\n\n\nEnter a \nRule Name\n.\n\n\n\n\n\n\nEnter a \nDescription.\n\n\n\n\n\n\nFrom the \nNo. of Segment\n drop-down menu, select how many\n    segments you want to mask.\n\n\n\n\nNOTE\n\n\nThis number does NOT include the values you want to preserve.\n\n\n\n\nThe minimum number of segments is 2; the maximum is 9. A box appears\nfor each segment.\n\n\n\n\n\n\nFor each segment, choose the \nType\n of segment from the\n    dropdown: \nNumeric\n or\n\nAlphanumeric\n.\n\n\n\n\nInfo\n\n\nNumeric\n segments are masked as whole segments. \nAlphanumeric\n segments are masked by individual character.\n\n\n\n\n\n\n\n\nFor each segment, select its \nLength\n (number of characters)\n    from the drop-down menu. The maximum is 4.\n\n\n\n\n\n\nOptionally, for each segment, specify range values. You might need\n    to specify range values to satisfy particular application\n    requirements, for example. See details below.\n\n\n\n\n\n\nPreserve Original Values\n by entering \nStarting position\n and\n    \nlength\n values. (Position starts at 1.) For example, to\n    preserve the second, third, and fourth values, enter Starting\n    position \n2\n and length \n3\n.\n\n\nIf you need additional value fields, click \nAdd\n.\n\n\n\n\n\n\nWhen you are finished, click \nSave\n.\n\n\n\n\n\n\nBefore you can use the algorithm in a profiling job,\n    you must add it to a domain. If you are not using\n    the Masking Engine Profiler to create your inventory, you do not\n    need to associate the algorithm with a domain.\n\n\n\n\n\n\nSpecifying Range Values\n\u00b6\n\n\nYou can specify ranges for \nReal Values\n and \nMask Values\n. With\nReal Values ranges, you can specify all the possible real values to map\nto the ranges of masked values. Any values NOT listed in the Real Values\nranges would then mask to themselves.\n\n\nSpecifying range values is optional. If you need unique values (for\nexample, masking a unique key column), you MUST leave the range values\nblank. If you plan to certify your data, you must specify range values.\n\n\nWhen determining a numeric or alphanumeric range, remember that a narrow\nrange will likely generate duplicate values, which will cause your job\nto fail.\n\n\n\n\n\n\nTo ignore specific characters, enter one or more characters in the\n    \nIgnore Character List\n box. Separate values with a comma.\n\n\n\n\n\n\nTo ignore the comma character (,), select the \nIgnore comma (,)\n\n    check box.\n\n\n\n\n\n\nTo ignore control characters, select \nAdd Control Characters\n.\n    The \nAdd Control Characters\n window appears.\n\n\n\n\n\n\n\n\nSelect the individual control characters that you would like to\n    ignore, or choose \nSelect All\n or \nSelect None\n.\n\n\n\n\n\n\nWhen you are finished, click \nSave\n.\n\n\n\n\n\n\nYou are returned to the Segment Mapping pane.\n\n\n\n\n\n\nNumeric segment type\n\u00b6\n\n\n\n\n\n\nMin#\n \u2014 A number; the first value in the range. Value can be 1\n    digit or up to the length of the segment. For example, for a\n    3-digit segment, you can specify 1, 2, or 3 digits. Acceptable\n    characters: 0-9.\n\n\n\n\n\n\nMax#\n \u2014 A number; the last value in the range. Value should be\n    the same length as the segment. For example, for a 3-digit\n    segment, you should specify 3 digits. Acceptable characters: 0-9.\n\n\n\n\n\n\nRange#\n \u2014 A range of numbers; separate values in this field\n    with a comma (,). Value should be the same length as the segment.\n    For example, for a 3-digit segment, you should specify 3 digits.\n    Acceptable characters:\n0-9.\n\n\n\n\n\n\n\n\nInfo\n\n\nIf you do not specify a range, the Masking Engine uses the full range. For example, for a 4-digit segment, the Masking Engine uses 0-9999.\n\n\n\n\nAlphanumeric segment type\n\u00b6\n\n\n\n\n\n\nMin#\n \u2014 A number from 0 to 9; the first value in the range.\n\n\n\n\n\n\nMax#\n \u2014 A number from 0 to 9; the last value in the range.\n\n\n\n\n\n\nMinChar\n \u2014 A letter from A to Z; the first value in the range.\n\n\n\n\n\n\nMaxChar\n \u2014 A letter from A to Z; the last value in the range.\n\n\n\n\n\n\nRange#\n \u2014 A range of alphanumeric characters; separate values\n    in this field with a comma (,). Individual values can be a number\n    from 0 to 9 or an uppercase letter from A to Z. (For example,\n    B,C,J,K,Y,Z or AB,DE.)\n\n\n\n\n\n\n\n\nInfo\n\n\nIf you do not specify a range, the Masking Engine uses the full range (A-Z, 0-9). If you do not know the format of the input, leave the range fields empty. If you know the format of the input (for example, always alphanumeric followed by numeric), you can enter range values such as A2 and S9.\n\n\n\n\nMapping Algorithm Framework\n\u00b6\n\n\nTo add a mapping algorithm:\n\n\n\n\n\n\nIn the upper right-hand corner of the \nAlgorithm\n tab, click\n    \nAdd Algorithm\n.\n\n\n\n\n\n\nSelect \nMapping Algorithm\n.\n\n\n\n\n\n\nThe \nCreate Mapping Algorithm\n pane appears.\n\n\n\n\n\n\n\n\nEnter a \nRule Name\n. This name MUST be unique.\n\n\n\n\n\n\nEnter a \nDescription\n.\n\n\n\n\n\n\nSpecify a \nLookup File\n.\n\n\n\n\n\n\nThe value file must have NO header. Make sure there are no spaces\n    or returns at the end of the last line in the file. The following\n    is sample file content. Notice that there is no header and only a\n    list of values.\n\n\nSmallville\nClarkville\nFarmville\nTownville\nCityname\nCitytown\nTowneaster\n\n\n\n\n\n\n\n\n\nTo ignore specific characters, enter one or more characters in the\n    \nIgnore Character List\n box. Separate values with a comma.\n\n\n\n\n\n\nTo ignore the comma character (,), select the \nIgnore comma (,)\n\n    check box.\n\n\n\n\n\n\nWhen you are finished, click \nSave\n.\n\n\n\n\n\n\nBefore you can use the algorithm by specifying it in a profiling\njob, you must add it to a domain. If you are not using the\nMasking Engine Profiler to create your inventory, you do not need to\nassociate the algorithm with a domain.\n\n\nMasking Binary Lookup Algorithm Framework\n\u00b6\n\n\nTo add a binary lookup algorithm:\n\n\n\n\n\n\nAt the top right of the \nAlgorithm\n tab, click \nAdd\n    Algorithm\n.\n\n\n\n\n\n\nSelect \nBinary Lookup Algorithm\n. The Binary SL Rule pane\n    appears.\n\n\n\n\n\n\n\n\nEnter a \nRule Name\n.\n\n\n\n\n\n\nEnter a \nDescription\n.\n\n\n\n\n\n\nSelect a \nBinary Lookup File\n on your filesystem.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nTokenization Algorithm Framework\n\u00b6\n\n\nTo add a Tokenization algorithm:\n\n\n\n\n\n\nEnter algorithm \nName\n.\n\n\n\n\n\n\nEnter a \nDescription\n.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nOnce you have created an algorithm, you will need to associate it with a\ndomain.\n\n\n\n\n\n\nNavigate to the \nHome>Settings>Domains\n page and click \nAdd\n    Domain\n.\n\n\n\n\n\n\nEnter a domain name.\n\n\n\n\n\n\nFrom the \nTokenization Algorithm Name\n drop-down menu, select\n    your algorithm.\n\n\n\n\n\n\nNext, create a Tokenization Environment:\n\n\n\n\n\n\nOn the home page, click \nEnvironments\n.\n\n\n\n\n\n\nClick \nAdd Environment\n.\n\n\n\n\n\n\n\n\nFor \nPurpose\n, select \nTokenize/Re-Identify\n.\n\n\n\n\n\n\nClick\n\nSave\n.\n\n\n\n\nInfo\n\n\nThis environment will be used to re-identify your data when required.\n\n\n\n\n\n\n\n\nSet up a Tokenize job using tokenization method. Execute the job.\n\n\n\n\n\n\n\n\nHere is a snapshot of the data before and after Tokenization to give you\nan idea of what the it will look like.\n\n\nBefore Tokenization\n\n\n\n\nAfter Tokenization\n\n\n\n\nMIN Max Algorithm Framework\n\u00b6\n\n\nThe Delphix Masking Engine provides a \"Min Max Algorithm\" to normalize\ndata within a range \u2013 for example, 10 to 400. Values that are extremely\nhigh or low in certain categories allow viewers to infer someone\u2019s\nidentity, even if their name has been masked. For example, a salary of\n$1 suggests a company\u2019s CEO, and some age ranges suggest higher\ninsurance risk. You can use a min max algorithm to move all values of\nthis kind into the midrange. This algorithm allows you to make sure that\nall the values in the database are within a specified range.\n\n\nIf the \nOut of range Replacement Values\n checkbox is selected, a\ndefault value is used when the input cannot be evaluated.\n\n\n\n\n\n\n\n\nEnter the \nAlgorithm Name\n.\n\n\n\n\n\n\nEnter a \nDescription\n.\n\n\n\n\n\n\nEnter \nMin Value\n and \nMax Value\n.\n\n\n\n\n\n\nClick \nOut of range Replacement Values\n.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nExample: Age less than 18 years - enter Min Value 0 and Max Value 18.\n\n\nData Cleansing Algorithm Framework\n\u00b6\n\n\nA data cleansing algorithm does not perform any masking. Instead, it\nstandardizes varied spellings, misspellings, and abbreviations for the\nsame name. For example, \u201cAriz,\u201d \u201cAz,\u201d and \u201cArizona\u201d can all be cleansed\nto \u201cAZ.\u201d Use this algorithm if the target data needs to be in a standard\nformat prior to masking.\n\n\n\n\n\n\n\n\nEnter an \nAlgorithm Name\n.\n\n\n\n\n\n\nEnter a \nDescription\n.\n\n\n\n\n\n\nSelect \nLookup File\n location.\n\n\n\n\n\n\nSpecify a \nDelimiter\n (key and value separator). The default delimiter is =. You can\n    change this to match the lookup file.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nBelow is an example of a lookup input file. It does not require a\nheader. Make sure there are no spaces or returns at the end of the last\nline in the file. The following is sample file content:\n\n\nNYC=NY\nNY City=NY\nNew York=NY\nManhattan=NY\n\n\n\n\n\nFree Text Algorithm Framework\n\u00b6\n\n\nTo add a free text redaction algorith:\n\n\n\n\n\n\n\n\nEnter an \nAlgorithm Name\n.\n\n\n\n\n\n\nEnter a \nDescription\n.\n\n\n\n\n\n\nSelect the \nBlack List\n or \nWhite List\n radio button.\n\n\n\n\n\n\nSelect \nLookup File\n and enter \nRedaction Value\n OR/AND\n\n\n\n\n\n\nSelect \nProfiler Sets\n from the drop-down menu and enter\n    \nRedaction Value\n.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nFree Text Redaction Example\n\u00b6\n\n\n\n\n\n\nCreate Input File.\n\n\n\n\n\n\nCreate input file using notepad. Enter the following text:\n\n\nThe customer Bob Jones is satisfied with the terms of the sales\nagreement. Please call to confirm at 718-223-7896.\n\n\n\n\n\n\n\n\n\nSave file as txt.\n\n\n\n\n\n\nCreate lookup file.\n\n\n\n\n\n\nCreate a lookup file.\n\n\n\n\n\n\nUse notepad to create a txt file and save the file as a TXT.\n    Be sure to hit return after each field. The lookup flat file\n    contains the following data:\n\n\nBob\nJones\nAgreement\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate an Algorithm\n\u00b6\n\n\nYou will be prompted for the following information:\n\n\n\n\n\n\nFor \nAlgorithm Name\n, enter \nBlacklist_Test1\n.\n\n\n\n\n\n\nFor \nDescription\n, enter \nBlacklist Test\n.\n\n\n\n\n\n\nSelect the \nBlack List\n radio button.\n\n\n\n\n\n\nSelect \nLookUp File\n.\n\n\n\n\n\n\nEnter redaction value \nXXXX\n.\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nCreate Rule Set\n\u00b6\n\n\n\n\n\n\nFrom the job page go to Rule Set and Click \nCreate Rule Set\n.\n\n\n\n\n\n\n\n\nFor \nRule Set Name\n, enter \nFree_ Text_RS\n.\n\n\n\n\n\n\nFrom the \nConnector\n drop-down menu, select \nFree Text\n.\n\n\n\n\n\n\nSelect the \nInput File\n by clicking the box next to your input\n    file\n\n\n\n\n\n\nClick \nSave\n.\n\n\n\n\n\n\nCreate Masking Job\n\u00b6\n\n\n\n\n\n\nUse Free_Text Rule Set\n\n\n\n\n\n\nExecute Masking job.\n\n\n\n\n\n\nThe results of the masking job will show the following:\n\n\nThe customer xxxx xxxx is satisfied with the terms\nof the sales xxxx. Please call to confirm at 718-223-7896.\n\n\n\n\n\n\"Bob,\" \"Jones,\" and \"agreement\" are redacted.",
            "title": "Configuring Your Own Algorithms"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#configuring-your-own-algorithms",
            "text": "This section describes how users can configure their own algorithms\nusing Delphix\u2019s built in algorithm frameworks.",
            "title": "Configuring Your Own Algorithms"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#algorithm-settings",
            "text": "The  Algorithm  tab displays algorithm Names along with Type and\nDescription. This is where you add (or create) new algorithms. The\ndefault algorithms and any algorithms you have defined appear on this tab.\nAll algorithm values are stored encrypted. These values are only\ndecrypted during the masking process.",
            "title": "Algorithm Settings"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#creating-new-algorithms",
            "text": "If none of the default algorithms meet your needs, you might want to\ncreate a new algorithm.  Algorithm Frameworks give you the ability to quickly and easily define the\nalgorithms you want, directly on the Settings page. Then, you can immediately\npropagate them. Anyone in your organization who has the Delphix Masking Engine\ncan then access the information.  Administrators can update  system -defined algorithms. User-defined\nalgorithms can be accessed by all users and updated by the owner/user\nwho created the algorithm.  To add an algorithm:    In the upper right-hand corner of the  Algorithm  settings tab,\n    click  Add Algorithm .     Select an algorithm type.    Complete the form to the right to name and describe your new\nalgorithm.    Click  Save.",
            "title": "Creating New Algorithms"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#choosing-an-algorithm-framework",
            "text": "See Out Of The Box Secure Methods/Algorithms for detailed description on each\nAlgorithm Framework. The algorithm framework you choose will depend on the\nformat of the data & your internal data security guidelines.",
            "title": "Choosing an Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#secure-lookup-algorithm-framework",
            "text": "To add a secure lookup algorithm:    In the upper right-hand corner of the  Algorithm  tab, click\n     Add Algorithm .    Choose  Secure Lookup Algorithm . The Create SL Algorithm pane\n    appears.     Enter a  Algorithm Name .   Info  This MUST be unique.     Enter a  Description .    Specify a  Lookup File .  This file is a single list of values. It does not require a header.\nMake sure there are no spaces or returns at the end of the last line\nin the file. The following is sample file content:  Smallville\nClarkville\nFarmville\nTownville\nCityname\nCitytown\nTowneaster    When you are finished, click  Save .    Before you can use the algorithm in a profiling job, you must add it to a domain.     Info  The masking engine supports lookup files saved in ASCII or UTF-8 format only. If the lookup file contains foreign alphabet characters, the file must be saved in UTF-8 format with no BOM (Byte Order Marker) for Masking Engine to read the Unicode text correctly. Some applications, e.g. Notepad on Windows, write a BOM (Byte Order Marker) at the beginning of Unicode files which irritates the masking engine and will lead to SQL update or insert errors when trying to run a masking job that applies a Secure Lookup algorithm that has been created based on a UTF-8 file that included a BOM.",
            "title": "Secure Lookup Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#segmented-mapping-algorithm-framework",
            "text": "In the upper right-hand region of the  Algorithm  tab, click\n     Add Algorithm .    Select  Segment Mapping Algorithm . The Create Segment Mapping\n    Algorithm pane appears.     Enter a  Rule Name .    Enter a  Description.    From the  No. of Segment  drop-down menu, select how many\n    segments you want to mask.   NOTE  This number does NOT include the values you want to preserve.   The minimum number of segments is 2; the maximum is 9. A box appears\nfor each segment.    For each segment, choose the  Type  of segment from the\n    dropdown:  Numeric  or Alphanumeric .   Info  Numeric  segments are masked as whole segments.  Alphanumeric  segments are masked by individual character.     For each segment, select its  Length  (number of characters)\n    from the drop-down menu. The maximum is 4.    Optionally, for each segment, specify range values. You might need\n    to specify range values to satisfy particular application\n    requirements, for example. See details below.    Preserve Original Values  by entering  Starting position  and\n     length  values. (Position starts at 1.) For example, to\n    preserve the second, third, and fourth values, enter Starting\n    position  2  and length  3 .  If you need additional value fields, click  Add .    When you are finished, click  Save .    Before you can use the algorithm in a profiling job,\n    you must add it to a domain. If you are not using\n    the Masking Engine Profiler to create your inventory, you do not\n    need to associate the algorithm with a domain.",
            "title": "Segmented Mapping Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#specifying-range-values",
            "text": "You can specify ranges for  Real Values  and  Mask Values . With\nReal Values ranges, you can specify all the possible real values to map\nto the ranges of masked values. Any values NOT listed in the Real Values\nranges would then mask to themselves.  Specifying range values is optional. If you need unique values (for\nexample, masking a unique key column), you MUST leave the range values\nblank. If you plan to certify your data, you must specify range values.  When determining a numeric or alphanumeric range, remember that a narrow\nrange will likely generate duplicate values, which will cause your job\nto fail.    To ignore specific characters, enter one or more characters in the\n     Ignore Character List  box. Separate values with a comma.    To ignore the comma character (,), select the  Ignore comma (,) \n    check box.    To ignore control characters, select  Add Control Characters .\n    The  Add Control Characters  window appears.     Select the individual control characters that you would like to\n    ignore, or choose  Select All  or  Select None .    When you are finished, click  Save .    You are returned to the Segment Mapping pane.",
            "title": "Specifying Range Values"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#numeric-segment-type",
            "text": "Min#  \u2014 A number; the first value in the range. Value can be 1\n    digit or up to the length of the segment. For example, for a\n    3-digit segment, you can specify 1, 2, or 3 digits. Acceptable\n    characters: 0-9.    Max#  \u2014 A number; the last value in the range. Value should be\n    the same length as the segment. For example, for a 3-digit\n    segment, you should specify 3 digits. Acceptable characters: 0-9.    Range#  \u2014 A range of numbers; separate values in this field\n    with a comma (,). Value should be the same length as the segment.\n    For example, for a 3-digit segment, you should specify 3 digits.\n    Acceptable characters:\n0-9.     Info  If you do not specify a range, the Masking Engine uses the full range. For example, for a 4-digit segment, the Masking Engine uses 0-9999.",
            "title": "Numeric segment type"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#alphanumeric-segment-type",
            "text": "Min#  \u2014 A number from 0 to 9; the first value in the range.    Max#  \u2014 A number from 0 to 9; the last value in the range.    MinChar  \u2014 A letter from A to Z; the first value in the range.    MaxChar  \u2014 A letter from A to Z; the last value in the range.    Range#  \u2014 A range of alphanumeric characters; separate values\n    in this field with a comma (,). Individual values can be a number\n    from 0 to 9 or an uppercase letter from A to Z. (For example,\n    B,C,J,K,Y,Z or AB,DE.)     Info  If you do not specify a range, the Masking Engine uses the full range (A-Z, 0-9). If you do not know the format of the input, leave the range fields empty. If you know the format of the input (for example, always alphanumeric followed by numeric), you can enter range values such as A2 and S9.",
            "title": "Alphanumeric segment type"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#mapping-algorithm-framework",
            "text": "To add a mapping algorithm:    In the upper right-hand corner of the  Algorithm  tab, click\n     Add Algorithm .    Select  Mapping Algorithm .    The  Create Mapping Algorithm  pane appears.     Enter a  Rule Name . This name MUST be unique.    Enter a  Description .    Specify a  Lookup File .    The value file must have NO header. Make sure there are no spaces\n    or returns at the end of the last line in the file. The following\n    is sample file content. Notice that there is no header and only a\n    list of values.  Smallville\nClarkville\nFarmville\nTownville\nCityname\nCitytown\nTowneaster    To ignore specific characters, enter one or more characters in the\n     Ignore Character List  box. Separate values with a comma.    To ignore the comma character (,), select the  Ignore comma (,) \n    check box.    When you are finished, click  Save .    Before you can use the algorithm by specifying it in a profiling\njob, you must add it to a domain. If you are not using the\nMasking Engine Profiler to create your inventory, you do not need to\nassociate the algorithm with a domain.",
            "title": "Mapping Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#masking-binary-lookup-algorithm-framework",
            "text": "To add a binary lookup algorithm:    At the top right of the  Algorithm  tab, click  Add\n    Algorithm .    Select  Binary Lookup Algorithm . The Binary SL Rule pane\n    appears.     Enter a  Rule Name .    Enter a  Description .    Select a  Binary Lookup File  on your filesystem.    Click  Save .",
            "title": "Masking Binary Lookup Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#tokenization-algorithm-framework",
            "text": "To add a Tokenization algorithm:    Enter algorithm  Name .    Enter a  Description .    Click  Save .    Once you have created an algorithm, you will need to associate it with a\ndomain.    Navigate to the  Home>Settings>Domains  page and click  Add\n    Domain .    Enter a domain name.    From the  Tokenization Algorithm Name  drop-down menu, select\n    your algorithm.    Next, create a Tokenization Environment:    On the home page, click  Environments .    Click  Add Environment .     For  Purpose , select  Tokenize/Re-Identify .    Click Save .   Info  This environment will be used to re-identify your data when required.     Set up a Tokenize job using tokenization method. Execute the job.     Here is a snapshot of the data before and after Tokenization to give you\nan idea of what the it will look like.  Before Tokenization   After Tokenization",
            "title": "Tokenization Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#min-max-algorithm-framework",
            "text": "The Delphix Masking Engine provides a \"Min Max Algorithm\" to normalize\ndata within a range \u2013 for example, 10 to 400. Values that are extremely\nhigh or low in certain categories allow viewers to infer someone\u2019s\nidentity, even if their name has been masked. For example, a salary of\n$1 suggests a company\u2019s CEO, and some age ranges suggest higher\ninsurance risk. You can use a min max algorithm to move all values of\nthis kind into the midrange. This algorithm allows you to make sure that\nall the values in the database are within a specified range.  If the  Out of range Replacement Values  checkbox is selected, a\ndefault value is used when the input cannot be evaluated.     Enter the  Algorithm Name .    Enter a  Description .    Enter  Min Value  and  Max Value .    Click  Out of range Replacement Values .    Click  Save .    Example: Age less than 18 years - enter Min Value 0 and Max Value 18.",
            "title": "MIN Max Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#data-cleansing-algorithm-framework",
            "text": "A data cleansing algorithm does not perform any masking. Instead, it\nstandardizes varied spellings, misspellings, and abbreviations for the\nsame name. For example, \u201cAriz,\u201d \u201cAz,\u201d and \u201cArizona\u201d can all be cleansed\nto \u201cAZ.\u201d Use this algorithm if the target data needs to be in a standard\nformat prior to masking.     Enter an  Algorithm Name .    Enter a  Description .    Select  Lookup File  location.    Specify a  Delimiter  (key and value separator). The default delimiter is =. You can\n    change this to match the lookup file.    Click  Save .    Below is an example of a lookup input file. It does not require a\nheader. Make sure there are no spaces or returns at the end of the last\nline in the file. The following is sample file content:  NYC=NY\nNY City=NY\nNew York=NY\nManhattan=NY",
            "title": "Data Cleansing Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#free-text-algorithm-framework",
            "text": "To add a free text redaction algorith:     Enter an  Algorithm Name .    Enter a  Description .    Select the  Black List  or  White List  radio button.    Select  Lookup File  and enter  Redaction Value  OR/AND    Select  Profiler Sets  from the drop-down menu and enter\n     Redaction Value .    Click  Save .",
            "title": "Free Text Algorithm Framework"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#free-text-redaction-example",
            "text": "Create Input File.    Create input file using notepad. Enter the following text:  The customer Bob Jones is satisfied with the terms of the sales\nagreement. Please call to confirm at 718-223-7896.    Save file as txt.    Create lookup file.    Create a lookup file.    Use notepad to create a txt file and save the file as a TXT.\n    Be sure to hit return after each field. The lookup flat file\n    contains the following data:  Bob\nJones\nAgreement",
            "title": "Free Text Redaction Example"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#create-an-algorithm",
            "text": "You will be prompted for the following information:    For  Algorithm Name , enter  Blacklist_Test1 .    For  Description , enter  Blacklist Test .    Select the  Black List  radio button.    Select  LookUp File .    Enter redaction value  XXXX .    Click  Save .",
            "title": "Create an Algorithm"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#create-rule-set",
            "text": "From the job page go to Rule Set and Click  Create Rule Set .     For  Rule Set Name , enter  Free_ Text_RS .    From the  Connector  drop-down menu, select  Free Text .    Select the  Input File  by clicking the box next to your input\n    file    Click  Save .",
            "title": "Create Rule Set"
        },
        {
            "location": "/Securing_Sensitive_Data/Configuring_Your_Own_Algorithms/#create-masking-job",
            "text": "Use Free_Text Rule Set    Execute Masking job.    The results of the masking job will show the following:  The customer xxxx xxxx is satisfied with the terms\nof the sales xxxx. Please call to confirm at 718-223-7896.  \"Bob,\" \"Jones,\" and \"agreement\" are redacted.",
            "title": "Create Masking Job"
        },
        {
            "location": "/Securing_Sensitive_Data/Creating_Masking_Job/",
            "text": "Creating Masking Job\n\u00b6\n\n\nThis section describes how users can create a masking job.\n\n\nCreating New Jobs\n\u00b6\n\n\nIn the \nEnvironment Overview\n screen, select one of the jobs icons to\ncreate the corresponding job:\n\n\n\n\n\n\nProfile\n\n\n\n\n\n\nMask\n\n\n\n\n\n\n\n\nCreating a New Masking Job\n\u00b6\n\n\nTo create a new masking job:\n\n\n\n\n\n\nClick \nMask\n. The \nCreate Masking Job\n window appears.\n\n\n\n\n\n\n\n\nYou will be prompted for the following information:\n\n\n\n\n\n\nJob Name\n \u2014 A free-form name for the job you are creating.\n    Must be unique across the entire application.\n\n\n\n\n\n\nMasking Method\n \u2014 Select either \nIn-Place\n or\n    \nOn-The-Fly\n. For more information on masking type, see Mask\n    Data. [Need to create this link]\n\n\n\n\n\n\nMulti Tenant\n \u2014 Check box if the job is for a multi-tenant\n    database.\n\n\n\n\nINFO: Provisioning Masked VDBs.\n\n\nA job must be Multi Tenant to use it when creating a masked virtual database (VDB).\n\n\n\n\n\n\n\n\n\nRule Set\n \u2014 Select a rule set that this job will execute\n    against.\n\n\n\n\n\n\nGenerator\n \u2014 The default value is \nDelphix\n.\n\n\n\n\n\n\nRepository Folder name\n \u2014 The folder name in the repository\n    where the objects should be imported.\n\n\n\n\n\n\nParameter File Path\n \u2014 (optional) If checked, this tells\n    Delphix to configure the sessions and workflows to use a parameter\n    file that contains the source and target connection information.\n    If unchecked, the Delphix Engine will generate sessions/workflows\n    that use the connector names as defined within the Delphix Engine,\n    which will require connections with the same names defined within\n    the repository.\n\n\n\n\n\n\nImport Mapplet\n \u2014 (optional) if checked, this tells the Delphix\n    Engine to import mapplets that are assigned to columns in the\n    inventory along with the mappings/sessions/workflows. If\n    unchecked, Delphix will not attempt to import any mapplets that\n    are assigned in the inventory.\n\n\n\n\n\n\nMask Method\n \u2014 Choose either of the following:\n\n\n\n\n\n\nNo. of Streams\n\u2014The number of parallel streams to use when\n    running the jobs. For example, you can select two streams to\n    run two tables in the Rule Set concurrently in the job instead\n    of one table at a time.\n\n\n\n\n\n\nImport\n \u2014 When you click the Run icon, creates the mappings\n    but does not execute the workflow. You later run the job.\n\n\n\n\n\n\nImport and Run\n \u2014 When you click the Run icon, creates the\n    mappings and executes the workflow.\n\n\n\n\n\n\n\n\n\n\nRemote Server\n \u2014 (optional) The remote server that will execute\n    the jobs. This option lets you choose to execute jobs on a remote\n    server, rather than on the local Delphix instance. Note: This is\n    an optional feature for Delphix.\n\n\n\n\n\n\nMin Memory (MB)\n \u2014 (optional) Minimum amount of memory to\n    allocate for the job, in megabytes.\n\n\n\n\n\n\nMax Memory (MB)\n \u2014 (optional) Maximum amount of memory to\n    allocate for the job, in megabytes.\n\n\n\n\n\n\nUpdate Threads\n \u2014 The number of update threads to run in\n    parallel to update the target database.\n\n\n\n\nInfo\n\n\nMultiple threads should not be used if the masking job contains any table without an index. Multi-threaded masking jobs can lead to deadlocks on the database engine.\n\n\nMultiple threads can cause database engine deadlocks for databases using T-SQL If masking jobs fail and a deadlock error exists on the database engine, then reduce the number of threads.\n\n\n\n\n\n\n\n\nCommit Size\n \u2014 (optional) The number of rows to process before\n    issuing a commit to the database.\n\n\n\n\n\n\nFeedback Size\n \u2014 (optional) The number of rows to process\n    before writing a message to the logs. Set this parameter to the\n    appropriate level of detail required for monitoring your job. For\n    example, if you set this number significantly higher than the\n    actual number of rows in a job, the progress for that job will\n    only show 0 or 100%.\n\n\n\n\n\n\nBulk Data\n \u2014 (optional) For In-Place masking only. The default\n    is for this check box to be clear. If you are masking very large\n    tables in-place and require performance improvements, check this\n    box. Delphix will mask data to a flat file, and then use inserts\n    instead of updates to bulk load the target table.\n\n\n\n\n\n\nDisable Constraint\n \u2014 (optional) Whether to automatically\n    disable database constraints. The default is for this check box to\n    be clear and therefore not perform automatic disabling of\n    constraints. For more information about database constraints.\n\n\n\n\n\n\nBatch Update\n \u2014 (optional) Enable or disable use of a batch for\n    updates. A job's statements can either be executed individually,\n    or can be put in a batch file and executed at once, which is\n    faster.\n\n\n\n\n\n\nDisable Trigger\n \u2014 (optional) Whether to automatically disable\n    database triggers. The default is for this check box to be clear\n    and therefore not perform automatic disabling of triggers.\n\n\n\n\n\n\nDrop Index\n \u2014 (optional) Whether to automatically drop indexes\n    on columns which are being masked and automatically re-create the\n    index when the masking job is completed. The default is for this\n    check box to be clear and therefore not perform automatic dropping\n    of indexes.\n\n\n\n\n\n\nPrescript\n \u2014 (optional) Specify the full pathname of a file\n    that contains SQL statements to be run before the job starts, or\n    click \nBrowse\n to specify a file. If you are editing the job and\n    a prescript file is already specified, you can click the\n    \nDelete\n button to remove the file. (The Delete button only\n    appears if a prescript file was already specified.) For\n    information about creating your own prescript files.\n\n\n\n\n\n\nPostscript\n \u2014 (optional) Specify the full pathname of a file\n    that contains SQL statements to be run after the job finishes, or\n    click \nBrowse\n to specify a file. If you are editing the job and\n    a postscript file is already specified, you can click the\n    \nDelete\n button to remove the file. (The Delete button only\n    appears if a postscript file was already specified.) For\n    information about creating your own postscript files.\n\n\n\n\n\n\nComments\n \u2014 (optional) Add comments related to this masking\n    job.\n\n\n\n\n\n\nEmail\n \u2014 (optional) Add e-mail address(es) to which to send\n    status messages.\n\n\n\n\n\n\n\n\n\n\nWhen you are finished, click \nSave\n.",
            "title": "Creating Masking Job"
        },
        {
            "location": "/Securing_Sensitive_Data/Creating_Masking_Job/#creating-masking-job",
            "text": "This section describes how users can create a masking job.",
            "title": "Creating Masking Job"
        },
        {
            "location": "/Securing_Sensitive_Data/Creating_Masking_Job/#creating-new-jobs",
            "text": "In the  Environment Overview  screen, select one of the jobs icons to\ncreate the corresponding job:    Profile    Mask",
            "title": "Creating New Jobs"
        },
        {
            "location": "/Securing_Sensitive_Data/Creating_Masking_Job/#creating-a-new-masking-job",
            "text": "To create a new masking job:    Click  Mask . The  Create Masking Job  window appears.     You will be prompted for the following information:    Job Name  \u2014 A free-form name for the job you are creating.\n    Must be unique across the entire application.    Masking Method  \u2014 Select either  In-Place  or\n     On-The-Fly . For more information on masking type, see Mask\n    Data. [Need to create this link]    Multi Tenant  \u2014 Check box if the job is for a multi-tenant\n    database.   INFO: Provisioning Masked VDBs.  A job must be Multi Tenant to use it when creating a masked virtual database (VDB).     Rule Set  \u2014 Select a rule set that this job will execute\n    against.    Generator  \u2014 The default value is  Delphix .    Repository Folder name  \u2014 The folder name in the repository\n    where the objects should be imported.    Parameter File Path  \u2014 (optional) If checked, this tells\n    Delphix to configure the sessions and workflows to use a parameter\n    file that contains the source and target connection information.\n    If unchecked, the Delphix Engine will generate sessions/workflows\n    that use the connector names as defined within the Delphix Engine,\n    which will require connections with the same names defined within\n    the repository.    Import Mapplet  \u2014 (optional) if checked, this tells the Delphix\n    Engine to import mapplets that are assigned to columns in the\n    inventory along with the mappings/sessions/workflows. If\n    unchecked, Delphix will not attempt to import any mapplets that\n    are assigned in the inventory.    Mask Method  \u2014 Choose either of the following:    No. of Streams \u2014The number of parallel streams to use when\n    running the jobs. For example, you can select two streams to\n    run two tables in the Rule Set concurrently in the job instead\n    of one table at a time.    Import  \u2014 When you click the Run icon, creates the mappings\n    but does not execute the workflow. You later run the job.    Import and Run  \u2014 When you click the Run icon, creates the\n    mappings and executes the workflow.      Remote Server  \u2014 (optional) The remote server that will execute\n    the jobs. This option lets you choose to execute jobs on a remote\n    server, rather than on the local Delphix instance. Note: This is\n    an optional feature for Delphix.    Min Memory (MB)  \u2014 (optional) Minimum amount of memory to\n    allocate for the job, in megabytes.    Max Memory (MB)  \u2014 (optional) Maximum amount of memory to\n    allocate for the job, in megabytes.    Update Threads  \u2014 The number of update threads to run in\n    parallel to update the target database.   Info  Multiple threads should not be used if the masking job contains any table without an index. Multi-threaded masking jobs can lead to deadlocks on the database engine.  Multiple threads can cause database engine deadlocks for databases using T-SQL If masking jobs fail and a deadlock error exists on the database engine, then reduce the number of threads.     Commit Size  \u2014 (optional) The number of rows to process before\n    issuing a commit to the database.    Feedback Size  \u2014 (optional) The number of rows to process\n    before writing a message to the logs. Set this parameter to the\n    appropriate level of detail required for monitoring your job. For\n    example, if you set this number significantly higher than the\n    actual number of rows in a job, the progress for that job will\n    only show 0 or 100%.    Bulk Data  \u2014 (optional) For In-Place masking only. The default\n    is for this check box to be clear. If you are masking very large\n    tables in-place and require performance improvements, check this\n    box. Delphix will mask data to a flat file, and then use inserts\n    instead of updates to bulk load the target table.    Disable Constraint  \u2014 (optional) Whether to automatically\n    disable database constraints. The default is for this check box to\n    be clear and therefore not perform automatic disabling of\n    constraints. For more information about database constraints.    Batch Update  \u2014 (optional) Enable or disable use of a batch for\n    updates. A job's statements can either be executed individually,\n    or can be put in a batch file and executed at once, which is\n    faster.    Disable Trigger  \u2014 (optional) Whether to automatically disable\n    database triggers. The default is for this check box to be clear\n    and therefore not perform automatic disabling of triggers.    Drop Index  \u2014 (optional) Whether to automatically drop indexes\n    on columns which are being masked and automatically re-create the\n    index when the masking job is completed. The default is for this\n    check box to be clear and therefore not perform automatic dropping\n    of indexes.    Prescript  \u2014 (optional) Specify the full pathname of a file\n    that contains SQL statements to be run before the job starts, or\n    click  Browse  to specify a file. If you are editing the job and\n    a prescript file is already specified, you can click the\n     Delete  button to remove the file. (The Delete button only\n    appears if a prescript file was already specified.) For\n    information about creating your own prescript files.    Postscript  \u2014 (optional) Specify the full pathname of a file\n    that contains SQL statements to be run after the job finishes, or\n    click  Browse  to specify a file. If you are editing the job and\n    a postscript file is already specified, you can click the\n     Delete  button to remove the file. (The Delete button only\n    appears if a postscript file was already specified.) For\n    information about creating your own postscript files.    Comments  \u2014 (optional) Add comments related to this masking\n    job.    Email  \u2014 (optional) Add e-mail address(es) to which to send\n    status messages.      When you are finished, click  Save .",
            "title": "Creating a New Masking Job"
        },
        {
            "location": "/Securing_Sensitive_Data/Provision_Masked_VDBs/",
            "text": "Provision Masked VDBs\n\u00b6\n\n\nMasked virtual databases (VDBs) function just like normal VDBs. The only distinction is that the data they contain has been masked by a masking job. Masked VDBs can be replicated to a separate Delphix Engine (in non-prod) without sending the original data that was obfuscated during masking using a process called Selective Data Distribution (SDD). This topic describes how to work with masked VDBs.\n\n\nPrerequisites\n\u00b6\n\n\nBefore attempting to create a Masked VDB, you should be familiar with both Delphix Virtualization and Delphix Masking concepts and workflows.\n\n\nRestrictions\n\u00b6\n\n\n\n\nA single masking job cannot be assigned to multiple VDBs simultaneously. If you are using the same masking ruleset on multiple VDBs, be sure to create a unique job for each VDB to avoid any issues with provisioning or refreshing.\n\n\nProvisioning or refreshing masked VDBs is only supported for Oracle, MS SQL Server and Sysbase. Provisioning or refreshing other types of masked VDBs such as DB2 are not support.\n\n\nYou cannot apply additional masking jobs to a masked VDB or its children.\n\n\nIf a masking job has been applied to a VDB, you cannot create an unmasked snapshot of that VDB.\n\n\nMasking must take place during the process of provisioning a VDB. If an existing VDB has not had a masking job applied to it, then you cannot mask that particular VDB at any point in the future. All the data within the VDB and its parents will be accessible if it is replicated using SDD.\n\n\nWhen selecting a connector to use for Masked Provisioning, a \"basic\" connector must be used \nunless\n you are masking an Oracle Pluggable Database (PDB), in which case an \"advanced\" connector must be used.\n\n\n\n\nIdentifying and Navigating to Masked VDBs\n\u00b6\n\n\nMasked VDBs appear in the Virtualization Engine's Datasets pane, just like regular VDBs. They are most obviously identified by the different icon used to represent them. In addition, a masked VDBs Configuration tab will contain information about the masking job that you applied to it. Generally, anything you can do with an unmasked VDB is also possible with a masked VDB.\n\n\n\nProvisioning Masked VDBs\n\u00b6\n\n\n\n\nIn the Virtualization Engine, associate a masking job with a dSource.\n\n\nUse the dSource provision wizard to provision a VDB with a masking job.\n\n\n\n\nAssociating a Masking Job with the dSource\n\u00b6\n\n\nTo provision a masked VDB, you must first indicate that the masking job you are using is complete and applicable to a particular database. You do this by associating the masking job with a dSource.\n\n\n\n\nIn the \nDatasets\n panel on the left-hand side of the screen, click the dSource to which the masking job is applicable and with which it will be associated.\n\n\nClick the \nConfiguration\n tab.\n\n\n\n\nClick the \nMasking\n tab. \n  \n\n\n\n\n\n\nClick the \npencil\n icon to edit.  All masking jobs on this Delphix Engine that have not been associated with another dSource will be listed on the right-hand side.\n\n\n\n\nSelect the \njob\n you want to associate with this dSource.\n\n\nClick the \ngreen checkmark\n to confirm. \n  \n\n\nRepeat for any other jobs that you want to associate with this dSource at this time.\n\n\n\n\nThe Delphix Engine now considers this masking job to be applicable to this dSource and ready for use. When provisioning from snapshots of this dSource, this masking job will now be available.\n\n\n\n\nNote\n\n\nMasking jobs can also be associated with virtual sources in addition to dSources.\n\n\n\n\nProvisioning a Masked VDB using the dSource Provisioning Wizard\n\u00b6\n\n\nThe steps required to provision a masked VDB are almost identical to the steps required to provision an unmasked VDB. Once you have created a masked VDB, you cannot un-mask it, nor can you alter which masking job it uses. All snapshots in the VDBs TimeFlow will always be masked using the masking method that you selected when you provisioned the masked VDB.\n\n\n\n\nIn the \nDatasets\n panel on the left-hand side of the screen, select the dSource.\n\n\nClick the \nTimeFlow\n tab.\n\n\nClick \nProvision VDB\n icon.\n\n\nReview the information for Installation Home, Database Unique Name, SID, and Database Name. Edit as necessary.\n\n\nReview the Mount Base and Environment User. Edit as necessary.\n\n\nIf you want to use login credentials on the target environment that are different from the login credentials associated with the Environment User, select Specify Privileged Credentials.\n\n\nClick \nNext\n.\n\n\nIf necessary, edit the \nTarget Group\n for the VDB.\n\n\nSelect the \nNone\n option for the Snapshot Policy for theVDB .\n\n\n\n\n\n\nSnapshot Policy Selection\n\n\nFor almost all use cases involving Masked VDBs, a Snapshot Policy of None is appropriate. Using a Snapshot Policy in conjunction with SDD can result in the leak of sensitive data.\n\n\n\n\n\n\nClick \nNext\n.\n\n\nClick \nMask this VDB\n. You will be presented with two options to mask this VDB:\n\n\nSelect an existing masking job: Choose this option if you want to mask using preconfigured Masking Job. Only masking jobs that have been associated with the parent dSource will be available.\n\n\n\n\n\n\nSelecting Unique Masking Jobs\n\n\nIf you are using the same masking ruleset on multiple VDBs, be sure to create a unique job for each VDB to avoid any issues when provisioning or refreshing.\n\n\n\n\n\n\n\n\nMasking using scripts(s): Alternatively, you may define some Configure Clone scripts in the Hooks step to perform masking.\n\n\n\n\n\n\nDefining Configure Clone Hooks to Mask VDB\n\n\nIf you choose to mask using script(s), you must define the Configure Clone hooks to run masking jobs yourself. If you don't define any Configure Clone hooks in the Hooks step, the data will be marked as masked, but it will not be masked.\n\n\n\n\n\n\n\n\nClick \nNext\n.\n\n\n\n\nSpecify any \nPre or Post Scripts\n that should be used during the provisioning process. If the VDB was configured before running the masking job using scripts that impact either user access or the database schema, those same scripts should also be used here. Be sure to define the \nConfigure Clone\n hooks to run the masking job if you choose to mask using script(s) in the Masking step.\n\n\n\n\n\n\n\nClick \nNext\n.\n\n\n\n\nClick \nSubmit\n.\n\n\n\n\nIf you click Actions in the the upper right-hand corner, the Actions sidebar will appear and list an action indicating that masking is running. You can verify this and monitor progress by going to the Masking Engine page and clicking the Monitor tab.\n\n\n\n\n\n\nNote\n\n\nOnce you have created a masked VDB, you can provision its masked data to create additional VDBs, in the same way that you can provision normal VDBs. Since the parent masked VDB contains masked data, child VDBs will only have masked data. This is a great way to distribute multiple independent copies of masked data that is both time- and space-efficient.\n\n\n\n\nRefresh a Masked VDB\n\u00b6\n\n\nYou refresh a masked VDB in exactly the same way as you refresh a normal VDB. As with provisioning a masked VDB, the masking job will be run during the refresh process. \n\n\n\n\nLogin to the Delphix Management application.\n\n\nClick Manage.\n\n\nSelect Datasets.\n\n\nSelect the VDB you want to refresh.\n\n\nClick the Refresh VDB button (2 circular arrows).\n\n\nSelect More Accurate and Next.\n\n\nSelect desired refresh point snapshot or click the eye icon to choose Latest available range, A point in time, or An SCN to refresh from.\n\n\nClick Next. \n\n\nClick Submit to confirm.\n\n\nClick the Actions link to watch the progress of the refresh job.\n\n\nTo see when the VDB was last refreshed/provisioned, check the Time Point on the Status page.\n\n\n\n\nDisassociating a Masking Operation on a dSource\n\u00b6\n\n\nIf a masking job is found to be unsuitable or should be retired, you can disassociate it though the same database card that you used to associate it.\n\n\n\n\nDeselect the job.\n\n\nClick green arrow to confirm.\nNote that this will only prevent the creation of new masked VDBs with this job. It will not alter existing masked VDBs in any way. When disassociating a job, review the existing masked VDBs and consider whether you need to delete or disable any of them.\n\n\n\n\nMasked VDB Data Operations\n\u00b6\n\n\nThe following data operations are available to masked VDBs:\n\n\n\n\nRewind : Alter the database to contain masked data from a previous point in time.  \n\n\nRefresh : Get new data from the parent dSouce and mask it.\n\n\nDisable   : Turn off the database and remove it from the host system. \n\n\nEnable : Turn on the database and make it available on the host system.\n\n\n\n\nVirtualization and Masking Engine Compatibility Matrix\n\u00b6\n\n\n\n\n\n\n\n\nVirtualization Engine Version\n\n\nMasking Engine Version\n\n\n\n\n\n\n\n\n\n\n5.0 releases\n\n\n5.0 releases (minor versions do not need to match)\n\n\n\n\n\n\n5.1 releases\n\n\n5.1 releases (minor versions do not need to match)\n\n\n\n\n\n\n5.2 releases\n\n\n5.2 releases (minor versions do not need to match)\n\n\n\n\n\n\n5.2.5.0 (or later 5.2 minor release)\n\n\n5.2.5.0 (or later 5.2 minor release)\n\n\n\n\n\n\n5.3 releases\n\n\n5.3 releases (minor versions do not need to match)",
            "title": "Provision Masked VDBs"
        },
        {
            "location": "/Securing_Sensitive_Data/Provision_Masked_VDBs/#provision-masked-vdbs",
            "text": "Masked virtual databases (VDBs) function just like normal VDBs. The only distinction is that the data they contain has been masked by a masking job. Masked VDBs can be replicated to a separate Delphix Engine (in non-prod) without sending the original data that was obfuscated during masking using a process called Selective Data Distribution (SDD). This topic describes how to work with masked VDBs.",
            "title": "Provision Masked VDBs"
        },
        {
            "location": "/Securing_Sensitive_Data/Provision_Masked_VDBs/#prerequisites",
            "text": "Before attempting to create a Masked VDB, you should be familiar with both Delphix Virtualization and Delphix Masking concepts and workflows.",
            "title": "Prerequisites"
        },
        {
            "location": "/Securing_Sensitive_Data/Provision_Masked_VDBs/#restrictions",
            "text": "A single masking job cannot be assigned to multiple VDBs simultaneously. If you are using the same masking ruleset on multiple VDBs, be sure to create a unique job for each VDB to avoid any issues with provisioning or refreshing.  Provisioning or refreshing masked VDBs is only supported for Oracle, MS SQL Server and Sysbase. Provisioning or refreshing other types of masked VDBs such as DB2 are not support.  You cannot apply additional masking jobs to a masked VDB or its children.  If a masking job has been applied to a VDB, you cannot create an unmasked snapshot of that VDB.  Masking must take place during the process of provisioning a VDB. If an existing VDB has not had a masking job applied to it, then you cannot mask that particular VDB at any point in the future. All the data within the VDB and its parents will be accessible if it is replicated using SDD.  When selecting a connector to use for Masked Provisioning, a \"basic\" connector must be used  unless  you are masking an Oracle Pluggable Database (PDB), in which case an \"advanced\" connector must be used.",
            "title": "Restrictions"
        },
        {
            "location": "/Securing_Sensitive_Data/Provision_Masked_VDBs/#identifying-and-navigating-to-masked-vdbs",
            "text": "Masked VDBs appear in the Virtualization Engine's Datasets pane, just like regular VDBs. They are most obviously identified by the different icon used to represent them. In addition, a masked VDBs Configuration tab will contain information about the masking job that you applied to it. Generally, anything you can do with an unmasked VDB is also possible with a masked VDB.",
            "title": "Identifying and Navigating to Masked VDBs"
        },
        {
            "location": "/Securing_Sensitive_Data/Provision_Masked_VDBs/#provisioning-masked-vdbs",
            "text": "In the Virtualization Engine, associate a masking job with a dSource.  Use the dSource provision wizard to provision a VDB with a masking job.",
            "title": "Provisioning Masked VDBs"
        },
        {
            "location": "/Securing_Sensitive_Data/Provision_Masked_VDBs/#associating-a-masking-job-with-the-dsource",
            "text": "To provision a masked VDB, you must first indicate that the masking job you are using is complete and applicable to a particular database. You do this by associating the masking job with a dSource.   In the  Datasets  panel on the left-hand side of the screen, click the dSource to which the masking job is applicable and with which it will be associated.  Click the  Configuration  tab.   Click the  Masking  tab. \n      Click the  pencil  icon to edit.  All masking jobs on this Delphix Engine that have not been associated with another dSource will be listed on the right-hand side.   Select the  job  you want to associate with this dSource.  Click the  green checkmark  to confirm. \n    Repeat for any other jobs that you want to associate with this dSource at this time.   The Delphix Engine now considers this masking job to be applicable to this dSource and ready for use. When provisioning from snapshots of this dSource, this masking job will now be available.   Note  Masking jobs can also be associated with virtual sources in addition to dSources.",
            "title": "Associating a Masking Job with the dSource"
        },
        {
            "location": "/Securing_Sensitive_Data/Provision_Masked_VDBs/#provisioning-a-masked-vdb-using-the-dsource-provisioning-wizard",
            "text": "The steps required to provision a masked VDB are almost identical to the steps required to provision an unmasked VDB. Once you have created a masked VDB, you cannot un-mask it, nor can you alter which masking job it uses. All snapshots in the VDBs TimeFlow will always be masked using the masking method that you selected when you provisioned the masked VDB.   In the  Datasets  panel on the left-hand side of the screen, select the dSource.  Click the  TimeFlow  tab.  Click  Provision VDB  icon.  Review the information for Installation Home, Database Unique Name, SID, and Database Name. Edit as necessary.  Review the Mount Base and Environment User. Edit as necessary.  If you want to use login credentials on the target environment that are different from the login credentials associated with the Environment User, select Specify Privileged Credentials.  Click  Next .  If necessary, edit the  Target Group  for the VDB.  Select the  None  option for the Snapshot Policy for theVDB .    Snapshot Policy Selection  For almost all use cases involving Masked VDBs, a Snapshot Policy of None is appropriate. Using a Snapshot Policy in conjunction with SDD can result in the leak of sensitive data.    Click  Next .  Click  Mask this VDB . You will be presented with two options to mask this VDB:  Select an existing masking job: Choose this option if you want to mask using preconfigured Masking Job. Only masking jobs that have been associated with the parent dSource will be available.    Selecting Unique Masking Jobs  If you are using the same masking ruleset on multiple VDBs, be sure to create a unique job for each VDB to avoid any issues when provisioning or refreshing.     Masking using scripts(s): Alternatively, you may define some Configure Clone scripts in the Hooks step to perform masking.    Defining Configure Clone Hooks to Mask VDB  If you choose to mask using script(s), you must define the Configure Clone hooks to run masking jobs yourself. If you don't define any Configure Clone hooks in the Hooks step, the data will be marked as masked, but it will not be masked.     Click  Next .   Specify any  Pre or Post Scripts  that should be used during the provisioning process. If the VDB was configured before running the masking job using scripts that impact either user access or the database schema, those same scripts should also be used here. Be sure to define the  Configure Clone  hooks to run the masking job if you choose to mask using script(s) in the Masking step.    Click  Next .   Click  Submit .   If you click Actions in the the upper right-hand corner, the Actions sidebar will appear and list an action indicating that masking is running. You can verify this and monitor progress by going to the Masking Engine page and clicking the Monitor tab.    Note  Once you have created a masked VDB, you can provision its masked data to create additional VDBs, in the same way that you can provision normal VDBs. Since the parent masked VDB contains masked data, child VDBs will only have masked data. This is a great way to distribute multiple independent copies of masked data that is both time- and space-efficient.",
            "title": "Provisioning a Masked VDB using the dSource Provisioning Wizard"
        },
        {
            "location": "/Securing_Sensitive_Data/Provision_Masked_VDBs/#refresh-a-masked-vdb",
            "text": "You refresh a masked VDB in exactly the same way as you refresh a normal VDB. As with provisioning a masked VDB, the masking job will be run during the refresh process.    Login to the Delphix Management application.  Click Manage.  Select Datasets.  Select the VDB you want to refresh.  Click the Refresh VDB button (2 circular arrows).  Select More Accurate and Next.  Select desired refresh point snapshot or click the eye icon to choose Latest available range, A point in time, or An SCN to refresh from.  Click Next.   Click Submit to confirm.  Click the Actions link to watch the progress of the refresh job.  To see when the VDB was last refreshed/provisioned, check the Time Point on the Status page.",
            "title": "Refresh a Masked VDB"
        },
        {
            "location": "/Securing_Sensitive_Data/Provision_Masked_VDBs/#disassociating-a-masking-operation-on-a-dsource",
            "text": "If a masking job is found to be unsuitable or should be retired, you can disassociate it though the same database card that you used to associate it.   Deselect the job.  Click green arrow to confirm.\nNote that this will only prevent the creation of new masked VDBs with this job. It will not alter existing masked VDBs in any way. When disassociating a job, review the existing masked VDBs and consider whether you need to delete or disable any of them.",
            "title": "Disassociating a Masking Operation on a dSource"
        },
        {
            "location": "/Securing_Sensitive_Data/Provision_Masked_VDBs/#masked-vdb-data-operations",
            "text": "The following data operations are available to masked VDBs:   Rewind : Alter the database to contain masked data from a previous point in time.    Refresh : Get new data from the parent dSouce and mask it.  Disable   : Turn off the database and remove it from the host system.   Enable : Turn on the database and make it available on the host system.",
            "title": "Masked VDB Data Operations"
        },
        {
            "location": "/Securing_Sensitive_Data/Provision_Masked_VDBs/#virtualization-and-masking-engine-compatibility-matrix",
            "text": "Virtualization Engine Version  Masking Engine Version      5.0 releases  5.0 releases (minor versions do not need to match)    5.1 releases  5.1 releases (minor versions do not need to match)    5.2 releases  5.2 releases (minor versions do not need to match)    5.2.5.0 (or later 5.2 minor release)  5.2.5.0 (or later 5.2 minor release)    5.3 releases  5.3 releases (minor versions do not need to match)",
            "title": "Virtualization and Masking Engine Compatibility Matrix"
        },
        {
            "location": "/Securing_Sensitive_Data/Monitoring_Masking_Job/",
            "text": "Monitoring Masking Job\n\u00b6\n\n\nThis section describes how users can monitor the progress of a masking\njob.\n\n\nRunning and Stopping Jobs from the Environment Overview Screen\n\u00b6\n\n\nTo run or rerun a job from the Environment Overview screen:\n\n\n\n\nClick the Run icon (play icon) in the Action column for the\n    desired job.\n\n\n\n\nThe Run icon changes to a Stop icon while the job is running. When the\njob is complete, the Status changes.\n\n\nTo stop a running job from the Environment Overview screen:\n\n\n\n\n\n\nLocate the job you want to stop.\n\n\n\n\n\n\nIn the job's Action column, click the Stop icon.\n\n\n\n\n\n\nA popup appears asking, \"Are you sure you want to stop job?\" Click\n    OK.\n\n\n\n\n\n\nWhen the job has been stopped, its status changes.\n\n\n\n\n\n\nAfter the job completes successfully, return to the Inventory and\n    check that the Domain and Method populated automatically for\n    sensitive data. Sample screenshot below.",
            "title": "Monitoring Masking Job"
        },
        {
            "location": "/Securing_Sensitive_Data/Monitoring_Masking_Job/#monitoring-masking-job",
            "text": "This section describes how users can monitor the progress of a masking\njob.",
            "title": "Monitoring Masking Job"
        },
        {
            "location": "/Securing_Sensitive_Data/Monitoring_Masking_Job/#running-and-stopping-jobs-from-the-environment-overview-screen",
            "text": "To run or rerun a job from the Environment Overview screen:   Click the Run icon (play icon) in the Action column for the\n    desired job.   The Run icon changes to a Stop icon while the job is running. When the\njob is complete, the Status changes.  To stop a running job from the Environment Overview screen:    Locate the job you want to stop.    In the job's Action column, click the Stop icon.    A popup appears asking, \"Are you sure you want to stop job?\" Click\n    OK.    When the job has been stopped, its status changes.    After the job completes successfully, return to the Inventory and\n    check that the Domain and Method populated automatically for\n    sensitive data. Sample screenshot below.",
            "title": "Running and Stopping Jobs from the Environment Overview Screen"
        },
        {
            "location": "/Securing_Sensitive_Data/Masking_Job_Wizard/",
            "text": "Masking Job Wizard\n\u00b6\n\n\nThe Delphix Masking job wizard enables users to create and modify masking jobs.\nWhile the wizard facilitates a number of workflows and operations, more\nadvanced functionality and a finer control of features is available directly\nin the masking application. The Job Wizard currently functions only with certain\ndata platforms, but these constraints do not apply when working directly in the\nmasking application.\n\n\nSupported Data Platforms\n\u00b6\n\n\nThe following data platforms are currently supported from within the Job Wizard:\n - Oracle Database\n - RDS Oracle Database\n - MSSQL Server Database\n - Sybase Database\n\n\nThis restricted list only affects your use of the wizard; an expanded number of\nplatforms are supported directly in the masking application. Some operations\nwithin the Job Wizard are also limited. See below for details.\n\n\nSupported Operations\n\u00b6\n\n\nWhile creating a masking job in the Job Wizard, you are able to do the following:\n\n\n\n\nCreate a new application or use an existing application\n\n\nCreate a new environment or use an existing environment\n\n\nCreate a new connector*\n\n\nCreate a new rule set\n\n\nUpdate inventory*\n\n\nCreate a masking job*\n\n\nUpdate a masking job\n\n\nChange the connector for an existing job\n\n\nChange the rule set for an existing connector\n\n\nRun a newly created job immediately\n\n\nRun an updated job immediately after the update\n\n\n\n\n\n\nNote\n\n\nOperations marked with an asterisk are limited in the Job Wizard but fully supported in the main application.\n\n\n\n\nWhat is Not Supported in the Wizard\n\u00b6\n\n\nThe following data platforms and operations are not supported in the Job Wizard.\nTo access additional functionality, use the main masking application.\n\n\nUnsupported Data Types\n\u00b6\n\n\nThe following data types are supported when using the main masking application\nbut are not currently supported in the Job Wizard:\n\n\n\n\nDB2 Database\n\n\nPostgreSQL Database\n\n\nGeneric Database\n\n\nDelimited File\n\n\nExcel Sheet File\n\n\nFixed File\n\n\nMainframe File\n\n\nXML File\n\n\n\n\nUnsupported Operations\n\u00b6\n\n\nThe following operations are not yet supported from within the Job Wizard:\n\n\n\n\nCreating any connector or rule set for an unsupported data type\n\n\nDeleting any application, environment, connector, rule set, or masking job\n\n\nImporting or exporting any object\n\n\nUpdating an environment\n\n\nCreating a connector using Advanced mode\n\n\nUpdating a connector\n\n\nUpdating a rule set\n\n\nCreating a job for an unsupported data type\n\n\nModifying a job for an unsupported data type\n\n\nMonitoring running jobs\n\n\nCreating, editing, deleting, or running any Profile jobs\n\n\n\n\nOpening the Masking Job Wizard\n\u00b6\n\n\nWhen you first login to masking, the welcome screen offers a link to learn more\nor begin masking immediately. To open the Job Wizard, click Run on the welcome\npage.\n\n\n\nTo use the Job Wizard from the masking application, click the Create Job button\nin the upper right-hand corner, as highlighted in the screenshot below.\n\n\n\n\nCreating a New Masking Job\n\u00b6\n\n\nThe Job Wizard makes creating a new masking job much easier by guiding you\nthrough the process. You can create new objects or choose to use existing ones\nthat have already been defined.  When creating a new masking job, the Job Wizard\nfollows this sequence:\n\n\n\n\nJob Naming\n\n\nApplication/Environment Selection\n\n\nConnection Selection\n\n\nRule Set Selection\n\n\nInventory Selection\n\n\nSummary Page\n\n\n\n\nYou can navigate back and forth through the pages of the Job Wizard.\n\n\n\n\nNote\n\n\nIf the product times out due to long inactivity, you will need to start over. |\n\n\n\n\nTo create a new masking Job using the new Job Wizard, follow the procedure below:\n\n\n\n\nLog into your Delphix Masking Engine and from the Welcome screen select  Run.\n\n\nSelect the New radio button and enter a name for your Masking job.\n   \n\n\nClick Next.\n\n\nFrom the drop-down menu select an Application and Environment. If none exist use the Add button to add one.\n   \n\n\nClick Next.\n\n\nSelect a Connector from the drop-down menu. If none exists select the Add button, then use the Add Connector dialog to add a new connector. The Job Wizard only supports the following  Connector types:\n\n\nDatabase - MS SQL\n\n\nDatabase - Oracle\n\n\nDatabase - RDS Oracle\n\n\nDatabase - Sybase\n   \n\n7.Click Next.\n\n\nOn the Rule Set screen select an existing Rule set or create a new one by clicking the Add button.\n  \n\n\nClick Next.\n\n\nFrom the Inventory screen select how your data will be masked. In the screenshot below we are masking subscriber last names.\n   \n\n\nClick Next.\n\n\nThe final screen of the Job Wizard displays a Summary of your selections.\n   \n\n\nClicking Run Masking Job Now and go to Monitor progress, saves your job and runs it immediately. Save Job allows you to save your job and run it at a later date. Note: Selecting this option means your data will not be masked until you run the job.\n\n\n\n\nWhen Objects Are Saved\n\u00b6\n\n\nApplication, environment, connector, and rule set objects are created and\npersist after you click the Add button and see a success message. If you cancel\nthe Job Wizard before completing the job setup, the objects you created will be\nsaved, and they will be available for use the next time you launch the Job Wizard.\n\n\nThe Inventory definition is saved when you change the selection of a table or\ncolumn, or when another View filter is applied.\n\n\nThe masking job is saved when you click either Save Job or Run Masking Job Now\nand go to Monitor progress and a success message is returned on the\nSummary screen.\n\n\nUpdating an Existing Masking Job\n\u00b6\n\n\nYou can use the Job Wizard to modify any masking job that targets a supported\ndata type.\n\n\n\n\nOn the Job screen of the Job Wizard, select Modify Existing\n\n\nFrom the list of available jobs slect the one you want to modify. This list only shows jobs that are supported in the wizard. You can filter the job list by selecting the filter icon .\n\n\nOnce you select a job, you can change the following as part of the Modify flow:\n\n\nChange/create new connector\n\n\nChange/create new rule set\n\n\nUpdate inventory\n\n\nSave or run the modified job\n\n\n\n\nYou cannot alter application and environment settings as part of the Modify\n flow, but you can do so in the main masking application.",
            "title": "Masking Job Wizard"
        },
        {
            "location": "/Securing_Sensitive_Data/Masking_Job_Wizard/#masking-job-wizard",
            "text": "The Delphix Masking job wizard enables users to create and modify masking jobs.\nWhile the wizard facilitates a number of workflows and operations, more\nadvanced functionality and a finer control of features is available directly\nin the masking application. The Job Wizard currently functions only with certain\ndata platforms, but these constraints do not apply when working directly in the\nmasking application.",
            "title": "Masking Job Wizard"
        },
        {
            "location": "/Securing_Sensitive_Data/Masking_Job_Wizard/#supported-data-platforms",
            "text": "The following data platforms are currently supported from within the Job Wizard:\n - Oracle Database\n - RDS Oracle Database\n - MSSQL Server Database\n - Sybase Database  This restricted list only affects your use of the wizard; an expanded number of\nplatforms are supported directly in the masking application. Some operations\nwithin the Job Wizard are also limited. See below for details.",
            "title": "Supported Data Platforms"
        },
        {
            "location": "/Securing_Sensitive_Data/Masking_Job_Wizard/#supported-operations",
            "text": "While creating a masking job in the Job Wizard, you are able to do the following:   Create a new application or use an existing application  Create a new environment or use an existing environment  Create a new connector*  Create a new rule set  Update inventory*  Create a masking job*  Update a masking job  Change the connector for an existing job  Change the rule set for an existing connector  Run a newly created job immediately  Run an updated job immediately after the update    Note  Operations marked with an asterisk are limited in the Job Wizard but fully supported in the main application.",
            "title": "Supported Operations"
        },
        {
            "location": "/Securing_Sensitive_Data/Masking_Job_Wizard/#what-is-not-supported-in-the-wizard",
            "text": "The following data platforms and operations are not supported in the Job Wizard.\nTo access additional functionality, use the main masking application.",
            "title": "What is Not Supported in the Wizard"
        },
        {
            "location": "/Securing_Sensitive_Data/Masking_Job_Wizard/#unsupported-data-types",
            "text": "The following data types are supported when using the main masking application\nbut are not currently supported in the Job Wizard:   DB2 Database  PostgreSQL Database  Generic Database  Delimited File  Excel Sheet File  Fixed File  Mainframe File  XML File",
            "title": "Unsupported Data Types"
        },
        {
            "location": "/Securing_Sensitive_Data/Masking_Job_Wizard/#unsupported-operations",
            "text": "The following operations are not yet supported from within the Job Wizard:   Creating any connector or rule set for an unsupported data type  Deleting any application, environment, connector, rule set, or masking job  Importing or exporting any object  Updating an environment  Creating a connector using Advanced mode  Updating a connector  Updating a rule set  Creating a job for an unsupported data type  Modifying a job for an unsupported data type  Monitoring running jobs  Creating, editing, deleting, or running any Profile jobs",
            "title": "Unsupported Operations"
        },
        {
            "location": "/Securing_Sensitive_Data/Masking_Job_Wizard/#opening-the-masking-job-wizard",
            "text": "When you first login to masking, the welcome screen offers a link to learn more\nor begin masking immediately. To open the Job Wizard, click Run on the welcome\npage.  \nTo use the Job Wizard from the masking application, click the Create Job button\nin the upper right-hand corner, as highlighted in the screenshot below.",
            "title": "Opening the Masking Job Wizard"
        },
        {
            "location": "/Securing_Sensitive_Data/Masking_Job_Wizard/#creating-a-new-masking-job",
            "text": "The Job Wizard makes creating a new masking job much easier by guiding you\nthrough the process. You can create new objects or choose to use existing ones\nthat have already been defined.  When creating a new masking job, the Job Wizard\nfollows this sequence:   Job Naming  Application/Environment Selection  Connection Selection  Rule Set Selection  Inventory Selection  Summary Page   You can navigate back and forth through the pages of the Job Wizard.   Note  If the product times out due to long inactivity, you will need to start over. |   To create a new masking Job using the new Job Wizard, follow the procedure below:   Log into your Delphix Masking Engine and from the Welcome screen select  Run.  Select the New radio button and enter a name for your Masking job.\n     Click Next.  From the drop-down menu select an Application and Environment. If none exist use the Add button to add one.\n     Click Next.  Select a Connector from the drop-down menu. If none exists select the Add button, then use the Add Connector dialog to add a new connector. The Job Wizard only supports the following  Connector types:  Database - MS SQL  Database - Oracle  Database - RDS Oracle  Database - Sybase\n    \n7.Click Next.  On the Rule Set screen select an existing Rule set or create a new one by clicking the Add button.\n    Click Next.  From the Inventory screen select how your data will be masked. In the screenshot below we are masking subscriber last names.\n     Click Next.  The final screen of the Job Wizard displays a Summary of your selections.\n     Clicking Run Masking Job Now and go to Monitor progress, saves your job and runs it immediately. Save Job allows you to save your job and run it at a later date. Note: Selecting this option means your data will not be masked until you run the job.",
            "title": "Creating a New Masking Job"
        },
        {
            "location": "/Securing_Sensitive_Data/Masking_Job_Wizard/#when-objects-are-saved",
            "text": "Application, environment, connector, and rule set objects are created and\npersist after you click the Add button and see a success message. If you cancel\nthe Job Wizard before completing the job setup, the objects you created will be\nsaved, and they will be available for use the next time you launch the Job Wizard.  The Inventory definition is saved when you change the selection of a table or\ncolumn, or when another View filter is applied.  The masking job is saved when you click either Save Job or Run Masking Job Now\nand go to Monitor progress and a success message is returned on the\nSummary screen.",
            "title": "When Objects Are Saved"
        },
        {
            "location": "/Securing_Sensitive_Data/Masking_Job_Wizard/#updating-an-existing-masking-job",
            "text": "You can use the Job Wizard to modify any masking job that targets a supported\ndata type.   On the Job screen of the Job Wizard, select Modify Existing  From the list of available jobs slect the one you want to modify. This list only shows jobs that are supported in the wizard. You can filter the job list by selecting the filter icon .  Once you select a job, you can change the following as part of the Modify flow:  Change/create new connector  Change/create new rule set  Update inventory  Save or run the modified job   You cannot alter application and environment settings as part of the Modify\n flow, but you can do so in the main masking application.",
            "title": "Updating an Existing Masking Job"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Working_with_Multiple_Masking_Engines/",
            "text": "Your organization may have more than one masking engine, and in certain\ncircumstances, it may want to coordinate the operation of those engines.\nIn particular, there are two specific scenarios in which an organization \ncould benefit from some level of interaction and orchestration between multiple\nmasking engines.\n\n\nSoftware Development Life Cycle (SDLC)\n\u00b6\n\n\nUsing an SDLC process often requires setting up multiple masking\nengines, each for a different part of the cycle (Development, QA,\nProduction).\n\n\n\n\nDistributed Execution\n\u00b6\n\n\nFor many organizations, the size of the profiling and masking workloads\nrequires more than one production masking engine. These masking engines\ncan be identical in configuration or be partially equivalent depending\non the organization's needs.\n\n\n\n\nFor both of these use cases, you will need to be able to move various\nobjects between masking engines. These objects may include the\nfollowing:\n\n\n\n\nAlgorithms\n\n\nConnectors\n\n\nDomains\n\n\nFile Formats\n\n\nInventories\n\n\nMasking Jobs\n\n\nProfile Expressions\n\n\nProfile Jobs\n\n\nProfile Sets\n\n\nRulesets\n\n\n\n\nYou can move a subset of these objects between engines using the Masking\nV5 APIs. See the following sections for instructions.\n\n\nBest Practice Guide & Example Architectures for Synchronizing\n\u00b6\n\n\nEngine synchronization provides a general and flexible way to move\nmasking algorithms and objects necessary to run an identical job on\nanother engine. It is recommended that the syncable objects move in only\none direction. That is, objects should be exported from one engine and\nimported into others but should not go in the other direction. This\nrecommendation is primarily to simplify management of which objects \nexist on which engine.\n\n\nTwo example architectures are described below. Note that the two\narchitectures could be combined by having multiple production engines\ninstead of a single one.\n\n\nHorizontal Scale\n\u00b6\n\n\nThe first architecture aims to address the problem of horizontal scale\n-- that is, achieving consistent masking across a large data estate by\ndeploying multiple masking engines. In this architecture, syncable\nobjects are authored on one engine, labeled \u201cControl Masking Engine\u201d in\nthe diagram below. Those objects are then distributed to \u201cCompute\nMasking Engines\u201d using the engine synchronization APIs. The synchronized\nalgorithms and masking jobs will produce the same masked output on all\nof the engines, thus enabling large data estates to be masked\nconsistently.\n\n\n\n\nSDLC\n\u00b6\n\n\nThe second architecture addresses the desire to author algorithms on one\nengine, to test and certify them on another, and finally to deploy them\nto a production engine. Here, algorithms are authored on the first\nengine, labeled \u201cDev Engine\u201d in the diagram below. When the developer is\nsatisfied, the algorithms are exported from the Dev Engine and imported\nto the QA Engine where they can be tested and certified. Finally, they\nare exported from the QA engine and imported to the production engine.",
            "title": "Working with Multiple Masking Engines"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Working_with_Multiple_Masking_Engines/#software-development-life-cycle-sdlc",
            "text": "Using an SDLC process often requires setting up multiple masking\nengines, each for a different part of the cycle (Development, QA,\nProduction).",
            "title": "Software Development Life Cycle (SDLC)"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Working_with_Multiple_Masking_Engines/#distributed-execution",
            "text": "For many organizations, the size of the profiling and masking workloads\nrequires more than one production masking engine. These masking engines\ncan be identical in configuration or be partially equivalent depending\non the organization's needs.   For both of these use cases, you will need to be able to move various\nobjects between masking engines. These objects may include the\nfollowing:   Algorithms  Connectors  Domains  File Formats  Inventories  Masking Jobs  Profile Expressions  Profile Jobs  Profile Sets  Rulesets   You can move a subset of these objects between engines using the Masking\nV5 APIs. See the following sections for instructions.",
            "title": "Distributed Execution"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Working_with_Multiple_Masking_Engines/#best-practice-guide-example-architectures-for-synchronizing",
            "text": "Engine synchronization provides a general and flexible way to move\nmasking algorithms and objects necessary to run an identical job on\nanother engine. It is recommended that the syncable objects move in only\none direction. That is, objects should be exported from one engine and\nimported into others but should not go in the other direction. This\nrecommendation is primarily to simplify management of which objects \nexist on which engine.  Two example architectures are described below. Note that the two\narchitectures could be combined by having multiple production engines\ninstead of a single one.",
            "title": "Best Practice Guide &amp; Example Architectures for Synchronizing"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Working_with_Multiple_Masking_Engines/#horizontal-scale",
            "text": "The first architecture aims to address the problem of horizontal scale\n-- that is, achieving consistent masking across a large data estate by\ndeploying multiple masking engines. In this architecture, syncable\nobjects are authored on one engine, labeled \u201cControl Masking Engine\u201d in\nthe diagram below. Those objects are then distributed to \u201cCompute\nMasking Engines\u201d using the engine synchronization APIs. The synchronized\nalgorithms and masking jobs will produce the same masked output on all\nof the engines, thus enabling large data estates to be masked\nconsistently.",
            "title": "Horizontal Scale"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Working_with_Multiple_Masking_Engines/#sdlc",
            "text": "The second architecture addresses the desire to author algorithms on one\nengine, to test and certify them on another, and finally to deploy them\nto a production engine. Here, algorithms are authored on the first\nengine, labeled \u201cDev Engine\u201d in the diagram below. When the developer is\nsatisfied, the algorithms are exported from the Dev Engine and imported\nto the QA Engine where they can be tested and certified. Finally, they\nare exported from the QA engine and imported to the production engine.",
            "title": "SDLC"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/",
            "text": "Masking API Call Concepts\n\u00b6\n\n\nSyncable object\n\u00b6\n\n\nSyncable objects are external representations of objects within the\nmasking engine that can be exported from one engine and imported into\nanother. EngineSync currently supports exporting a subset of algorithms,\nthe encryption key and all the objects necessary for a masking job.\nNote: We do not currently support Mainframe masking jobs.\n\n\nObject Identifiers and Types\n\u00b6\n\n\nEngineSync uses object identifiers to name unique objects within the\nengine. The follow object types are currently supported:\n\n\n\n\nDATABASE_CONNECTOR\n\n\nDATABASE_RULESET\n\n\nDOMAIN\n\n\nFILE_CONNECTOR\n\n\nFILE_FORMAT\n\n\nFILE_RULESET\n\n\nGLOBAL_OBJECT\n\n\nKEY\n\n\nCertain algorithms:\n\n\nBINARYLOOKUP\n\n\nCLEANSING\n\n\nDATE_SHIFT\n\n\nLOOKUP\n\n\nMIN_MAX\n\n\nREDACTION\n\n\nSEGMENT\n\n\nTOKENIZATION\n\n\nMAPPLET\n\n\n\n\n\n\nMASKING_JOB\n\n\nPROFILE_EXPRESSION\n\n\nPROFILE_JOB\n\n\nPROFILE_SET\n\n\n\n\nThe following lists the object types that are simply for the purpose of\nreferencing a particular state of the exported object. These are not\nmeant to be exported by request. The functions of these are further\nexplained in the latter sections.\n\n\n\n\nALGORITHM_REFERENCE\n\n\nDOMAIN_REFERENCE\n\n\nPROFILE_EXPRESSION_REFERENCE\n\n\nPROFILE_SET_REFERENCE\n\n\nSOURCE_DATABASE_CONNECTOR\n\n\nSOURCE_FILE_CONNECTOR\n\n\n\n\nDependencies\n\u00b6\n\n\nMost objects within the Masking Engine are compositional. In order to\nproperly capture the behavior of a syncable object, you must export its\ndependencies along with the object itself. Fortunately, all the\nnecessary dependencies are exported along with the object you request;\nthus, it is not something you need to keep track of and worry about.\n\n\nSyncable Object dependencies relationship\n\n\nNote: Green represents global objects (objects that are central to the\nentire engine), and blue represents objects that need to be a part of an\nenvironment\n\n\n\n\nObject Revision Tracking\n\u00b6\n\n\nThe revision hash is used to help you determine whether the behavior of a syncable object is the\nsame between engines. Because objects within the Masking Engine are\ncompositional, the behavior of an object is influenced by all of its\ndependencies. When a syncable object is listed or exported, the\nMasking Engine computes a revision_hash, which uniquely identifies the\nobject\u2019s behavior.\n\n\nThe revision_hash is a SHA1 hash that represents that object\u2019s state,\nas well as the state of all objects it depends on. If two objects have\nthe same revision hash, it is safe to assume that the behavior the\nobjects is the same. However, it is possible for two objects to have the\nsame behavior but have divergent revision hashes. For example, you could\nhave two lookup algorithms with the same name, lookup file, and key, and\nthey do not necessarily guaranteed to have the same revision hash.\n\n\n\n\nNote\n\n\nThe revision_hash does not change when the password or the ssh key for either the FILE_CONNECTOR or DATABASE_CONNECTOR is updated. This is intentionally done because we do not export the password or the ssh key for security purposes. This allows users to update the password after import without changing the revision_hash. If a user is \noverriding\n a connector that already has a password set, the import \ndoes not\n reset the password and will leave the current, pre-import value.\n\n\n\n\nExport Document\n\u00b6\n\n\nYou can export one or more syncable objects that are listed in the\n\n/syncable-objects\n endpoint. The export document will include the set\nof objects that you requested for export and all of their dependencies that are\nrequired to properly import those objects into another engine.\n\n\nThe export document is exported as an opaque blob. Do not edit it\noutside of the Masking Engine.\n\n\nExport Document Encryption\n\u00b6\n\n\nYou can request that the export document be encrypted using a\npassphrase. Once the document is encrypted with the passphrase, the\nengine forgets the passphrase. You will need to provide the same\npassphrase during import to decrypt the document.\n\n\nDigital Signature\n\u00b6\n\n\nIn order to detect accidental or malicious modification of the export\ndocument, each document is digitally signed. If the export document does\nnot match its expected digital signature, a Masking Engine will not\nimport the document.\n\n\nOverwrite\n\u00b6\n\n\nWhen an object to be imported has the same name as a currently existing\nobject, importing it will cause the other object to be changed. Since\nthis might not be intended, we offer a flag called force_overwrite. If\nforce_overwrite is set to false and doing the import will change an\nexisting object on the masking engine, we fail the import. This workflow\nis shown below.\n\n\n\n\nAttempting to Import Identical Objects\n\u00b6\n\n\nThe Masking Engine checks for the existence of the same object contents\nduring the import of an object. If it is determined that the engine and\nthe document being imported contain the same content, a result of\nSUCCESS will be returned without repeating the work of a full import.\nFor example, importing an entire ruleset with hundreds of thousands of\ntables can be quite time consuming, and this should not be repeated if\nthe same object already exists. If the object content matches and we\nskip the full import we note this in the application log.\n\n\nBelow is an example log statement when an identical database connector\nwas imported:\n\n\n2017-07-19 10:17:06,075 [http-nio-8282-exec-4] INFO\nc.d.s.marshallers.SyncableMarshaller - Skipping import process for\n{  \n\"objectType\": \"DATABASE_CONNECTOR\",  \n\"id\": {  \n\"@type\": \"type.googleapis.com/IntegerIdentifier\",  \n\"id\": 1  \n}  \n}, due to no discrepancy between the existing and importing object\n\n\n\n\n\n\u00b6\n\n\nDepending on the object type, some define an object by a String (name)\nand some by an Integer (object id). Objects that can have the same name\nin multiple environments, such as connectors, rulesets, and masking\njobs, are exported based on a unique id associated with them. Global\nobjects, which do not have overlapping names, are exported and\nidentified based on their names. Something to note here is that objects\nexported based on their ids will overwrite the object with the \nsame\nname\n rather than the same id. This means that for all importing\nobjects, we define the identity of an object to be based on the name in\nthe same environment.\n\n\nFor example, if I export a database connector named \ntestConnector\n with\nthe following export object metadata:\n\n\n{\n  \n\n\"objectIdentifier\"\n:\n \n{\n  \n\n\"id\"\n:\n \n5\n  \n\n},\n\n\n\"objectType\"\n:\n \n\"DATABASE_CONNECTOR\"\n,\n\n\n\"revisionHash\"\n:\n \n\"68eaffef400e426520a5fcbb683419db3be53317\"\n\n\n}\n\n\n\n\n\n\nAnd then I import this object into some engine\u2019s environment with the\nfollowing list of connectors:\n\n\n\n\n\n\n\n\nid\n\n\nconnector name\n\n\nmore information\n\n\n\n\n\n\n\n\n\n\n1\n\n\ntestConnector\n\n\n...\n\n\n\n\n\n\n5\n\n\notherConnector\n\n\n...\n\n\n\n\n\n\n\n\ntestConnector\n of id 1 will be overwritten, instead of\n\notherConnector\n.\n\n\nOverwrite of the Encryption Key\n\u00b6\n\n\nThe global encryption key is somewhat special in that it always exists.\nSpecifying \nforce_overwrite=false\n will always fail to import the\nencryption key unless the encryption key has been previously\nsynchronized using \nforce_overwrite=true\n.\n\n\nSpecifying \nforce_overwrite=true\n will always overwrite the engine\u2019s\nencryption key with the contents of the encryption key in the export\ndocument.\n\n\nError handling\n\u00b6\n\n\nExport documents often have multiple objects to be imported at once. For\nexample, when exporting a database ruleset, you will export both the\ndatabase ruleset and the database connector since a ruleset depends on a\nconnector.\n\n\nThe engine will import one object at a time, where the dependencies are\nimported first. If there is an error importing an object, the import\nprocess will abort and all objects that have successfully been imported\nduring this request will get rolled back. For example, say you are\nimporting objects A, B, and C. Import successfully imports A. During the\nimport of B, the engine encounters an error. The import of A will roll\nback, and import of C will never execute. This will leave the engine in\na state identical to the one it was in prior to the failed import.\n\n\nConcurrent Sync Operations\n\u00b6\n\n\nTo prevent race conditions with concurrent imports and jobs running, we\ncurrently do not allow concurrent import operations. We also do not\nallow imports while masking jobs or exports are running. It is best to\ndo imports when a machine is not running jobs or other exports in order\nto guarantee that the final state of each of those operations is as\nexpected. If they are done at the same time, the operations will fail\nwith relevant error messages.\n\n\nGlobal Objects\n\u00b6\n\n\nGLOBAL_OBJECT is a syncable object type that is a collection of all\nsyncable algorithms, DOMAIN(s), PROFILE_SET(s), PROFILE_EXPRESSION(s)\nand KEY (global key). This represents\nobjects in the Masking Engine that are available across all\nenvironments, and are not a part of any specific environment. When a\nuser requests to export GLOBAL_OBJECT, every syncable algorithm, profile\nset, profile expression and\ndomain on the engine will be exported as the bundle. If a DOMAIN,\nPROFILE_SET, or PROFILE_EXPRESSION has a\ndependency on a non-syncable algorithm, such as Mapping, it will not be\nexported.\n\n\nThis separation was added because global objects 1) containing large\nlookup files are projected to be time consuming and 2) are expected to\nbe synchronized much less frequently than any masking job related metadata.\nExamples on how to use it will be available in the \nExample User\nWorkflow\n section.\n\n\nReferences Objects\n\u00b6\n\n\nAs mentioned in the \nGlobal Objects\n section, we expect the users to\nsynchronize global objects and masking jobs at different frequencies. To\navoid any unnecessary export of large algorithms, any objects\n(MASKING_JOB, PROFILE_JOB, DATABASE_RULESET, FILE_FORMAT and FILE_RULESET) that\nhave dependencies on algorithms will export just the references to the\nobjects by default. This way we check whether the necessary dependency\nexists on the importing engine by comparing the references; if not, we\nfail the import execution with an appropriate message. Domains, profile\nsets, and profile expressions are the\nexception to this. Exporting any of these objects will also export the full\nalgorithm.\n\n\nOn-The-Fly Masking Jobs\n\u00b6\n\n\nBy definition, On-The-Fly (OTF) masking jobs work with a source\nenvironment/connector and a target environment/connector, masking the\ndata from the source connector into that of the target connector. With\nmasking jobs, a target \nenvironment_id\n is always required to specify\nwhich environment to import the job and its target connector. In\naddition to the target \nenvironment_id,\n OTF masking jobs require the\nspecification of a \nsource_environment_id\n into which to import the\nsource connector. The source connector is copied into the specified\nsource environment (\nsource_environment_id\n ), and is represented by\nthe SOURCE_DATABASE_CONNECTOR or\n\n\nSOURCE_FILE_CONNECTOR for database and file masking jobs respectively\nin the export document. These source connectors are virtually identical\nto their DATABASE_CONNECTOR and FILE_CONNECTOR counterparts, but are\nrepresented differently in the OTF jobs to distinguish them from the\ntarget connector (i.e., DATABASE_CONNECTOR or FILE_CONNECTOR).\n\n\nCircular Dependencies\n\u00b6\n\n\nIt is possible to have a set of objects that end up depending\non each other. This would be the case if a PROFILE_SET depended on a\nPROFILE_EXPRESSION that depended on a DOMAIN that depended on a REDACTION\nalgorithm that depended on the original PROFILE_SET. The masking application\nwill detect such scenarios on export and refuse to export such configurations.\n\n\nThis can be worked around by creating a second PROFILE_SET that contains\nPROFILE_EXPRESSIONS that do not depend on a DOMAIN that depends on a REDACTION\nalgorithm. Simply ensure that the regular expressions are the same in the newly\ncreated PROFILE_EXPRESSIONs and assign the REDACTION algorithm to the new\nPROFILE_SET instead. The REDACTION algorithm will function the same but the\ndependency loop will have been broken.",
            "title": "Masking API Call Concepts"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#masking-api-call-concepts",
            "text": "",
            "title": "Masking API Call Concepts"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#syncable-object",
            "text": "Syncable objects are external representations of objects within the\nmasking engine that can be exported from one engine and imported into\nanother. EngineSync currently supports exporting a subset of algorithms,\nthe encryption key and all the objects necessary for a masking job.\nNote: We do not currently support Mainframe masking jobs.",
            "title": "Syncable object"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#object-identifiers-and-types",
            "text": "EngineSync uses object identifiers to name unique objects within the\nengine. The follow object types are currently supported:   DATABASE_CONNECTOR  DATABASE_RULESET  DOMAIN  FILE_CONNECTOR  FILE_FORMAT  FILE_RULESET  GLOBAL_OBJECT  KEY  Certain algorithms:  BINARYLOOKUP  CLEANSING  DATE_SHIFT  LOOKUP  MIN_MAX  REDACTION  SEGMENT  TOKENIZATION  MAPPLET    MASKING_JOB  PROFILE_EXPRESSION  PROFILE_JOB  PROFILE_SET   The following lists the object types that are simply for the purpose of\nreferencing a particular state of the exported object. These are not\nmeant to be exported by request. The functions of these are further\nexplained in the latter sections.   ALGORITHM_REFERENCE  DOMAIN_REFERENCE  PROFILE_EXPRESSION_REFERENCE  PROFILE_SET_REFERENCE  SOURCE_DATABASE_CONNECTOR  SOURCE_FILE_CONNECTOR",
            "title": "Object Identifiers and Types"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#dependencies",
            "text": "Most objects within the Masking Engine are compositional. In order to\nproperly capture the behavior of a syncable object, you must export its\ndependencies along with the object itself. Fortunately, all the\nnecessary dependencies are exported along with the object you request;\nthus, it is not something you need to keep track of and worry about.  Syncable Object dependencies relationship  Note: Green represents global objects (objects that are central to the\nentire engine), and blue represents objects that need to be a part of an\nenvironment",
            "title": "Dependencies"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#object-revision-tracking",
            "text": "The revision hash is used to help you determine whether the behavior of a syncable object is the\nsame between engines. Because objects within the Masking Engine are\ncompositional, the behavior of an object is influenced by all of its\ndependencies. When a syncable object is listed or exported, the\nMasking Engine computes a revision_hash, which uniquely identifies the\nobject\u2019s behavior.  The revision_hash is a SHA1 hash that represents that object\u2019s state,\nas well as the state of all objects it depends on. If two objects have\nthe same revision hash, it is safe to assume that the behavior the\nobjects is the same. However, it is possible for two objects to have the\nsame behavior but have divergent revision hashes. For example, you could\nhave two lookup algorithms with the same name, lookup file, and key, and\nthey do not necessarily guaranteed to have the same revision hash.   Note  The revision_hash does not change when the password or the ssh key for either the FILE_CONNECTOR or DATABASE_CONNECTOR is updated. This is intentionally done because we do not export the password or the ssh key for security purposes. This allows users to update the password after import without changing the revision_hash. If a user is  overriding  a connector that already has a password set, the import  does not  reset the password and will leave the current, pre-import value.",
            "title": "Object Revision Tracking"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#export-document",
            "text": "You can export one or more syncable objects that are listed in the /syncable-objects  endpoint. The export document will include the set\nof objects that you requested for export and all of their dependencies that are\nrequired to properly import those objects into another engine.  The export document is exported as an opaque blob. Do not edit it\noutside of the Masking Engine.",
            "title": "Export Document"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#export-document-encryption",
            "text": "You can request that the export document be encrypted using a\npassphrase. Once the document is encrypted with the passphrase, the\nengine forgets the passphrase. You will need to provide the same\npassphrase during import to decrypt the document.",
            "title": "Export Document Encryption"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#digital-signature",
            "text": "In order to detect accidental or malicious modification of the export\ndocument, each document is digitally signed. If the export document does\nnot match its expected digital signature, a Masking Engine will not\nimport the document.",
            "title": "Digital Signature"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#overwrite",
            "text": "When an object to be imported has the same name as a currently existing\nobject, importing it will cause the other object to be changed. Since\nthis might not be intended, we offer a flag called force_overwrite. If\nforce_overwrite is set to false and doing the import will change an\nexisting object on the masking engine, we fail the import. This workflow\nis shown below.",
            "title": "Overwrite"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#attempting-to-import-identical-objects",
            "text": "The Masking Engine checks for the existence of the same object contents\nduring the import of an object. If it is determined that the engine and\nthe document being imported contain the same content, a result of\nSUCCESS will be returned without repeating the work of a full import.\nFor example, importing an entire ruleset with hundreds of thousands of\ntables can be quite time consuming, and this should not be repeated if\nthe same object already exists. If the object content matches and we\nskip the full import we note this in the application log.  Below is an example log statement when an identical database connector\nwas imported:  2017-07-19 10:17:06,075 [http-nio-8282-exec-4] INFO\nc.d.s.marshallers.SyncableMarshaller - Skipping import process for\n{  \n\"objectType\": \"DATABASE_CONNECTOR\",  \n\"id\": {  \n\"@type\": \"type.googleapis.com/IntegerIdentifier\",  \n\"id\": 1  \n}  \n}, due to no discrepancy between the existing and importing object",
            "title": "Attempting to Import Identical Objects"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#overwrite-of-the-encryption-key",
            "text": "The global encryption key is somewhat special in that it always exists.\nSpecifying  force_overwrite=false  will always fail to import the\nencryption key unless the encryption key has been previously\nsynchronized using  force_overwrite=true .  Specifying  force_overwrite=true  will always overwrite the engine\u2019s\nencryption key with the contents of the encryption key in the export\ndocument.",
            "title": "Overwrite of the Encryption Key"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#error-handling",
            "text": "Export documents often have multiple objects to be imported at once. For\nexample, when exporting a database ruleset, you will export both the\ndatabase ruleset and the database connector since a ruleset depends on a\nconnector.  The engine will import one object at a time, where the dependencies are\nimported first. If there is an error importing an object, the import\nprocess will abort and all objects that have successfully been imported\nduring this request will get rolled back. For example, say you are\nimporting objects A, B, and C. Import successfully imports A. During the\nimport of B, the engine encounters an error. The import of A will roll\nback, and import of C will never execute. This will leave the engine in\na state identical to the one it was in prior to the failed import.",
            "title": "Error handling"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#concurrent-sync-operations",
            "text": "To prevent race conditions with concurrent imports and jobs running, we\ncurrently do not allow concurrent import operations. We also do not\nallow imports while masking jobs or exports are running. It is best to\ndo imports when a machine is not running jobs or other exports in order\nto guarantee that the final state of each of those operations is as\nexpected. If they are done at the same time, the operations will fail\nwith relevant error messages.",
            "title": "Concurrent Sync Operations"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#global-objects",
            "text": "GLOBAL_OBJECT is a syncable object type that is a collection of all\nsyncable algorithms, DOMAIN(s), PROFILE_SET(s), PROFILE_EXPRESSION(s)\nand KEY (global key). This represents\nobjects in the Masking Engine that are available across all\nenvironments, and are not a part of any specific environment. When a\nuser requests to export GLOBAL_OBJECT, every syncable algorithm, profile\nset, profile expression and\ndomain on the engine will be exported as the bundle. If a DOMAIN,\nPROFILE_SET, or PROFILE_EXPRESSION has a\ndependency on a non-syncable algorithm, such as Mapping, it will not be\nexported.  This separation was added because global objects 1) containing large\nlookup files are projected to be time consuming and 2) are expected to\nbe synchronized much less frequently than any masking job related metadata.\nExamples on how to use it will be available in the  Example User\nWorkflow  section.",
            "title": "Global Objects"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#references-objects",
            "text": "As mentioned in the  Global Objects  section, we expect the users to\nsynchronize global objects and masking jobs at different frequencies. To\navoid any unnecessary export of large algorithms, any objects\n(MASKING_JOB, PROFILE_JOB, DATABASE_RULESET, FILE_FORMAT and FILE_RULESET) that\nhave dependencies on algorithms will export just the references to the\nobjects by default. This way we check whether the necessary dependency\nexists on the importing engine by comparing the references; if not, we\nfail the import execution with an appropriate message. Domains, profile\nsets, and profile expressions are the\nexception to this. Exporting any of these objects will also export the full\nalgorithm.",
            "title": "References Objects"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#on-the-fly-masking-jobs",
            "text": "By definition, On-The-Fly (OTF) masking jobs work with a source\nenvironment/connector and a target environment/connector, masking the\ndata from the source connector into that of the target connector. With\nmasking jobs, a target  environment_id  is always required to specify\nwhich environment to import the job and its target connector. In\naddition to the target  environment_id,  OTF masking jobs require the\nspecification of a  source_environment_id  into which to import the\nsource connector. The source connector is copied into the specified\nsource environment ( source_environment_id  ), and is represented by\nthe SOURCE_DATABASE_CONNECTOR or  SOURCE_FILE_CONNECTOR for database and file masking jobs respectively\nin the export document. These source connectors are virtually identical\nto their DATABASE_CONNECTOR and FILE_CONNECTOR counterparts, but are\nrepresented differently in the OTF jobs to distinguish them from the\ntarget connector (i.e., DATABASE_CONNECTOR or FILE_CONNECTOR).",
            "title": "On-The-Fly Masking Jobs"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Masking_API_Call_Concepts/#circular-dependencies",
            "text": "It is possible to have a set of objects that end up depending\non each other. This would be the case if a PROFILE_SET depended on a\nPROFILE_EXPRESSION that depended on a DOMAIN that depended on a REDACTION\nalgorithm that depended on the original PROFILE_SET. The masking application\nwill detect such scenarios on export and refuse to export such configurations.  This can be worked around by creating a second PROFILE_SET that contains\nPROFILE_EXPRESSIONS that do not depend on a DOMAIN that depends on a REDACTION\nalgorithm. Simply ensure that the regular expressions are the same in the newly\ncreated PROFILE_EXPRESSIONs and assign the REDACTION algorithm to the new\nPROFILE_SET instead. The REDACTION algorithm will function the same but the\ndependency loop will have been broken.",
            "title": "Circular Dependencies"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Endpoints/",
            "text": "GET /syncable-objects[?object_type=\\<type>]\n\u00b6\n\n\nThis endpoint lists all objects in an engine that are syncable and can\nbe exported. Any object which can be exported, can be imported into\nanother engine. The endpoint takes an optional parameter to filter by a\nspecific object type. Each object is listed with its revision_hash.\nNote that if a syncable object depends on a non-syncable object (i.e.\nDOMAIN using a mapping algorithm), it will say so in the \u201crevisionHash\u201d\nattribute, and will not be exportable.\n\n\nExample CURL command:\n\n\ncurl -X GET \n--header 'Accept: application/json' \n--header 'Authorization: 21c45f0e-82f4-4b04-9072-b49072986231' \n'http://masking-engine.com:8282/masking/api/syncable-objects?page_number=1'\n\n\n\n\n\nPOST /export\n\u00b6\n\n\nThis endpoint allows you to export one or more objects in batch fashion.\nThe result of the export is an export document and a set of metadata\nthat describes what was exported. You are expected to specify which\nobjects to export by copying their object identifiers from the\n/syncable-objects endpoint.\n\n\nThe endpoint has a single optional header, \npassphrase\n. If you provide\nthe passphrase, the export document will be encrypted using it.\n\n\nExample CURL command:\n\n\ncurl -X POST \n--header 'Content-Type: application/json' \n--header 'Accept: application/json' \n--header 'Authorization: 21c45f0e-82f4-4b04-9072-b49072986231' \n-d '[\n{  \n\"objectIdentifier\": {\u201cid\u201d: 1},  \n\"objectType\": \"MASKING_JOB\",  \n\"revisionHash\": \"asdfjkl12jijfdsaklfj21ojasdk\"  \n}  \n]' \n'http://masking-engine.com:8282/masking/api/export'\n\n\n\n\n\nPOST /export-async\n\u00b6\n\n\nThis endpoint does exactly the same thing as /export, but the execution\nis done asynchronously. The response returns an async task in the form\nof this:\n\n\n{\n  \n\n\"asyncTaskId\"\n:\n \n66\n,\n  \n\n\"operation\"\n:\n \n\"EXPORT\"\n,\n  \n\n\"reference\"\n:\n \n\"EXPORT-ZXhwb3J0X2RvY3VtZW50XzJjcm1EV09yLmpzb24=\"\n,\n  \n\n\"status\"\n:\n \n\"RUNNING\"\n,\n  \n\n\"startTime\"\n:\n \n\"2018-04-13T17:49:55.354+0000\"\n,\n  \n\n\"cancellable\"\n:\n \nfalse\n  \n\n}\n\n\n\n\n\n\nExample CURL command:\n\n\ncurl -s -X POST \n--header 'Content-Type: application/json' \n--header 'Accept: application/json' \n--header 'Authorization: 21c45f0e-82f4-4b04-9072-b49072986231' \n-d \"[  \n{  \n\"objectIdentifier\": {\u201cid\u201d: 1},  \n\"objectType\": \"MASKING_JOB\",  \n\"revisionHash\": \"asdfjkl12jijfdsaklfj21ojasdk\"  \n}  \n]\" \n\"http://masking-engine.com:8282/masking/api/export-async\"\n\n\n\n\n\nThe \nreference\n is used to retrieve the export document of completed\nasync export tasks from the /file-downloads endpoint. The downloaded\nfile from this reference should look exactly the same as the response\nfrom /export.\n\n\nExample CURL command:\n\n\ncurl -s -X GET \n--header 'Accept: application/octet-stream' \n--header 'Authorization: 21c45f0e-82f4-4b04-9072-b49072986231' \n-o \"<OUTPUT_FILE_PATH>\" \"http://masking-engine.com:8282/masking/api/file-downloads/EXPORT-ZXhwb3J0X2RvY3VtZW50XzJjcm1EV09yLmpzb24=\"\n\n\n\n\n\nError handling\n\u00b6\n\n\nIf an error occurs while exporting one or more elements in the export\ndocument, the entire export will\nabort.\n\n\nPOST /import\n\u00b6\n\n\nPOST /import?force_overwrite=<true|false>[&environment_id=<id>][&source_environment_id=<id>]\n\n\n\n\n\nThis endpoint allows you to import a document exported from another\nengine. The response returns a list of objects that were imported and\nwhether the import was successful.\n\n\nThe endpoint has one required parameter, \nforce_overwrite\n, two\noptional parameters \nenvironment_id\n and \nsource_environment_id\n, and\nan optional HTTP header, \npassphrase\n, which if provided, will cause the\nengine to attempt to decrypt the document using the specified\npassphrase. The required \nforce_overwrite\n parameter dictates how to\ndeal with conflicting objects. \nenvironment_id\n is necessary for all\nnon-global objects that need to belong in an environment.\n\nsource_environment_id\n is used for On-The-Fly masking jobs.\n\n\nExample CURL command:\n\n\ncurl -X POST\n--header 'Content-Type: application/json'\n--header 'Accept: application/json'\n--header 'Authorization: 21c45f0e-82f4-4b04-9072-b49072986231'\n-d '{  \n\"exportResponseMetadata\": {  \n\"exportHost\": \"masking-engine.com:8282\",  \n\"exportDate\": \"Mon Aug 13 16:29:30 UTC 2018\",  \n\"exportedObjectList\": [  \n{  \n\"objectIdentifier\": {  \n\"algorithmName\": \"lookup_alg\"  \n},  \n\"objectType\": \"LOOKUP\",  \n\"revisionHash\": \"cf84d82c21f0e9d4105d37ae7979c0848486d861\"  \n},  \n{  \n\"objectIdentifier\": {  \n\"keyId\": \"global\"  \n},  \n\"objectType\": \"KEY\",  \n\"revisionHash\": \"1d8e9bc552d3ca1dcd218f9e197ea3955ccc29be\"  \n}  \n]  \n},\n\"blob\": \"<OMITTED>\",\n\"signature\": \"<OMITTED>\", \\ \n\"publicKey\": \"<OMITTED>\" \\  \n}'\n'http://masking-engine.com:8282/masking/api/import?force_overwrite=true'\n\n\n\n\n\nPOST /import-async\n\u00b6\n\n\nPOST /import-async?force_overwrite=<true|false>[&environment_id=<id>][&source_environment_id=<id>]\n\n\n\n\n\nThis endpoint does exactly the same thing as /import, but the execution\nis done asynchronously and the body is taken in as a file. The response\nreturns an async task in the form of this:\n\n\n{\n  \n\n\"asyncTaskId\"\n:\n \n67\n,\n  \n\n\"operation\"\n:\n \n\"IMPORT\"\n,\n  \n\n\"reference\"\n:\n \n\"IMPORT-ZXhwb3J0X2RvY3VtZW50XzJjcm1EV09yLmpzb24=\"\n,\n  \n\n\"status\"\n:\n \n\"RUNNING\"\n,\n  \n\n\"startTime\"\n:\n \n\"2018-04-13T17:49:55.354+0000\"\n,\n  \n\n\"cancellable\"\n:\n \nfalse\n  \n\n}\n\n\n\n\n\n\nThe \nreference\n is used to retrieve the import status of completed async\nimport tasks from the /file-downloads endpoint. The downloaded file from\nthis reference should look exactly the same as the response from\n/import.\n\n\nExample CURL command:\n\n\ncurl -s -X POST\n--header 'Content-Type: multipart/form-data'\n--header 'Accept: application/json'\n--header 'Authorization: 21c45f0e-82f4-4b04-9072-b49072986231'\n-F \"file=@<DOWNLOADED_FILE_PATH>\"\n\"http://masking-engine.com:8282/masking/api/import-async?force_overwrite=true\"",
            "title": "Endpoints"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Endpoints/#get-syncable-objects91object95typetype6293",
            "text": "This endpoint lists all objects in an engine that are syncable and can\nbe exported. Any object which can be exported, can be imported into\nanother engine. The endpoint takes an optional parameter to filter by a\nspecific object type. Each object is listed with its revision_hash.\nNote that if a syncable object depends on a non-syncable object (i.e.\nDOMAIN using a mapping algorithm), it will say so in the \u201crevisionHash\u201d\nattribute, and will not be exportable.  Example CURL command:  curl -X GET \n--header 'Accept: application/json' \n--header 'Authorization: 21c45f0e-82f4-4b04-9072-b49072986231' \n'http://masking-engine.com:8282/masking/api/syncable-objects?page_number=1'",
            "title": "GET /syncable-objects[?object_type=\\&lt;type>]"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Endpoints/#post-export",
            "text": "This endpoint allows you to export one or more objects in batch fashion.\nThe result of the export is an export document and a set of metadata\nthat describes what was exported. You are expected to specify which\nobjects to export by copying their object identifiers from the\n/syncable-objects endpoint.  The endpoint has a single optional header,  passphrase . If you provide\nthe passphrase, the export document will be encrypted using it.  Example CURL command:  curl -X POST \n--header 'Content-Type: application/json' \n--header 'Accept: application/json' \n--header 'Authorization: 21c45f0e-82f4-4b04-9072-b49072986231' \n-d '[\n{  \n\"objectIdentifier\": {\u201cid\u201d: 1},  \n\"objectType\": \"MASKING_JOB\",  \n\"revisionHash\": \"asdfjkl12jijfdsaklfj21ojasdk\"  \n}  \n]' \n'http://masking-engine.com:8282/masking/api/export'",
            "title": "POST /export"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Endpoints/#post-export-async",
            "text": "This endpoint does exactly the same thing as /export, but the execution\nis done asynchronously. The response returns an async task in the form\nof this:  {    \"asyncTaskId\" :   66 ,    \"operation\" :   \"EXPORT\" ,    \"reference\" :   \"EXPORT-ZXhwb3J0X2RvY3VtZW50XzJjcm1EV09yLmpzb24=\" ,    \"status\" :   \"RUNNING\" ,    \"startTime\" :   \"2018-04-13T17:49:55.354+0000\" ,    \"cancellable\" :   false    }   Example CURL command:  curl -s -X POST \n--header 'Content-Type: application/json' \n--header 'Accept: application/json' \n--header 'Authorization: 21c45f0e-82f4-4b04-9072-b49072986231' \n-d \"[  \n{  \n\"objectIdentifier\": {\u201cid\u201d: 1},  \n\"objectType\": \"MASKING_JOB\",  \n\"revisionHash\": \"asdfjkl12jijfdsaklfj21ojasdk\"  \n}  \n]\" \n\"http://masking-engine.com:8282/masking/api/export-async\"  The  reference  is used to retrieve the export document of completed\nasync export tasks from the /file-downloads endpoint. The downloaded\nfile from this reference should look exactly the same as the response\nfrom /export.  Example CURL command:  curl -s -X GET \n--header 'Accept: application/octet-stream' \n--header 'Authorization: 21c45f0e-82f4-4b04-9072-b49072986231' \n-o \"<OUTPUT_FILE_PATH>\" \"http://masking-engine.com:8282/masking/api/file-downloads/EXPORT-ZXhwb3J0X2RvY3VtZW50XzJjcm1EV09yLmpzb24=\"",
            "title": "POST /export-async"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Endpoints/#error-handling",
            "text": "If an error occurs while exporting one or more elements in the export\ndocument, the entire export will\nabort.",
            "title": "Error handling"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Endpoints/#post-import",
            "text": "POST /import?force_overwrite=<true|false>[&environment_id=<id>][&source_environment_id=<id>]  This endpoint allows you to import a document exported from another\nengine. The response returns a list of objects that were imported and\nwhether the import was successful.  The endpoint has one required parameter,  force_overwrite , two\noptional parameters  environment_id  and  source_environment_id , and\nan optional HTTP header,  passphrase , which if provided, will cause the\nengine to attempt to decrypt the document using the specified\npassphrase. The required  force_overwrite  parameter dictates how to\ndeal with conflicting objects.  environment_id  is necessary for all\nnon-global objects that need to belong in an environment. source_environment_id  is used for On-The-Fly masking jobs.  Example CURL command:  curl -X POST\n--header 'Content-Type: application/json'\n--header 'Accept: application/json'\n--header 'Authorization: 21c45f0e-82f4-4b04-9072-b49072986231'\n-d '{  \n\"exportResponseMetadata\": {  \n\"exportHost\": \"masking-engine.com:8282\",  \n\"exportDate\": \"Mon Aug 13 16:29:30 UTC 2018\",  \n\"exportedObjectList\": [  \n{  \n\"objectIdentifier\": {  \n\"algorithmName\": \"lookup_alg\"  \n},  \n\"objectType\": \"LOOKUP\",  \n\"revisionHash\": \"cf84d82c21f0e9d4105d37ae7979c0848486d861\"  \n},  \n{  \n\"objectIdentifier\": {  \n\"keyId\": \"global\"  \n},  \n\"objectType\": \"KEY\",  \n\"revisionHash\": \"1d8e9bc552d3ca1dcd218f9e197ea3955ccc29be\"  \n}  \n]  \n},\n\"blob\": \"<OMITTED>\",\n\"signature\": \"<OMITTED>\", \\ \n\"publicKey\": \"<OMITTED>\" \\  \n}'\n'http://masking-engine.com:8282/masking/api/import?force_overwrite=true'",
            "title": "POST /import"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Endpoints/#post-import-async",
            "text": "POST /import-async?force_overwrite=<true|false>[&environment_id=<id>][&source_environment_id=<id>]  This endpoint does exactly the same thing as /import, but the execution\nis done asynchronously and the body is taken in as a file. The response\nreturns an async task in the form of this:  {    \"asyncTaskId\" :   67 ,    \"operation\" :   \"IMPORT\" ,    \"reference\" :   \"IMPORT-ZXhwb3J0X2RvY3VtZW50XzJjcm1EV09yLmpzb24=\" ,    \"status\" :   \"RUNNING\" ,    \"startTime\" :   \"2018-04-13T17:49:55.354+0000\" ,    \"cancellable\" :   false    }   The  reference  is used to retrieve the import status of completed async\nimport tasks from the /file-downloads endpoint. The downloaded file from\nthis reference should look exactly the same as the response from\n/import.  Example CURL command:  curl -s -X POST\n--header 'Content-Type: multipart/form-data'\n--header 'Accept: application/json'\n--header 'Authorization: 21c45f0e-82f4-4b04-9072-b49072986231'\n-F \"file=@<DOWNLOADED_FILE_PATH>\"\n\"http://masking-engine.com:8282/masking/api/import-async?force_overwrite=true\"",
            "title": "POST /import-async"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Key_Management/",
            "text": "One important piece of data used by many masking algorithms is the key,\nwhich determines the masked outcome of some value. Changing the key\nchanges the output of these algorithms. For example, if the FIRST NAME\nalgorithm masks \u201cMichelle\u201d to \u201cRachael,\u201d changing the key might cause it\nto mask \u201cMichelle\u201d to \u201cBen\u201d. There are two types of keys that the\nalgorithms can depend on: either 1) global key or 2) individual key.\n\n\nGlobal key\n\u00b6\n\n\nThe following algorithm types depend on the global key for consistent\nmasked results:\n\n\nCustom Algorithm* (MAPPLET)\n\n\n\n\nNote\n\n\nA Custom Algorithm does not depend on the global key by nature. However, most mapplets currently used are implemented to use the global key.\n\n\n\n\nA user with Administrator privileges can change the key by clicking the\n\nGenerate New Key\n button in the \nAdmin\n\ntab.\n\n\n\n\nTip\n\n\nOther actions are not allowed during the key generation process. Wait for the \nGenerate New Key\n process to complete and a success dialogue to display in the user interface before performing additional actions on the Masking Engine (e.g., running a masking job).\n\n\n\n\n\n\nSynchronizing the Global Key between Multiple Engines\n\u00b6\n\n\nIn order for Custom Algorithms to behave the same way across several\nengines, all of those engines must have the same global key. Changing an\nengine\u2019s global key alters the behavior of all of the algorithms that\ndepend on the global key.\n\n\nYou may want to change the key from time to time as a security\nmanagement practice. If so, change it on all of the engines at the same\ntime. That is, generate a new key on one engine, export that key, and\nimport it to all of the other engines in the deployment.\n\n\nKeys can be imported and exported independently of algorithms. To export\nthe key from an engine, login to the engine through the login endpoint\nand then call export with the body shown below. Like all objects, you\ncan encrypt the payload by supplying a passphrase header.\n\n\n[{\n\n\n\"objectIdentifier\"\n:\n \n{\n\n\n\"keyId\"\n:\n \n\"global\"\n},\n\n\n\"objectType\"\n:\n \n\"KEY\"\n\n\n}]\n\n\n\n\n\n\nThe API will return a JSON payload containing an encoded form of the key\nthat you can install on other engines through the import endpoint. Like\nall exported objects, it is encoded in an opaque blob.\n\n\nIndividual Key\n\u00b6\n\n\nThe following algorithm types have their own key that determines the\nmasked results:\n\n\nBINARYLOOKUP\n\n\nDATE_SHIFT (only applies to DateShiftDiscrete)\n\n\nLOOKUP\n\n\nTOKENIZATION\n\n\nThe keys for each algorithm gets exported and imported with the\nalgorithm itself, not separately. These individually associated keys can\nbe randomized with an endpoint.\n\n\nPUT http://masking-engine-A:8282/masking/api/algorithms/{algorithmName}/randomize-key",
            "title": "Key Management"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Key_Management/#global-key",
            "text": "The following algorithm types depend on the global key for consistent\nmasked results:  Custom Algorithm* (MAPPLET)   Note  A Custom Algorithm does not depend on the global key by nature. However, most mapplets currently used are implemented to use the global key.   A user with Administrator privileges can change the key by clicking the Generate New Key  button in the  Admin \ntab.   Tip  Other actions are not allowed during the key generation process. Wait for the  Generate New Key  process to complete and a success dialogue to display in the user interface before performing additional actions on the Masking Engine (e.g., running a masking job).",
            "title": "Global key"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Key_Management/#synchronizing-the-global-key-between-multiple-engines",
            "text": "In order for Custom Algorithms to behave the same way across several\nengines, all of those engines must have the same global key. Changing an\nengine\u2019s global key alters the behavior of all of the algorithms that\ndepend on the global key.  You may want to change the key from time to time as a security\nmanagement practice. If so, change it on all of the engines at the same\ntime. That is, generate a new key on one engine, export that key, and\nimport it to all of the other engines in the deployment.  Keys can be imported and exported independently of algorithms. To export\nthe key from an engine, login to the engine through the login endpoint\nand then call export with the body shown below. Like all objects, you\ncan encrypt the payload by supplying a passphrase header.  [{  \"objectIdentifier\" :   {  \"keyId\" :   \"global\" },  \"objectType\" :   \"KEY\"  }]   The API will return a JSON payload containing an encoded form of the key\nthat you can install on other engines through the import endpoint. Like\nall exported objects, it is encoded in an opaque blob.",
            "title": "Synchronizing the Global Key between Multiple Engines"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Key_Management/#individual-key",
            "text": "The following algorithm types have their own key that determines the\nmasked results:  BINARYLOOKUP  DATE_SHIFT (only applies to DateShiftDiscrete)  LOOKUP  TOKENIZATION  The keys for each algorithm gets exported and imported with the\nalgorithm itself, not separately. These individually associated keys can\nbe randomized with an endpoint.  PUT http://masking-engine-A:8282/masking/api/algorithms/{algorithmName}/randomize-key",
            "title": "Individual Key"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Algorithm_Syncability/",
            "text": "The following tables specify which algorithms are syncable between\nmasking engines (in addition to the masking engine key).\n\n\n\n\nNote\n\n\nOnly users with masking admin privilege are able to export and import algorithms.\n\n\n\n\nUser-defined Algorithms\n\u00b6\n\n\n\n\n\n\n\n\nType\n\n\nSyncable\n\n\nWorkaround\n\n\n\n\n\n\n\n\n\n\nLookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nBinary Lookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nSegmented Mapping\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nMapping\n\n\nNo\n\n\nNone\n\n\n\n\n\n\nTokenization\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nMinmax\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nCleansing\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nFree Text Redaction\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nCustom Algorithm/Mapplet\n\n\nYes\n\n\nNA (see \u201cCustom Algorithm Syncability Guide\u201d section)\n\n\n\n\n\n\n\n\nBuilt-In Algorithms\n\u00b6\n\n\nNote that syncing built-in algorithms do not actually import the\nfiles associated with them but just updates their individual keys if they\nhave them.\n\n\nWhile some of the built in algorithms are not synchronizable, mainly due\nto them being non-deterministic, we still can support export of\ninventories that contain any built in algorithm. We just do not\nguarantee consistent masking of those non-synchronizable built in\nalgorithms between engines.\n\n\n\n\n\n\n\n\nAlgorithm API Name\n\n\nAlgorithm UI Name\n\n\nType\n\n\nSyncable\n\n\nWorkaround\n\n\n\n\n\n\n\n\n\n\nAccNoLookup\n\n\nACCOUNT SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nAccountTK\n\n\nACCOUNT_TK\n\n\ntokenization\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nAddrLine2Lookup\n\n\nADDRESS LINE 2 SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nAddrLookup\n\n\nADDRESS LINE SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nBusinessLegalEntityLookup\n\n\nBUSINESS LEGAL ENTITY SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nCommentLookup\n\n\nCOMMENT SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nCreditCard\n\n\nCREDIT CARD\n\n\ncalculated\n\n\nNo\n\n\nNone\n\n\n\n\n\n\nDateShiftDiscrete\n\n\nDATE SHIFT(DISCRETE)\n\n\ncalculated\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nDateShiftFixed\n\n\nDATE SHIFT(FIXED)\n\n\ncalculated\n\n\nNo\n\n\nAlready synchronized\n\n\n\n\n\n\nDateShiftVariable\n\n\nDATE SHIFT(VARIABLE)\n\n\ncalculated\n\n\nNo\n\n\nNone\n\n\n\n\n\n\nDrivingLicenseNoLookup\n\n\nDR LICENSE SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nDummyHospitalNameLookup\n\n\nDUMMY_HOSPITAL_NAME_SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nEmailLookup\n\n\nEMAIL SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nFirstNameLookup\n\n\nFIRST NAME SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nFullNMLookup\n\n\nFULL_NM_SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nLastNameLookup\n\n\nLAST NAME SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nLastCommaFirstLookup\n\n\nLAST_COMMA_FIRST_SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nNameTK\n\n\nNAME_TK\n\n\ntokenization\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nNullValueLookup\n\n\nNULL SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nTelephoneNoLookup\n\n\nPHONE SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nRandomValueLookup\n\n\nRANDOM_VALUE_SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nSchoolNameLookup\n\n\nSCHOOL NAME SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nSecureShuffle\n\n\nSECURE SHUFFLE\n\n\ncalculated\n\n\nNo\n\n\nNone\n\n\n\n\n\n\nSsnTK\n\n\nSSN_TK\n\n\ntokenization\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nUSCountiesLookup\n\n\nUS_COUNTIES_SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nUSCitiesLookup\n\n\nUSCITIES_SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nUSstatecodesLookup\n\n\nUSSTATE_CODES_SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nUSstatesLookup\n\n\nUSSTATES_SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nWebURLsLookup\n\n\nWEB_URLS_SL\n\n\nlookup\n\n\nYes\n\n\nNA\n\n\n\n\n\n\nRepeatFirstDigit\n\n\nZIP+4\n\n\ncalculated\n\n\nNo\n\n\nAlready synchronized",
            "title": "Algorithm Syncability"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Algorithm_Syncability/#user-defined-algorithms",
            "text": "Type  Syncable  Workaround      Lookup  Yes  NA    Binary Lookup  Yes  NA    Segmented Mapping  Yes  NA    Mapping  No  None    Tokenization  Yes  NA    Minmax  Yes  NA    Cleansing  Yes  NA    Free Text Redaction  Yes  NA    Custom Algorithm/Mapplet  Yes  NA (see \u201cCustom Algorithm Syncability Guide\u201d section)",
            "title": "User-defined Algorithms"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Algorithm_Syncability/#built-in-algorithms",
            "text": "Note that syncing built-in algorithms do not actually import the\nfiles associated with them but just updates their individual keys if they\nhave them.  While some of the built in algorithms are not synchronizable, mainly due\nto them being non-deterministic, we still can support export of\ninventories that contain any built in algorithm. We just do not\nguarantee consistent masking of those non-synchronizable built in\nalgorithms between engines.     Algorithm API Name  Algorithm UI Name  Type  Syncable  Workaround      AccNoLookup  ACCOUNT SL  lookup  Yes  NA    AccountTK  ACCOUNT_TK  tokenization  Yes  NA    AddrLine2Lookup  ADDRESS LINE 2 SL  lookup  Yes  NA    AddrLookup  ADDRESS LINE SL  lookup  Yes  NA    BusinessLegalEntityLookup  BUSINESS LEGAL ENTITY SL  lookup  Yes  NA    CommentLookup  COMMENT SL  lookup  Yes  NA    CreditCard  CREDIT CARD  calculated  No  None    DateShiftDiscrete  DATE SHIFT(DISCRETE)  calculated  Yes  NA    DateShiftFixed  DATE SHIFT(FIXED)  calculated  No  Already synchronized    DateShiftVariable  DATE SHIFT(VARIABLE)  calculated  No  None    DrivingLicenseNoLookup  DR LICENSE SL  lookup  Yes  NA    DummyHospitalNameLookup  DUMMY_HOSPITAL_NAME_SL  lookup  Yes  NA    EmailLookup  EMAIL SL  lookup  Yes  NA    FirstNameLookup  FIRST NAME SL  lookup  Yes  NA    FullNMLookup  FULL_NM_SL  lookup  Yes  NA    LastNameLookup  LAST NAME SL  lookup  Yes  NA    LastCommaFirstLookup  LAST_COMMA_FIRST_SL  lookup  Yes  NA    NameTK  NAME_TK  tokenization  Yes  NA    NullValueLookup  NULL SL  lookup  Yes  NA    TelephoneNoLookup  PHONE SL  lookup  Yes  NA    RandomValueLookup  RANDOM_VALUE_SL  lookup  Yes  NA    SchoolNameLookup  SCHOOL NAME SL  lookup  Yes  NA    SecureShuffle  SECURE SHUFFLE  calculated  No  None    SsnTK  SSN_TK  tokenization  Yes  NA    USCountiesLookup  US_COUNTIES_SL  lookup  Yes  NA    USCitiesLookup  USCITIES_SL  lookup  Yes  NA    USstatecodesLookup  USSTATE_CODES_SL  lookup  Yes  NA    USstatesLookup  USSTATES_SL  lookup  Yes  NA    WebURLsLookup  WEB_URLS_SL  lookup  Yes  NA    RepeatFirstDigit  ZIP+4  calculated  No  Already synchronized",
            "title": "Built-In Algorithms"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Changes_from_Version_1/",
            "text": "New Syncable Objects\n\u00b6\n\n\nWe added the following new syncable objects in 5.3. Refer to the\nmain documentation for more information on what they are, and how to use\nthem.\n\n\n\n\nDATABASE_CONNECTOR\n\n\nDATABASE_RULESET\n\n\nDATE_SHIFT\n\n\nDOMAIN\n\n\nFILE_CONNECTOR\n\n\nFILE_FORMAT\n\n\nFILE_RULESET\n\n\nGLOBAL_OBJECT\n\n\nMASKING_JOB\n\n\nPROFILE_EXPRESSION (5.3.3.0)\n\n\nPROFILE_JOB (5.3.3.0)\n\n\nPROFILE_SET (5.3.3.0)\n\n\n\n\nWe also added the following new syncable algorithms in 5.3.\n\n\n\n\nCLEANSING (5.3.2.0)\n\n\nMIN_MAX (5.3.2.0)\n\n\nREDACTION (5.3.3.0)\n\n\n\n\nKey per Algorithm\n\u00b6\n\n\nIn pre-5.3, a global key for the engine was used by all algorithms that\nrequired a seed to determine the outcome of masked values. This included\nalgorithms such as Lookup and Binary Lookup. Thus, in 5.2, exporting a\nLookup Algorithm would automatically export the global encryption key as\na dependency. In this release, we allow each algorithm to have its own\nindependent key, exported as a part of the algorithm. Refer to the \nKey\nManagement\n section for more detail.\n\n\nChanged Model of Import Status Reporting\n\u00b6\n\n\nIn 5.2, the import status looked like this:\n\n\n{\n\n\n\"objectIdentifier\"\n:\n \n{\n\n\n\"keyId\"\n:\n \n\"global\"\n\n\n},\n\n\n\"objectType\"\n:\n \n\"KEY\"\n,\n\n\n\"importStatus\"\n:\n \n\"SUCCESS\"\n\n\n}\n\n\n\n\n\n\nStarting in Krypton, the import status of an object has extended to\ninclude the id or name it has imported into to reduce any confusion\nintroduced with IntegerIdentifiers. For more information on the reason\nfor this change, refer to Logic Behind Overwrite of IntegerIdentifier\nand StringIdentifier. For examples on what it now looks like, refer to\nthe \nExample User Workflow\n section.\n\n\nChanged Granularity of Transactions for Import\n\u00b6\n\n\nStarting in 5.3, an import of however many objects is performed as an atomic\nexecution rather than using per-object atomicity. This means that the\nexecution will either succeed at importing all objects or fail and import none\nat all. Refer to the Error Handling of Import logic flow diagram for\nmore information.\n\n\nFilter for /syncable-objects\n\u00b6\n\n\nNow that we have a large list of syncable objects, we have added a new\nfeature for filtering based on the object type. Refer to the\n\nEndpoint\n page and the \nExample User\nWorkflow\n section for more information.\n\n\nAsync Endpoints\n\u00b6\n\n\nExporting a large MASKING_JOB with many dependencies can potentially\ntake a long time. So we have decided to provide a new endpoint that\nexports and imports the objects asynchronously. Refer to the\n\nEndpoint\n section in the main documentation and the \nExample User\nWorkflow\n page for more information.",
            "title": "Changes from Version 1"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Changes_from_Version_1/#new-syncable-objects",
            "text": "We added the following new syncable objects in 5.3. Refer to the\nmain documentation for more information on what they are, and how to use\nthem.   DATABASE_CONNECTOR  DATABASE_RULESET  DATE_SHIFT  DOMAIN  FILE_CONNECTOR  FILE_FORMAT  FILE_RULESET  GLOBAL_OBJECT  MASKING_JOB  PROFILE_EXPRESSION (5.3.3.0)  PROFILE_JOB (5.3.3.0)  PROFILE_SET (5.3.3.0)   We also added the following new syncable algorithms in 5.3.   CLEANSING (5.3.2.0)  MIN_MAX (5.3.2.0)  REDACTION (5.3.3.0)",
            "title": "New Syncable Objects"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Changes_from_Version_1/#key-per-algorithm",
            "text": "In pre-5.3, a global key for the engine was used by all algorithms that\nrequired a seed to determine the outcome of masked values. This included\nalgorithms such as Lookup and Binary Lookup. Thus, in 5.2, exporting a\nLookup Algorithm would automatically export the global encryption key as\na dependency. In this release, we allow each algorithm to have its own\nindependent key, exported as a part of the algorithm. Refer to the  Key\nManagement  section for more detail.",
            "title": "Key per Algorithm"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Changes_from_Version_1/#changed-model-of-import-status-reporting",
            "text": "In 5.2, the import status looked like this:  {  \"objectIdentifier\" :   {  \"keyId\" :   \"global\"  },  \"objectType\" :   \"KEY\" ,  \"importStatus\" :   \"SUCCESS\"  }   Starting in Krypton, the import status of an object has extended to\ninclude the id or name it has imported into to reduce any confusion\nintroduced with IntegerIdentifiers. For more information on the reason\nfor this change, refer to Logic Behind Overwrite of IntegerIdentifier\nand StringIdentifier. For examples on what it now looks like, refer to\nthe  Example User Workflow  section.",
            "title": "Changed Model of Import Status Reporting"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Changes_from_Version_1/#changed-granularity-of-transactions-for-import",
            "text": "Starting in 5.3, an import of however many objects is performed as an atomic\nexecution rather than using per-object atomicity. This means that the\nexecution will either succeed at importing all objects or fail and import none\nat all. Refer to the Error Handling of Import logic flow diagram for\nmore information.",
            "title": "Changed Granularity of Transactions for Import"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Changes_from_Version_1/#filter-for-syncable-objects",
            "text": "Now that we have a large list of syncable objects, we have added a new\nfeature for filtering based on the object type. Refer to the Endpoint  page and the  Example User\nWorkflow  section for more information.",
            "title": "Filter for /syncable-objects"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Changes_from_Version_1/#async-endpoints",
            "text": "Exporting a large MASKING_JOB with many dependencies can potentially\ntake a long time. So we have decided to provide a new endpoint that\nexports and imports the objects asynchronously. Refer to the Endpoint  section in the main documentation and the  Example User\nWorkflow  page for more information.",
            "title": "Async Endpoints"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Example_User_Workflow/",
            "text": "This page provides some examples of some typical user workflows. More information on exactly how each endpoint works is available on the Endpoints page.\n\n\nSyncing all Global Objects\n\u00b6\n\n\nThe following steps can be used to sync all global objects from Masking Engine A to Masking Engine B. This will sync all algorithms and domains and should be done prior to syncing jobs or rulesets which might depend on them. For more information on the global object, see the \nMasking API Call Concepts\n section.\n\n\n\n\nOn Masking Engine A, get the Authorization from the /login API\n\n\n\n\nPOST http://masking-engine-A:8282/masking/api/login\n\nHEADER\nContent-Type : application/json\nAccept: application/json\n\nBODY (raw)\n{\"username\": \"user123\", \"password\": \"pw123\" }\n\n\n\n\n\nExpected Result:\n\n\n{ \"Authorization\": \"dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a\" }\n\n\n\n\n\n\n\nOn Masking Engine A, call GET /syncable-objects to get a list of syncable objects.\n\n\n\n\nGET http://masking-engine-A:8282/masking/api/syncable-objects\n\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a (whatever you get from login)\nContent-Type : application/json\nAccept: application/json\n\n\n\n\n\nExpected Result:\n\n\n[\n{\n\"objectIdentifier\": {\n\"keyId\": \"global\"\n},\n\"objectType\": \"KEY\",\n\"revisionHash\": \"68eaffef400e426520a5fcbb683419db3be53317\"\n},\n{\n\"objectIdentifier\": {\n\"id\": 4\n},\n\"objectType\": \"MASKING_JOB\",\n\"revisionHash\": \"485343f1a68698497946f4f70d1cfdd76d516fd8\"\n},\n{\n\"objectIdentifier\": {\n\"algorithmName\": \"AddrLine2Lookup\"\n},\n\"objectType\": \"LOOKUP\",\n\"revisionHash\": \"f397c46a97bddacf4203e35d7a538fda4bba6b12\"\n},\n{\n\"objectIdentifier\": {\n\"id\": \"global\"\n},\n\"objectType\": \"GLOBAL_OBJECT\",\n\"revisionHash\": \"e230c46a97bddacf4201a35d7a538fda4bca6b14\"\n}\n\u2026\n]\n\n\n\n\n\n\n\nOn Masking EngineA, call /export-async on GLOBAL_OBJECT.\n\n\n\n\nPOST http://masking-engine-A:8282/masking/api/export-async\n\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a\nContent-Type : application/json\nAccept: application/json\npassphrase (Optional): password to encrypt the export document\n\nBODY\n[\n{\n\"objectIdentifier\": {\n\"id\": \"global\"\n},\n\"objectType\": \"GLOBAL_OBJECT\"\n}\n]\n\n\n\n\n\nEXPECTED RESULT\n\n\n{\n\"asyncTaskId\": 2,\n\"operation\": \"EXPORT\",\n\"reference\": \"EXPORT-ZXhwb3J0X2RvY3VtZW50Xzk0Wjlva3JDLmpzb24=\",\n\"status\": \"RUNNING\",\n\"startTime\": \"2018-06-15T20:36:35.483+0000\",\n\"cancellable\": false\n}\n\n\n\n\n\n\n\nDownload the export document with the reference above via the /file-download endpoint.\n\n\n\n\nGET http://masking-engine-A:8282/masking/api/file-downloads/EXPORT-ZXhwb3J0X2RvY3VtZW50Xzk0Wjlva3JDLmpzb24=\n\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a\nAccept: application/octet-stream\n\nEXPECTED RESULT\nFile - The exported document that would look identical to the response from /export with the same request body and headers\n\n\n\n\n\nAn example export document will look like this.\n\n\n{\n\"exportResponseMetadata\": {\n\"exportHost\": \"masking-engine-A:8282\",\n\"exportDate\": \"Fri Jun 15 20:16:20 UTC 2018\",\n\"requestedObjectList\": [\n{\n\"objectIdentifier\": {\n\"id\": \"global\"\n},\n\"objectType\": \"GLOBAL_OBJECT\",\n\"revisionHash\": \"579850b1c88baf74cee6bad61d81e2aa3dcc206c\"\n}\n],\n\"exportedObjectList\": [\n{\n\"objectIdentifier\": {\n\"id\": \"DRIVING_LC\"\n},\n\"objectType\": \"DOMAIN\",\n\"revisionHash\": \"9ee90782488d14d369f9595dad7f593c961e785f\"\n},\n{\n\"objectIdentifier\": {\n\"algorithmName\": \"DrivingLicenseNoLookup\"\n},\n\"objectType\": \"LOOKUP\",\n\"revisionHash\": \"e08ac9bfd4ed9f64d486cb47cdc07deb30ccc20f\"\n},\n...\n]\n},\n\"blob\":     \"RAAAAAokZmZhNWIxNjktODMwMC00N2FlLWJjZmMtNjVhNDUzYWI3OTBjEhgyMDE4LTA2LTE1VDIwOjE2OjIwLjY2MFogBSgBFwIAAAokZmZhNWIxNjktODMwMC00N2FlLWJjZmMtNjVhNDUzYWI3OTBjEu4DCi8IFBIrCiV0eXBlLmdvb2dsZWFwaXMuY29tL0ludGVnZXJJZGVudGlmaWVyEgIIARIvCA4SKwoldHlwZS5nb29nbGVhcGlzLm...\",\n\"signature\": \"MCwCFAWGf/97wb+oYuSQizj8U12n7jpQAhQKGCaOJ4U8XyDAOEhMUWkzZXHrpw==\",\n\"publicKey\": \"MIHxMIGoBgcqhkjOOAQBMIGcAkEA/KaCzo4Syrom78z3EQ5SbbB4sF7ey80etKII864WF64B81uRpH5t9jQTxeEu0ImbzRMqzVDZkVG9xD7nN1kuFwIVAJYu3cw2nLqOuyYO5rahJtk0bjjFAkBnhHGyepz0TukaScUUfbGpq..\"\n}\n\n\n\n\n\n\n\nOn Masking Engine B, use the import-async endpoint to import the document downloaded from engine A.\n\n\n\n\nPOST http://masking-engine-B:8282/masking/api/import-async?force_overwrite=true\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a\nAccept: application/octet-stream\n\nEXPECTED RESULT\nFile - The import status document that would look identical to the response from /import with the same export document\n\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a\nContent-Type: multipart/form-data\n\nAccept: application/json\npassphrase (Optional): password to encrypt the export document\n\nPARAMETER\nforce_overwrite and environment_id. See the discussion in /import for more detail.\n\nBODY\nFile - The downloaded export document\n\n\n\n\n\nExpected Result:\n\n\nEXPECTED RESULT\n{\n\"asyncTaskId\": 3,\n\"operation\": \"IMPORT\",\n\"reference\": \"IMPORT-AWhwb3J0X2Ru2VtZW50Xzk0Wjlva3JDLmpzb24=\",\n\"status\": \"RUNNING\",\n\"startTime\": \"2018-06-16T20:38:31.483+0000\",\n\"cancellable\": false\n}\n\n\n\n\n\n\n\nOn Masking Engine B, retrieve the completed import status using the reference from the returned Async Task response with /file-downloads\n\n\n\n\nGET http://masking-engine-A:8282/masking/api/file-downloads/IMPORT-AWhwb3J0X2Ru2VtZW50Xzk0Wjlva3JDLmpzb24=\n\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a\nAccept: application/octet-stream\n\n\n\n\n\nExpected Result:\nFile - The import status document that would look identical to the response from /import with the same export document\n\n\nSyncing a Masking Job\n\u00b6\n\n\nThe following steps provide an example of how to export a Masking Job from Masking Engine A to Masking Engine B using the synchronous endpoints of /export and /import. This presumes that all of the global objects such as algorithms and domains that the masking job relies on have already been synced. This can also be done via the asynchronous endpoint with the same workflow as above.\n\n\n\n\nOn Masking Engine A, export the MASKING_JOB using the /export endpoint.\n\n\n\n\nPOST http://masking-engine-A:8282/masking/api/export\n\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a (whatever you get from login)\nContent-Type : application/json\nAccept: application/json\npassphrase (Optional): password to encrypt the export document\n\nBODY\n[\n{\n\"objectIdentifier\": {\n\"id\": 4\n},\n\"objectType\": \"MASKING_JOB\"\n}\n]\n\n\n\n\n\n\n\nNote\n\n\nTo sync a profile job, swap out the objectType for \"PROFILE_JOB\" and provide the id of the profile job to sync. Profile jobs are syncable starting in version 5.3.2.0.\n\n\n\n\nExpected Result:\n\n\n{\n\"exportResponseMetadata\": {\n\"exportHost\": \"masking-engine-A:8282\",\n\"exportDate\": \"Fri Jun 15 20:16:20 UTC 2018\",\n\"requestedObjectList\": [\n{\n\"objectIdentifier\": {\n\"id\": 1\n},\n\"objectType\": \"MASKING_JOB\",\n\"revisionHash\": \"579850b1c88baf74cee6bad61d81e2aa3dcc206c\"\n}\n],\n\"exportedObjectList\": [\n{\n\"objectIdentifier\": {\n\"id\": 1\n},\n\"objectType\": \"DATABASE_RULESET\",\n\"revisionHash\": \"bf63b401129cbc84f90eeb708281e98121f5a829\"\n},\n{\n\"objectIdentifier\": {\n\"id\": \"FIRST_NAME\"\n},\n\"objectType\": \"DOMAIN_REFERENCE\",\n\"revisionHash\": \"e6a52079843afd2625f20237fd50f56254c7e630\"\n},\n{\n\"objectIdentifier\": {\n\"id\": 1\n},\n\"objectType\": \"MASKING_JOB\",\n\"revisionHash\": \"579850b1c88baf74cee6bad61d81e2aa3dcc206c\"\n},\n{\n\"objectIdentifier\": {\n\"id\": 1\n},\n\"objectType\": \"DATABASE_CONNECTOR\",\n\"revisionHash\": \"6455f39dfa354a54bdf4ef69d6511a6c2bb19db3\"\n},\n{\n\"objectIdentifier\": {\n\"algorithmName\": \"FirstNameLookup\"\n},\n\"objectType\": \"ALGORITHM_REFERENCE\",\n\"revisionHash\": \"13b0a51a7e3904f52526c442419c54b39033dca3\"\n}\n]\n},\n\"blob\":     \"RAAAAAokZmZhNWIxNjktODMwMC00N2FlLWJjZmMtNjVhNDUzYWI3OTBjEhgyMDE4LTA2LTE1VDIwOjE2OjIwLjY2MFogBSgBFwIAAAokZmZhNWIxNjktODMwMC00N2FlLWJjZmMtNjVhNDUzYWI3OTBjEu4DCi8IFBIrCiV0eXBlLmdvb2dsZWFwaXMuY29tL0ludGVnZXJJZGVudGlmaWVyEgIIARIvCA4SKwoldHlwZS5nb29nbGVhcGlzLm...\",\n\"signature\": \"MCwCFAWGf/97wb+oYuSQizj8U12n7jpQAhQKGCaOJ4U8XyDAOEhMUWkzZXHrpw==\",\n\"publicKey\": \"MIHxMIGoBgcqhkjOOAQBMIGcAkEA/KaCzo4Syrom78z3EQ5SbbB4sF7ey80etKII864WF64B81uRpH5t9jQTxeEu0ImbzRMqzVDZkVG9xD7nN1kuFwIVAJYu3cw2nLqOuyYO5rahJtk0bjjFAkBnhHGyepz0TukaScUUfbGpq..\"\n}\n\n\n\n\n\n\n\nNote\n\n\nThe requestedObjectList returns the list of objects you\u2019ve requested in the export, and the exportedObjectList returns a list of all objects that were exported. This will include both the requested ones and their dependencies.\n\n\n\n\n\n\nOn Masking Engine B, import the masking job. You will need to provide an environment for it to import into.\n\n\n\n\nPOST http://masking-engine-B:8282/masking/api/import?force_overwrite=false&environment_id=1\n\nHEADER\n(same as export)\n\nPARAMETER\nforce_overwrite and environment_id. See the details in the Masking API Call Concepts section for more details .\n\nBODY\n(Whatever gets returned from export)\n\n\n\n\n\nExpected Result:\n\n\n[\n{\n\"objectIdentifier\": {\n\"id\": 3033\n},\n\"importedObjectIdentifier\": {\n\"id\": 1\n},\n\"objectType\": \"DATABASE_CONNECTOR\",\n\"importStatus\": \"SUCCESS\"\n},\n{\n\"objectIdentifier\": {\n\"id\": 5421\n},\n\"importedObjectIdentifier\": {\n\"id\": 1\n},\n\"objectType\": \"DATABASE_RULESET\",\n\"importStatus\": \"SUCCESS\"\n}\n...\n]",
            "title": "Example User Workflow"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Example_User_Workflow/#syncing-all-global-objects",
            "text": "The following steps can be used to sync all global objects from Masking Engine A to Masking Engine B. This will sync all algorithms and domains and should be done prior to syncing jobs or rulesets which might depend on them. For more information on the global object, see the  Masking API Call Concepts  section.   On Masking Engine A, get the Authorization from the /login API   POST http://masking-engine-A:8282/masking/api/login\n\nHEADER\nContent-Type : application/json\nAccept: application/json\n\nBODY (raw)\n{\"username\": \"user123\", \"password\": \"pw123\" }  Expected Result:  { \"Authorization\": \"dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a\" }   On Masking Engine A, call GET /syncable-objects to get a list of syncable objects.   GET http://masking-engine-A:8282/masking/api/syncable-objects\n\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a (whatever you get from login)\nContent-Type : application/json\nAccept: application/json  Expected Result:  [\n{\n\"objectIdentifier\": {\n\"keyId\": \"global\"\n},\n\"objectType\": \"KEY\",\n\"revisionHash\": \"68eaffef400e426520a5fcbb683419db3be53317\"\n},\n{\n\"objectIdentifier\": {\n\"id\": 4\n},\n\"objectType\": \"MASKING_JOB\",\n\"revisionHash\": \"485343f1a68698497946f4f70d1cfdd76d516fd8\"\n},\n{\n\"objectIdentifier\": {\n\"algorithmName\": \"AddrLine2Lookup\"\n},\n\"objectType\": \"LOOKUP\",\n\"revisionHash\": \"f397c46a97bddacf4203e35d7a538fda4bba6b12\"\n},\n{\n\"objectIdentifier\": {\n\"id\": \"global\"\n},\n\"objectType\": \"GLOBAL_OBJECT\",\n\"revisionHash\": \"e230c46a97bddacf4201a35d7a538fda4bca6b14\"\n}\n\u2026\n]   On Masking EngineA, call /export-async on GLOBAL_OBJECT.   POST http://masking-engine-A:8282/masking/api/export-async\n\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a\nContent-Type : application/json\nAccept: application/json\npassphrase (Optional): password to encrypt the export document\n\nBODY\n[\n{\n\"objectIdentifier\": {\n\"id\": \"global\"\n},\n\"objectType\": \"GLOBAL_OBJECT\"\n}\n]  EXPECTED RESULT  {\n\"asyncTaskId\": 2,\n\"operation\": \"EXPORT\",\n\"reference\": \"EXPORT-ZXhwb3J0X2RvY3VtZW50Xzk0Wjlva3JDLmpzb24=\",\n\"status\": \"RUNNING\",\n\"startTime\": \"2018-06-15T20:36:35.483+0000\",\n\"cancellable\": false\n}   Download the export document with the reference above via the /file-download endpoint.   GET http://masking-engine-A:8282/masking/api/file-downloads/EXPORT-ZXhwb3J0X2RvY3VtZW50Xzk0Wjlva3JDLmpzb24=\n\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a\nAccept: application/octet-stream\n\nEXPECTED RESULT\nFile - The exported document that would look identical to the response from /export with the same request body and headers  An example export document will look like this.  {\n\"exportResponseMetadata\": {\n\"exportHost\": \"masking-engine-A:8282\",\n\"exportDate\": \"Fri Jun 15 20:16:20 UTC 2018\",\n\"requestedObjectList\": [\n{\n\"objectIdentifier\": {\n\"id\": \"global\"\n},\n\"objectType\": \"GLOBAL_OBJECT\",\n\"revisionHash\": \"579850b1c88baf74cee6bad61d81e2aa3dcc206c\"\n}\n],\n\"exportedObjectList\": [\n{\n\"objectIdentifier\": {\n\"id\": \"DRIVING_LC\"\n},\n\"objectType\": \"DOMAIN\",\n\"revisionHash\": \"9ee90782488d14d369f9595dad7f593c961e785f\"\n},\n{\n\"objectIdentifier\": {\n\"algorithmName\": \"DrivingLicenseNoLookup\"\n},\n\"objectType\": \"LOOKUP\",\n\"revisionHash\": \"e08ac9bfd4ed9f64d486cb47cdc07deb30ccc20f\"\n},\n...\n]\n},\n\"blob\":     \"RAAAAAokZmZhNWIxNjktODMwMC00N2FlLWJjZmMtNjVhNDUzYWI3OTBjEhgyMDE4LTA2LTE1VDIwOjE2OjIwLjY2MFogBSgBFwIAAAokZmZhNWIxNjktODMwMC00N2FlLWJjZmMtNjVhNDUzYWI3OTBjEu4DCi8IFBIrCiV0eXBlLmdvb2dsZWFwaXMuY29tL0ludGVnZXJJZGVudGlmaWVyEgIIARIvCA4SKwoldHlwZS5nb29nbGVhcGlzLm...\",\n\"signature\": \"MCwCFAWGf/97wb+oYuSQizj8U12n7jpQAhQKGCaOJ4U8XyDAOEhMUWkzZXHrpw==\",\n\"publicKey\": \"MIHxMIGoBgcqhkjOOAQBMIGcAkEA/KaCzo4Syrom78z3EQ5SbbB4sF7ey80etKII864WF64B81uRpH5t9jQTxeEu0ImbzRMqzVDZkVG9xD7nN1kuFwIVAJYu3cw2nLqOuyYO5rahJtk0bjjFAkBnhHGyepz0TukaScUUfbGpq..\"\n}   On Masking Engine B, use the import-async endpoint to import the document downloaded from engine A.   POST http://masking-engine-B:8282/masking/api/import-async?force_overwrite=true\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a\nAccept: application/octet-stream\n\nEXPECTED RESULT\nFile - The import status document that would look identical to the response from /import with the same export document\n\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a\nContent-Type: multipart/form-data\n\nAccept: application/json\npassphrase (Optional): password to encrypt the export document\n\nPARAMETER\nforce_overwrite and environment_id. See the discussion in /import for more detail.\n\nBODY\nFile - The downloaded export document  Expected Result:  EXPECTED RESULT\n{\n\"asyncTaskId\": 3,\n\"operation\": \"IMPORT\",\n\"reference\": \"IMPORT-AWhwb3J0X2Ru2VtZW50Xzk0Wjlva3JDLmpzb24=\",\n\"status\": \"RUNNING\",\n\"startTime\": \"2018-06-16T20:38:31.483+0000\",\n\"cancellable\": false\n}   On Masking Engine B, retrieve the completed import status using the reference from the returned Async Task response with /file-downloads   GET http://masking-engine-A:8282/masking/api/file-downloads/IMPORT-AWhwb3J0X2Ru2VtZW50Xzk0Wjlva3JDLmpzb24=\n\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a\nAccept: application/octet-stream  Expected Result:\nFile - The import status document that would look identical to the response from /import with the same export document",
            "title": "Syncing all Global Objects"
        },
        {
            "location": "/Managing_Multiple_Engines_for_Masking/Example_User_Workflow/#syncing-a-masking-job",
            "text": "The following steps provide an example of how to export a Masking Job from Masking Engine A to Masking Engine B using the synchronous endpoints of /export and /import. This presumes that all of the global objects such as algorithms and domains that the masking job relies on have already been synced. This can also be done via the asynchronous endpoint with the same workflow as above.   On Masking Engine A, export the MASKING_JOB using the /export endpoint.   POST http://masking-engine-A:8282/masking/api/export\n\nHEADER\nAuthorization : dc2cff8b-e20d-4e28-8b7e-5d7c4aad0e2a (whatever you get from login)\nContent-Type : application/json\nAccept: application/json\npassphrase (Optional): password to encrypt the export document\n\nBODY\n[\n{\n\"objectIdentifier\": {\n\"id\": 4\n},\n\"objectType\": \"MASKING_JOB\"\n}\n]   Note  To sync a profile job, swap out the objectType for \"PROFILE_JOB\" and provide the id of the profile job to sync. Profile jobs are syncable starting in version 5.3.2.0.   Expected Result:  {\n\"exportResponseMetadata\": {\n\"exportHost\": \"masking-engine-A:8282\",\n\"exportDate\": \"Fri Jun 15 20:16:20 UTC 2018\",\n\"requestedObjectList\": [\n{\n\"objectIdentifier\": {\n\"id\": 1\n},\n\"objectType\": \"MASKING_JOB\",\n\"revisionHash\": \"579850b1c88baf74cee6bad61d81e2aa3dcc206c\"\n}\n],\n\"exportedObjectList\": [\n{\n\"objectIdentifier\": {\n\"id\": 1\n},\n\"objectType\": \"DATABASE_RULESET\",\n\"revisionHash\": \"bf63b401129cbc84f90eeb708281e98121f5a829\"\n},\n{\n\"objectIdentifier\": {\n\"id\": \"FIRST_NAME\"\n},\n\"objectType\": \"DOMAIN_REFERENCE\",\n\"revisionHash\": \"e6a52079843afd2625f20237fd50f56254c7e630\"\n},\n{\n\"objectIdentifier\": {\n\"id\": 1\n},\n\"objectType\": \"MASKING_JOB\",\n\"revisionHash\": \"579850b1c88baf74cee6bad61d81e2aa3dcc206c\"\n},\n{\n\"objectIdentifier\": {\n\"id\": 1\n},\n\"objectType\": \"DATABASE_CONNECTOR\",\n\"revisionHash\": \"6455f39dfa354a54bdf4ef69d6511a6c2bb19db3\"\n},\n{\n\"objectIdentifier\": {\n\"algorithmName\": \"FirstNameLookup\"\n},\n\"objectType\": \"ALGORITHM_REFERENCE\",\n\"revisionHash\": \"13b0a51a7e3904f52526c442419c54b39033dca3\"\n}\n]\n},\n\"blob\":     \"RAAAAAokZmZhNWIxNjktODMwMC00N2FlLWJjZmMtNjVhNDUzYWI3OTBjEhgyMDE4LTA2LTE1VDIwOjE2OjIwLjY2MFogBSgBFwIAAAokZmZhNWIxNjktODMwMC00N2FlLWJjZmMtNjVhNDUzYWI3OTBjEu4DCi8IFBIrCiV0eXBlLmdvb2dsZWFwaXMuY29tL0ludGVnZXJJZGVudGlmaWVyEgIIARIvCA4SKwoldHlwZS5nb29nbGVhcGlzLm...\",\n\"signature\": \"MCwCFAWGf/97wb+oYuSQizj8U12n7jpQAhQKGCaOJ4U8XyDAOEhMUWkzZXHrpw==\",\n\"publicKey\": \"MIHxMIGoBgcqhkjOOAQBMIGcAkEA/KaCzo4Syrom78z3EQ5SbbB4sF7ey80etKII864WF64B81uRpH5t9jQTxeEu0ImbzRMqzVDZkVG9xD7nN1kuFwIVAJYu3cw2nLqOuyYO5rahJtk0bjjFAkBnhHGyepz0TukaScUUfbGpq..\"\n}   Note  The requestedObjectList returns the list of objects you\u2019ve requested in the export, and the exportedObjectList returns a list of all objects that were exported. This will include both the requested ones and their dependencies.    On Masking Engine B, import the masking job. You will need to provide an environment for it to import into.   POST http://masking-engine-B:8282/masking/api/import?force_overwrite=false&environment_id=1\n\nHEADER\n(same as export)\n\nPARAMETER\nforce_overwrite and environment_id. See the details in the Masking API Call Concepts section for more details .\n\nBODY\n(Whatever gets returned from export)  Expected Result:  [\n{\n\"objectIdentifier\": {\n\"id\": 3033\n},\n\"importedObjectIdentifier\": {\n\"id\": 1\n},\n\"objectType\": \"DATABASE_CONNECTOR\",\n\"importStatus\": \"SUCCESS\"\n},\n{\n\"objectIdentifier\": {\n\"id\": 5421\n},\n\"importedObjectIdentifier\": {\n\"id\": 1\n},\n\"objectType\": \"DATABASE_RULESET\",\n\"importStatus\": \"SUCCESS\"\n}\n...\n]",
            "title": "Syncing a Masking Job"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Masking_API_Client/",
            "text": "Masking API Client\n\u00b6\n\n\nThis section describes the API client available on the masking engine.\n\n\nIntroduction\n\u00b6\n\n\nWith the release of API v5 on the Masking Engine, Delphix has opened up\nthe possibility of scripting and automation against the Masking Engine.\nWhile this is exciting for us internally at Delphix, we are sure that\nthis will be even more exciting for the consumers of the Masking Engine.\nThis document is intended to be a high-level overview of what to expect\nwith API v5 as well as some helpful links to get you started.\n\n\nREST\n\u00b6\n\n\nAPI v5 is a RESTful API. REST stands for REpresentational State\nTransfer. A REST API will allow you to access and manipulate a textual\nrepresentation of objects and resources using a predefined set of\noperations to accomplish various tasks.\n\n\nJSON\n\u00b6\n\n\nAPI v5 uses JSON (JavaScript Object Notation) to ingest and return\nrepresentations of the various objects used throughout various\noperations. JSON is a standard format and, as such, has many tools\navailable to help with creating and parsing the request and response\npayloads, respectively.\n\n\nHere are some UNIX tools that can be used to parse JSON -\n\nhttps://stackoverflow.com/questions/1955505/parsing-json-with-unix-tools\n.\nThat being said, this is only the tip of the iceberg when it comes to\nJSON parsing and the reader is encouraged to use their method of choice.\n\n\nAPI Client\n\u00b6\n\n\nThe various operations and objects used to interact with API v5 are\ndefined in a specification document. This allows us to utilize various\ntooling to ingest that specification to generate documentation and an\nAPI Client, which can be used to generate cURL commands for all\noperations.\n\n\nTo access the API client on your Masking Engine, go to\n\nhttp://myMaskingEngine.myDomain.com:8282/masking/api-client\n.\n\n\nTo see how to log into the API client and for some starter recipes,\nplease check out API Cookbook document. Happy programming!\n\n\nSupported Features\n\u00b6\n\n\nAPI v5 is in active development but does not currently support all\nfeatures that are accessible in the GUI. The list of supported features\nwill expand over the course of subsequent releases.\n\n\nFor a full list of supported APIs, the best place to look is the API\nclient on your Masking Engine\n\n\nhttp://myMaskingEngine.com:8282/masking/api-client.\nHigh-level\noperations that are \nnot currently supported\n via the v5 APIs include,\nbut are not limited to:\n\n\n\n\n\n\nJob Scheduler\n\n\n\n\n\n\nAudit and application logs\n\n\n\n\n\n\nCopybook formats\n\n\n\n\n\n\nTokenization jobs\n\n\n\n\n\n\nReidentification jobs\n\n\n\n\n\n\nAPI Calls for Masking Administration\n\u00b6\n\n\nThe Delphix Masking Engine supports the following two types of\nadministrative APIs:\n\n\n\n\n\n\nAnalytics APIs\n\n\n\n\nThese APIs are for including Masking performance information in the\nsupport bundle and do not need to be used unless that information is\nrequested.\n\n\n\n\n\n\n\n\nApplication Setting APIs\n\n\n\n\nApplication Setting APIs allow an administrator to change the\nDelphix Masking Engine settings. Presently there are five categories\nof settings: analytics settings, LDAP settings, general settings,\nmask settings and profile settings. Over time, more settings will be\nadded to give users direct control over the product's various\nsettings. Below are the details of currently supported settings.\n\n\n\n\n\n\n\n\nApplication Settings APIs\n\u00b6\n\n\nGeneral Group Settings\n\u00b6\n\n\n\n\n\n\n\n\n\n\nSetting Group\n\n\n\n\n\n\nSetting Name\n\n\n\n\n\n\nType\n\n\n\n\n\n\nDescription\n\n\n\n\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\nEnableMonitorRowCount\n\n\n\n\nBoolean\n\n\n\n\nControls whether a job displays the total number of rows that are being masked.\nSetting this to false reduces the startup time of all jobs.\n\n\n\n\ntrue\n\n\n\n\n\n\n\n\n\n\nPasswordTimeSpan\n\n\n\n\nInteger\n[0, \u221e)\n\n\n\n\nThe number of hours a user is locked out for before they can attempt to log in again.\n\n\n\n\n23\n\n\n\n\n\n\n\n\n\n\nPasswordCount\n\n\n\n\nInteger\n[0, \u221e)\n\n\n\n\nThe number of incorrect password attempts before a user is locked out.\n\n\n\n\n3\n\n\n\n\n\n\n\n\n\n\nAllowPasswordResetRequest\n\n\n\n\nBoolean\n\n\n\n\nWhen true, users can request a password reset link be sent to the email associated with their account.\n\n\n\n\ntrue\n\n\n\n\n\n\n\n\n\n\nPasswordResetLinkDuration\n\n\n\n\nInteger\n[1, \u221e)\n\n\n\n\nControls how many minutes the password reset link is valid for.\n\n\n\n\n5\n\n\n\n\n\n\n\n\n\n\nLDAP Group Settings\n\u00b6\n\n\n\n\n\n\n\n\nSetting Group\n\n\nSetting Name\n\n\nType\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\nldap\n\n\nEnable\n\n\nBoolean\n\n\nUsed to enable and disable LDAP authentication\n\n\nfalse\n\n\n\n\n\n\n\n\nLdapHost\n\n\nString\n\n\nHost of LDAP server\n\n\n10.10.10.31\n\n\n\n\n\n\n\n\nLdapPort\n\n\nInteger\n[0, \u221e)\n\n\nPort of LDAP server\n\n\n389\n\n\n\n\n\n\n\n\nLdapBasedn\n\n\nString\n\n\nBase DN of LDAP server\n\n\nDC=tbspune,DC=com\n\n\n\n\n\n\n\n\nLdapFilter\n\n\nString\n\n\nFilter for LDAP authentication\n\n\n(&(objectClass=person)(sAMAccountName=?))\n\n\n\n\n\n\n\n\nMsadDomain\n\n\nString\n\n\nMSAD Domain for LDAP authentication\n\n\nAD\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nIn the LDAP group, once the \"Enable\" setting is set to \"true\", all users logging in will be authenticated via the LDAP server. Local authentication will no longer work. Before setting this to true set all other LDAP settings correctly and create the necessary LDAP users on the masking engine.\n\n\n\n\nMask Group Settings\n\u00b6\n\n\n\n\n\n\n\n\n\n\nSetting Group\n\n\n\n\n\n\nSetting Name\n\n\n\n\n\n\nType\n\n\n\n\n\n\nDescription\n\n\n\n\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\nmask\n\n\n\n\nDatabaseCommitSize\n\n\n\n\nInteger\n[1, \u221e)\n\n\n\n\nControls how many rows are updated (Batch Update) or inserted (Bulk Data) to the database before the transaction is committed.\n\n\n\n\n10000\n\n\n\n\n\n\n\n\n\n\nBulkDataSeparator\n\n\n\n\nString\n\n\n\n\nCharacters used to separate fields in a bulk data masking job.\n\n\n\n\n#;#\n\n\n\n\n\n\n\n\n\n\nDefaultStreams\n\n\n\n\nInteger\n[1, \u221e)\n\n\n\n\nDefault number of streams for a masking job.\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n\nDefaultUpdateThreads\n\n\n\n\nInteger\n[1, \u221e)\n\n\n\n\nDefault number of database update threads for a masking job.\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n\nDefaultMaxMemory\n\n\n\n\nInteger\n[1024, \u221e)\n\n\n\n\nDefault maximum memory for masking jobs (in megabytes).\n\n\n\n\n1024\n\n\n\n\n\n\n\n\n\n\nDefaultMinMemory\n\n\n\n\nInteger\n[1024, \u221e)\n\n\n\n\nDefault minimum memory for masking jobs (in megabytes).\n\n\n\n\n1024\n\n\n\n\n\n\n\n\n\n\nProfile Group Settings\n\u00b6\n\n\n\n\n\n\n\n\nSetting Group\n\n\nSetting Name\n\n\nType\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\nprofile\n\n\nEnableDataLevelCount\n\n\nBoolean\n\n\nWhen enabled, only profile the number of rows specified by DataLevelRows when running data level profiling jobs.\n\nWhen disabled, profile all rows when running data level profiling jobs.\n\n\nfalse\n\n\n\n\n\n\n\n\nDataLevelRows\n\n\nInteger\n[1, \u221e)\n\n\nThe number of rows a data level profiling job samples when profiling a column. This is only used when EnableDataLevelCount is true.\n\n\n100\n\n\n\n\n\n\n\n\nDataLevelPercentage\n\n\nDouble\n(0, \u221e)\n\n\nPercentage of rows that must match the data level regex to consider this column a match, and thus sensitive.\n\n\n80.0\n\n\n\n\n\n\n\n\nIgnoreDatatype\n\n\nString\n\n\nDatatypes that a profiling job should ignore. Columns of these types will not be assigned a domain/algorithm pair.\n\n\nBIT,BOOLEAN,CHAR#1,VARCHAR#1,VARCHAR2#1,NCHAR#1,\nNVARCHAR#1,NVARCHAR2#1,BINARY,VARBINARY,IMAGE,\nLOB,LONG,BLOB,CLOB,NCLOB,BFILE,RAW,ENUM,BFILE\n\n\n\n\n\n\n\n\nDefaultStreams\n\n\nInteger\n[1, \u221e)\n\n\nDefault number of streams for a profiling job.\n\n\n1\n\n\n\n\n\n\n\n\nDefaultMaxMemory\n\n\nInteger\n[1024, \u221e)\n\n\nDefault maximum memory for profiling jobs (in megabytes).\n\n\n1024\n\n\n\n\n\n\n\n\nDefaultMinMemory\n\n\nInteger\n[1024, \u221e)\n\n\nDefault minimum memory for profiling jobs (in megabytes).\n\n\n1024",
            "title": "Masking API Client"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Masking_API_Client/#masking-api-client",
            "text": "This section describes the API client available on the masking engine.",
            "title": "Masking API Client"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Masking_API_Client/#introduction",
            "text": "With the release of API v5 on the Masking Engine, Delphix has opened up\nthe possibility of scripting and automation against the Masking Engine.\nWhile this is exciting for us internally at Delphix, we are sure that\nthis will be even more exciting for the consumers of the Masking Engine.\nThis document is intended to be a high-level overview of what to expect\nwith API v5 as well as some helpful links to get you started.",
            "title": "Introduction"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Masking_API_Client/#rest",
            "text": "API v5 is a RESTful API. REST stands for REpresentational State\nTransfer. A REST API will allow you to access and manipulate a textual\nrepresentation of objects and resources using a predefined set of\noperations to accomplish various tasks.",
            "title": "REST"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Masking_API_Client/#json",
            "text": "API v5 uses JSON (JavaScript Object Notation) to ingest and return\nrepresentations of the various objects used throughout various\noperations. JSON is a standard format and, as such, has many tools\navailable to help with creating and parsing the request and response\npayloads, respectively.  Here are some UNIX tools that can be used to parse JSON - https://stackoverflow.com/questions/1955505/parsing-json-with-unix-tools .\nThat being said, this is only the tip of the iceberg when it comes to\nJSON parsing and the reader is encouraged to use their method of choice.",
            "title": "JSON"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Masking_API_Client/#api-client",
            "text": "The various operations and objects used to interact with API v5 are\ndefined in a specification document. This allows us to utilize various\ntooling to ingest that specification to generate documentation and an\nAPI Client, which can be used to generate cURL commands for all\noperations.  To access the API client on your Masking Engine, go to http://myMaskingEngine.myDomain.com:8282/masking/api-client .  To see how to log into the API client and for some starter recipes,\nplease check out API Cookbook document. Happy programming!",
            "title": "API Client"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Masking_API_Client/#supported-features",
            "text": "API v5 is in active development but does not currently support all\nfeatures that are accessible in the GUI. The list of supported features\nwill expand over the course of subsequent releases.  For a full list of supported APIs, the best place to look is the API\nclient on your Masking Engine  http://myMaskingEngine.com:8282/masking/api-client. High-level\noperations that are  not currently supported  via the v5 APIs include,\nbut are not limited to:    Job Scheduler    Audit and application logs    Copybook formats    Tokenization jobs    Reidentification jobs",
            "title": "Supported Features"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Masking_API_Client/#api-calls-for-masking-administration",
            "text": "The Delphix Masking Engine supports the following two types of\nadministrative APIs:    Analytics APIs   These APIs are for including Masking performance information in the\nsupport bundle and do not need to be used unless that information is\nrequested.     Application Setting APIs   Application Setting APIs allow an administrator to change the\nDelphix Masking Engine settings. Presently there are five categories\nof settings: analytics settings, LDAP settings, general settings,\nmask settings and profile settings. Over time, more settings will be\nadded to give users direct control over the product's various\nsettings. Below are the details of currently supported settings.",
            "title": "API Calls for Masking Administration"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Masking_API_Client/#application-settings-apis",
            "text": "",
            "title": "Application Settings APIs"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Masking_API_Client/#general-group-settings",
            "text": "Setting Group    Setting Name    Type    Description    Default Value     \ngeneral  \nEnableMonitorRowCount  \nBoolean  \nControls whether a job displays the total number of rows that are being masked.\nSetting this to false reduces the startup time of all jobs.  \ntrue     \nPasswordTimeSpan  \nInteger\n[0, \u221e)  \nThe number of hours a user is locked out for before they can attempt to log in again.  \n23     \nPasswordCount  \nInteger\n[0, \u221e)  \nThe number of incorrect password attempts before a user is locked out.  \n3     \nAllowPasswordResetRequest  \nBoolean  \nWhen true, users can request a password reset link be sent to the email associated with their account.  \ntrue     \nPasswordResetLinkDuration  \nInteger\n[1, \u221e)  \nControls how many minutes the password reset link is valid for.  \n5",
            "title": "General Group Settings"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Masking_API_Client/#ldap-group-settings",
            "text": "Setting Group  Setting Name  Type  Description  Default Value    ldap  Enable  Boolean  Used to enable and disable LDAP authentication  false     LdapHost  String  Host of LDAP server  10.10.10.31     LdapPort  Integer\n[0, \u221e)  Port of LDAP server  389     LdapBasedn  String  Base DN of LDAP server  DC=tbspune,DC=com     LdapFilter  String  Filter for LDAP authentication  (&(objectClass=person)(sAMAccountName=?))     MsadDomain  String  MSAD Domain for LDAP authentication  AD      Warning  In the LDAP group, once the \"Enable\" setting is set to \"true\", all users logging in will be authenticated via the LDAP server. Local authentication will no longer work. Before setting this to true set all other LDAP settings correctly and create the necessary LDAP users on the masking engine.",
            "title": "LDAP Group Settings"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Masking_API_Client/#mask-group-settings",
            "text": "Setting Group    Setting Name    Type    Description    Default Value     \nmask  \nDatabaseCommitSize  \nInteger\n[1, \u221e)  \nControls how many rows are updated (Batch Update) or inserted (Bulk Data) to the database before the transaction is committed.  \n10000     \nBulkDataSeparator  \nString  \nCharacters used to separate fields in a bulk data masking job.  \n#;#     \nDefaultStreams  \nInteger\n[1, \u221e)  \nDefault number of streams for a masking job.  \n1     \nDefaultUpdateThreads  \nInteger\n[1, \u221e)  \nDefault number of database update threads for a masking job.  \n1     \nDefaultMaxMemory  \nInteger\n[1024, \u221e)  \nDefault maximum memory for masking jobs (in megabytes).  \n1024     \nDefaultMinMemory  \nInteger\n[1024, \u221e)  \nDefault minimum memory for masking jobs (in megabytes).  \n1024",
            "title": "Mask Group Settings"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Masking_API_Client/#profile-group-settings",
            "text": "Setting Group  Setting Name  Type  Description  Default Value    profile  EnableDataLevelCount  Boolean  When enabled, only profile the number of rows specified by DataLevelRows when running data level profiling jobs. \nWhen disabled, profile all rows when running data level profiling jobs.  false     DataLevelRows  Integer\n[1, \u221e)  The number of rows a data level profiling job samples when profiling a column. This is only used when EnableDataLevelCount is true.  100     DataLevelPercentage  Double\n(0, \u221e)  Percentage of rows that must match the data level regex to consider this column a match, and thus sensitive.  80.0     IgnoreDatatype  String  Datatypes that a profiling job should ignore. Columns of these types will not be assigned a domain/algorithm pair.  BIT,BOOLEAN,CHAR#1,VARCHAR#1,VARCHAR2#1,NCHAR#1, NVARCHAR#1,NVARCHAR2#1,BINARY,VARBINARY,IMAGE, LOB,LONG,BLOB,CLOB,NCLOB,BFILE,RAW,ENUM,BFILE     DefaultStreams  Integer\n[1, \u221e)  Default number of streams for a profiling job.  1     DefaultMaxMemory  Integer\n[1024, \u221e)  Default maximum memory for profiling jobs (in megabytes).  1024     DefaultMinMemory  Integer\n[1024, \u221e)  Default minimum memory for profiling jobs (in megabytes).  1024",
            "title": "Profile Group Settings"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/",
            "text": "API Calls for Creating an Inventory\n\u00b6\n\n\nBelow are examples of requests you might enter and responses you might\nreceive from the Masking API client. For commands specific to your\nmasking engine, work with your interactive client at\nhttp://\n<myMaskingEngine>:8282\n/masking/api-client/\n\n\n\n\nWarning\n\n\nHTTPS (SSL/TLS) is recommended, but for explanatory purposes these examples use insecure HTTP\n\n\n\n\n\n\nInfo\n\n\nIn all code examples, replace \n<myMaskingEngine>\n with the hostname or IP address of your virtual machine.\n\n\n\n\nFetch Table Names from Database Connector\n\u00b6\n\n\nObject references you will need:\n\n\n\n\nThe ID of the database connector to fetch tables\nfor\n\n\n\n\n\n\nNote\n\n\nThis database connector ID (1, in this example) is included in the PATH for this operation, NOT the payload.\n\n\n\n\nREQUEST\n\u00b6\n\n\ncurl -X GET --header \n'Accept: application/json'\n --header \n'Authorization:\n\n\n7c856e3d-5b20-4261-b5fe-cc2ffcee5ae0'\n\n\n'\nhttp://<myMaskingEngine>:8282/masking/api/database-connectors/1/fetch\u2019\n\n\n\n\n\nRESPONSE\n\u00b6\n\n\n[\n \n\"ALL_COLUMNS\"\n,\n \n\"DBVERIFICATION_TABLE\"\n]\n\n\n\n\n\n\nMore info\n\u00b6\n\n\nhttp://<myMaskingEngine>:8282/masking/api-client/#!/databaseConnector/fetchTableMetadata\n\n\nExample\n\u00b6\n\n\nSee how to use this in the context of a script\n\nhere\n.\n\n\nCreate Table Metadata\n\u00b6\n\n\nObject references you will need:\n\n\n\n\nThe name of the table to create the metadata for\n\n\nThe ruleset ID\n\n\n\n\nREQUEST\n\u00b6\n\n\ncurl -X POST --header \n'Content-Type: application/json'\n --header \n'Accept:\n\n\napplication/json'\n --header \n'Authorization:\n\n\n7c856e3d-5b20-4261-b5fe-cc2ffcee5ae0'\n -d \n'{ \"tableName\": \"ALL_COLUMNS\",\n\n\n\"rulesetId\": 2 }'\n\n\n'http://<myMaskingEngine>:8282/masking/api/table-metadata'\n\n\n\n\n\n\nRESPONSE\n\u00b6\n\n\n{\n \n\"tableMetadataId\"\n:\n \n2\n,\n \n\"tableName\"\n:\n \n\"ALL_COLUMNS\"\n,\n \n\"rulesetId\"\n:\n \n2\n\n\n}\n\n\n\n\n\n\nMore info\n\u00b6\n\n\nhttp://<myMaskingEngine>:8282/masking/api-client/#!/tableMetadata/createTableMetadata\n\n\nExample\n\u00b6\n\n\nSee how to use this in the context of a script\n\nhere\n.\n\n\nGet All Column Metadata Belonging to Table Metadata\n\u00b6\n\n\nObject references you will need:\n\n\n\n\nThe table metadata ID to get the columns\nfor\n\n\n\n\n\n\nTip\n\n\nThis table metadata ID (2, in this example) is included in the QUERY STRING for this operation, NOT the payload. \n\n\n\n\nREQUEST\n\u00b6\n\n\ncurl -X GET --header \n'Accept: application/json'\n --header \n'Authorization:\n\n\n7c856e3d-5b20-4261-b5fe-cc2ffcee5ae0'\n\n\n'http://<myMaskingEngine>:8282/masking/api/column-metadata?table_metadata_id=2'\n\n\n\n\n\n\nRESPONSE\n\u00b6\n\n\n[\n \n{\n \n\"columnMetadataId\"\n:\n \n12\n,\n \n\"columnName\"\n:\n \n\"schoolnme\"\n,\n\n\n\"tableMetadataId\"\n:\n \n2\n,\n \n\"columnLength\"\n:\n \n50\n,\n \n\"isMasked\"\n:\n \nfalse\n,\n\n\n\"isPrimaryKey\"\n:\n \nfalse\n,\n \n\"isIndex\"\n:\n \nfalse\n,\n \n\"isForeignKey\"\n:\n \nfalse\n \n},\n \n\u2026\n \n]\n\n\n\n\n\n\nNote that the above response has been truncated due to its length for\nthe purposes of this\ndocumentation.\n\n\nMore info\n\u00b6\n\n\nhttp://<myMaskingEngine>:8282/masking/api-client/#!/columnMetadata/getAllColumnMetadata\n\n\nExample\n\u00b6\n\n\nSee how to use this in the context of a script\n\nhere\n.\n\n\nUpdate Column Metadata with Algorithm Assignment\n\u00b6\n\n\nObject references you will need:\n\n\n\n\nColumn metadata ID for the column you wish to\nupdate\n\n\n\n\n\n\nTip\n\n\nThis column metadata ID (20, in this example) is included in the PATH for this operation, NOT the payload. \n\n\n\n\n\n\nSince the names can vary in the API and UI, you should use the names\n    obtained through the API (these may not align with the UI).\n\n\nAlgorithm name\n\n\nDomain name\n\n\n\n\nREQUEST\n\u00b6\n\n\ncurl -X PUT --header \n'Content-Type: application/json'\n --header \n'Accept:\n\n\napplication/json'\n --header \n'Authorization:\n\n\n7c856e3d-5b20-4261-b5fe-cc2ffcee5ae0'\n -d \n'{ \"algorithmName\":\n\n\n\"AddrLine2Lookup\", \"domainName\": \"ADDRESS_LINE2\" }'\n\n\n'http://<myMaskingEngine>:8282/masking/api/column-metadata/20'\n\n\n\n\n\n\nRESPONSE\n\u00b6\n\n\n{\n \n\"columnMetadataId\"\n:\n \n20\n,\n \n\"columnName\"\n:\n \n\"l2_address\"\n,\n\n\n\"tableMetadataId\"\n:\n \n2\n,\n \n\"algorithmName\"\n:\n \n\"AddrLine2Lookup\"\n,\n \n\"domainName\"\n:\n\n\n\"ADDRESS_LINE2\"\n,\n \n\"columnLength\"\n:\n \n512\n,\n \n\"isMasked\"\n:\n \ntrue\n,\n \n\"isPrimaryKey\"\n:\n\n\nfalse\n,\n \n\"isIndex\"\n:\n \nfalse\n,\n \n\"isForeignKey\"\n:\n \nfalse\n\n\n}\n\n\n\n\n\n\nMore info\n\u00b6\n\n\nhttp://<myMaskingEngine>:8282/masking/api-client/#!/columnMetadata/updateColumnMetadata\n\n\nExample\n\u00b6\n\n\nSee how to use this in the context of a script\n\nhere\n.",
            "title": "API Calls for Creating an Inventory"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#api-calls-for-creating-an-inventory",
            "text": "Below are examples of requests you might enter and responses you might\nreceive from the Masking API client. For commands specific to your\nmasking engine, work with your interactive client at\nhttp:// <myMaskingEngine>:8282 /masking/api-client/   Warning  HTTPS (SSL/TLS) is recommended, but for explanatory purposes these examples use insecure HTTP    Info  In all code examples, replace  <myMaskingEngine>  with the hostname or IP address of your virtual machine.",
            "title": "API Calls for Creating an Inventory"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#fetch-table-names-from-database-connector",
            "text": "Object references you will need:   The ID of the database connector to fetch tables\nfor    Note  This database connector ID (1, in this example) is included in the PATH for this operation, NOT the payload.",
            "title": "Fetch Table Names from Database Connector"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#request",
            "text": "curl -X GET --header  'Accept: application/json'  --header  'Authorization:  7c856e3d-5b20-4261-b5fe-cc2ffcee5ae0'  ' http://<myMaskingEngine>:8282/masking/api/database-connectors/1/fetch\u2019",
            "title": "REQUEST"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#response",
            "text": "[   \"ALL_COLUMNS\" ,   \"DBVERIFICATION_TABLE\" ]",
            "title": "RESPONSE"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#more-info",
            "text": "http://<myMaskingEngine>:8282/masking/api-client/#!/databaseConnector/fetchTableMetadata",
            "title": "More info"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#example",
            "text": "See how to use this in the context of a script here .",
            "title": "Example"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#create-table-metadata",
            "text": "Object references you will need:   The name of the table to create the metadata for  The ruleset ID",
            "title": "Create Table Metadata"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#request_1",
            "text": "curl -X POST --header  'Content-Type: application/json'  --header  'Accept:  application/json'  --header  'Authorization:  7c856e3d-5b20-4261-b5fe-cc2ffcee5ae0'  -d  '{ \"tableName\": \"ALL_COLUMNS\",  \"rulesetId\": 2 }'  'http://<myMaskingEngine>:8282/masking/api/table-metadata'",
            "title": "REQUEST"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#response_1",
            "text": "{   \"tableMetadataId\" :   2 ,   \"tableName\" :   \"ALL_COLUMNS\" ,   \"rulesetId\" :   2  }",
            "title": "RESPONSE"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#more-info_1",
            "text": "http://<myMaskingEngine>:8282/masking/api-client/#!/tableMetadata/createTableMetadata",
            "title": "More info"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#example_1",
            "text": "See how to use this in the context of a script here .",
            "title": "Example"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#get-all-column-metadata-belonging-to-table-metadata",
            "text": "Object references you will need:   The table metadata ID to get the columns\nfor    Tip  This table metadata ID (2, in this example) is included in the QUERY STRING for this operation, NOT the payload.",
            "title": "Get All Column Metadata Belonging to Table Metadata"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#request_2",
            "text": "curl -X GET --header  'Accept: application/json'  --header  'Authorization:  7c856e3d-5b20-4261-b5fe-cc2ffcee5ae0'  'http://<myMaskingEngine>:8282/masking/api/column-metadata?table_metadata_id=2'",
            "title": "REQUEST"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#response_2",
            "text": "[   {   \"columnMetadataId\" :   12 ,   \"columnName\" :   \"schoolnme\" ,  \"tableMetadataId\" :   2 ,   \"columnLength\" :   50 ,   \"isMasked\" :   false ,  \"isPrimaryKey\" :   false ,   \"isIndex\" :   false ,   \"isForeignKey\" :   false   },   \u2026   ]   Note that the above response has been truncated due to its length for\nthe purposes of this\ndocumentation.",
            "title": "RESPONSE"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#more-info_2",
            "text": "http://<myMaskingEngine>:8282/masking/api-client/#!/columnMetadata/getAllColumnMetadata",
            "title": "More info"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#example_2",
            "text": "See how to use this in the context of a script here .",
            "title": "Example"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#update-column-metadata-with-algorithm-assignment",
            "text": "Object references you will need:   Column metadata ID for the column you wish to\nupdate    Tip  This column metadata ID (20, in this example) is included in the PATH for this operation, NOT the payload.     Since the names can vary in the API and UI, you should use the names\n    obtained through the API (these may not align with the UI).  Algorithm name  Domain name",
            "title": "Update Column Metadata with Algorithm Assignment"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#request_3",
            "text": "curl -X PUT --header  'Content-Type: application/json'  --header  'Accept:  application/json'  --header  'Authorization:  7c856e3d-5b20-4261-b5fe-cc2ffcee5ae0'  -d  '{ \"algorithmName\":  \"AddrLine2Lookup\", \"domainName\": \"ADDRESS_LINE2\" }'  'http://<myMaskingEngine>:8282/masking/api/column-metadata/20'",
            "title": "REQUEST"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#response_3",
            "text": "{   \"columnMetadataId\" :   20 ,   \"columnName\" :   \"l2_address\" ,  \"tableMetadataId\" :   2 ,   \"algorithmName\" :   \"AddrLine2Lookup\" ,   \"domainName\" :  \"ADDRESS_LINE2\" ,   \"columnLength\" :   512 ,   \"isMasked\" :   true ,   \"isPrimaryKey\" :  false ,   \"isIndex\" :   false ,   \"isForeignKey\" :   false  }",
            "title": "RESPONSE"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#more-info_3",
            "text": "http://<myMaskingEngine>:8282/masking/api-client/#!/columnMetadata/updateColumnMetadata",
            "title": "More info"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_an_Inventory/#example_3",
            "text": "See how to use this in the context of a script here .",
            "title": "Example"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_and_Running_Masking_Jobs/",
            "text": "API Calls for Creating and Running Masking Jobs\n\u00b6\n\n\nBelow are examples of requests you might enter and responses you might\nreceive from the Masking API client. For commands specific to your\nmasking engine, work with your interactive client at\nhttp://\n<myMaskingEngine>\n:8282/masking/api-client/\n\n\n\n\nNote\n\n\nIn all code examples, replace \n<myMaskingEngine>\n with the hostname or IP address of your virtual machine.\n\n\n\n\n\n\nWarning\n\n\nHTTPS (SSL/TLS) is recommended, but for explanatory purposes these examples use insecure HTTP.\n\n\n\n\nCreating a Masking Job\n\u00b6\n\n\nObject references you will need:\n\n\n\n\nThe ID of the ruleset for which you wish to create the masking job\n\n\n\n\nREQUEST\n\n\ncurl -X POST --header \n'Content-Type: application/json'\n --header \n'Accept:\n\n\napplication/json'\n --header \n'Authorization:\n\n\ne23bad24-8760-4091-a131-34f235d9b2d6'\n -d \n'{ \"jobName\":\n\n\n\"some_masking_job\", \"rulesetId\": 7, \"jobDescription\": \"This example\n\n\nillustrates a MaskingJob with just a handful of the possible fields set.\n\n\nIt is meant to exemplify a simple JSON body that can be passed to the\n\n\nendpoint to create a MaskingJob.\", \"feedbackSize\": 100000,\n\n\n\"onTheFlyMasking\": false }'\n\n\n'http://<myMaskingEngine>:8282/masking/api/masking-jobs'\n\n\n\n\n\n\nRESPONSE\n\u00b6\n\n\n{\n \n\"jobId\"\n:\n \n1\n,\n \n\"jobName\"\n:\n \n\"some_masking_job\"\n,\n \n\"rulesetId\"\n:\n \n7\n,\n\n\n\"createdBy\"\n:\n \n\"Axistech\"\n,\n \n\"createdTime\"\n:\n \n\"2017-07-04T00:31:00.952+0000\"\n,\n\n\n\"environmentId\"\n:\n \n2\n,\n \n\"feedbackSize\"\n:\n \n100000\n,\n \n\"jobDescription\"\n:\n \n\"This\n\n\nexample illustrates a MaskingJob with just a handful of the possible\n\n\nfields set. It is meant to exemplify a simple JSON body that can be\n\n\npassed to the endpoint to create a MaskingJob.\"\n,\n \n\"maxMemory\"\n:\n \n1024\n,\n\n\n\"minMemory\"\n:\n \n1024\n,\n \n\"multiTenant\"\n:\n \nfalse\n,\n \n\"numInputStreams\"\n:\n \n1\n,\n\n\n\"onTheFlyMasking\"\n:\n \nfalse\n \n}\n\n\n\n\n\n\n\n\nNote\n\n\nThe response includes the ID of the newly created job (\u201cjobId\u201d).\n\n\n\n\nMore info\n\u00b6\n\n\nhttp://<myMaskingEngine>:8282/masking/api-client/#!/job/createMaskingJob\n\n\nRunning a Masking Job\n\u00b6\n\n\nCreate a new execution of a masking job.\n\n\nObject references you will need:\n\n\n\n\nThe ID of the job you want to run\n\n\n\n\nREQUEST\n\u00b6\n\n\ncurl -X POST --header \n'Content-Type: application/json'\n --header \n'Accept:\n\n\napplication/json'\n --header \n'Authorization:\n\n\ne23bad24-8760-4091-a131-34f235d9b2d6'\n -d \n'{ \"jobId\": 1 }'\n\n\n'http://<myMaskingEngine>:8282/masking/api/executions'\n\n\n\n\n\n\nRESPONSE\n\u00b6\n\n\n{\n \n\"executionId\"\n:\n \n1\n,\n \n\"jobId\"\n:\n \n1\n,\n \n\"status\"\n:\n \n\"RUNNING\"\n\n\n}\n\n\n\n\n\n\nMore info\n\u00b6\n\n\nhttp://<myMaskingEngine>:8282/masking/api-client/#!/execution/createExecution\n\n\nChecking the Status of a Masking Job\n\u00b6\n\n\nObject references you will need:\n\n\n\n\nThe ID of the execution you want to check (IN THE PATH)\n\n\n\n\n\n\nNote\n\n\nThis execution id (1, in this example) is included in the PATH for this operation, NOT the payload.\n\n\nThe executions endpoint only returns status for the most recent job run, this is the expected behavior. Although the Masking Service does not currently retain historical execution results, the API has been designed to allow for historical results to be returned in the future.\n\n\n\n\nREQUEST\n\u00b6\n\n\ncurl -X GET --header \n'Accept: application/json'\n --header \n'Authorization:\n\n\n8935f7f7-6de6-40ba-80d8-d8956b71248b'\n\n\n'http://<myMaskingEngine>:8282/masking/api/executions/1'\n\n\n\n\n\n\nRESPONSE\n\u00b6\n\n\n{\n \n\"executionId\"\n:\n \n1\n,\n \n\"jobId\"\n:\n \n1\n,\n \n\"status\"\n:\n \n\"SUCCEEDED\"\n\n\n}\n\n\n\n\n\n\nMore info\n\u00b6\n\n\nhttp://<myMaskingEngine>:8282/masking/api-client/#!/execution/getExecutionById",
            "title": "API Calls for Creating and Running Masking Jobs"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_and_Running_Masking_Jobs/#api-calls-for-creating-and-running-masking-jobs",
            "text": "Below are examples of requests you might enter and responses you might\nreceive from the Masking API client. For commands specific to your\nmasking engine, work with your interactive client at\nhttp:// <myMaskingEngine> :8282/masking/api-client/   Note  In all code examples, replace  <myMaskingEngine>  with the hostname or IP address of your virtual machine.    Warning  HTTPS (SSL/TLS) is recommended, but for explanatory purposes these examples use insecure HTTP.",
            "title": "API Calls for Creating and Running Masking Jobs"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_and_Running_Masking_Jobs/#creating-a-masking-job",
            "text": "Object references you will need:   The ID of the ruleset for which you wish to create the masking job   REQUEST  curl -X POST --header  'Content-Type: application/json'  --header  'Accept:  application/json'  --header  'Authorization:  e23bad24-8760-4091-a131-34f235d9b2d6'  -d  '{ \"jobName\":  \"some_masking_job\", \"rulesetId\": 7, \"jobDescription\": \"This example  illustrates a MaskingJob with just a handful of the possible fields set.  It is meant to exemplify a simple JSON body that can be passed to the  endpoint to create a MaskingJob.\", \"feedbackSize\": 100000,  \"onTheFlyMasking\": false }'  'http://<myMaskingEngine>:8282/masking/api/masking-jobs'",
            "title": "Creating a Masking Job"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_and_Running_Masking_Jobs/#response",
            "text": "{   \"jobId\" :   1 ,   \"jobName\" :   \"some_masking_job\" ,   \"rulesetId\" :   7 ,  \"createdBy\" :   \"Axistech\" ,   \"createdTime\" :   \"2017-07-04T00:31:00.952+0000\" ,  \"environmentId\" :   2 ,   \"feedbackSize\" :   100000 ,   \"jobDescription\" :   \"This  example illustrates a MaskingJob with just a handful of the possible  fields set. It is meant to exemplify a simple JSON body that can be  passed to the endpoint to create a MaskingJob.\" ,   \"maxMemory\" :   1024 ,  \"minMemory\" :   1024 ,   \"multiTenant\" :   false ,   \"numInputStreams\" :   1 ,  \"onTheFlyMasking\" :   false   }    Note  The response includes the ID of the newly created job (\u201cjobId\u201d).",
            "title": "RESPONSE"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_and_Running_Masking_Jobs/#more-info",
            "text": "http://<myMaskingEngine>:8282/masking/api-client/#!/job/createMaskingJob",
            "title": "More info"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_and_Running_Masking_Jobs/#running-a-masking-job",
            "text": "Create a new execution of a masking job.  Object references you will need:   The ID of the job you want to run",
            "title": "Running a Masking Job"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_and_Running_Masking_Jobs/#request",
            "text": "curl -X POST --header  'Content-Type: application/json'  --header  'Accept:  application/json'  --header  'Authorization:  e23bad24-8760-4091-a131-34f235d9b2d6'  -d  '{ \"jobId\": 1 }'  'http://<myMaskingEngine>:8282/masking/api/executions'",
            "title": "REQUEST"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_and_Running_Masking_Jobs/#response_1",
            "text": "{   \"executionId\" :   1 ,   \"jobId\" :   1 ,   \"status\" :   \"RUNNING\"  }",
            "title": "RESPONSE"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_and_Running_Masking_Jobs/#more-info_1",
            "text": "http://<myMaskingEngine>:8282/masking/api-client/#!/execution/createExecution",
            "title": "More info"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_and_Running_Masking_Jobs/#checking-the-status-of-a-masking-job",
            "text": "Object references you will need:   The ID of the execution you want to check (IN THE PATH)    Note  This execution id (1, in this example) is included in the PATH for this operation, NOT the payload.  The executions endpoint only returns status for the most recent job run, this is the expected behavior. Although the Masking Service does not currently retain historical execution results, the API has been designed to allow for historical results to be returned in the future.",
            "title": "Checking the Status of a Masking Job"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_and_Running_Masking_Jobs/#request_1",
            "text": "curl -X GET --header  'Accept: application/json'  --header  'Authorization:  8935f7f7-6de6-40ba-80d8-d8956b71248b'  'http://<myMaskingEngine>:8282/masking/api/executions/1'",
            "title": "REQUEST"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_and_Running_Masking_Jobs/#response_2",
            "text": "{   \"executionId\" :   1 ,   \"jobId\" :   1 ,   \"status\" :   \"SUCCEEDED\"  }",
            "title": "RESPONSE"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_for_Creating_and_Running_Masking_Jobs/#more-info_2",
            "text": "http://<myMaskingEngine>:8282/masking/api-client/#!/execution/getExecutionById",
            "title": "More info"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_Involving_File_Upload_and_Download/",
            "text": "API Calls Involving File Upload and Download\n\u00b6\n\n\nFile Download\n\u00b6\n\n\nAPI calls involving file download through API client are noteworthy because if the request fails, the API client will continue to show the \"loading\"  icon indefinitely.\n\n\nTo avoid this, make all file download calls through CURL instead. An example of a file download call using CURL is below.\n\n\ncurl -X GET --header 'Accept: application/octet-stream' --header\n'Authorization: ec443730-124e-4958-a872-324a975bb500'\n-o \"/home/user/downloads\"\n'http://<myMaskingEngine>:8282/masking/api/file-downloads/EXPORT-ZXhwb3J0X2RvY3VtZW50X2dGZU9JMVYxLmpzb24%3D'\n\n\n\n\n\nThe \n-o\n flag from above specifies the location to save the file to.\n\n\nFile Upload\n\u00b6\n\n\nAPI calls involving file upload are noteworthy because the generated\ncurl from the Masking API client will be \nmissing the parameter\nreferencing the file\n; as such, those commands from the Masking API\nclient \nwill not work\n.\n\n\nInstead, below are examples of working requests and responses for API\ncalls involving file upload.\n\n\nFor commands specific to your masking engine, work with your interactive\nclient at\nhttp://\n<myMaskingEngine>\n:8282/masking/api-client/\n\n\n\n\nWarning\n\n\nHTTPS (SSL/TLS) is recommended, but for explanatory purposes these examples use insecure HTTP.\n\n\n\n\n\n\nNote\n\n\nIn all code examples, replace \n\\<myMaskingEngine>\n with the hostname or IP address of your virtual machine.\n\n\n\n\nCreating a File Format\n\u00b6\n\n\nREQUEST\n\u00b6\n\n\ncurl -X POST --header 'Content-Type: multipart/form-data' --header\n'Accept: application/json' --header 'Authorization:\nd1313dd8-2ed9-4699-8e88-2b6a089ae2a6' -F\nfileFormat=@/path/to/file_format/delimited_format.txt -F\nfileFormatType=DELIMITED\n'http://<myMaskingEngine>:8282/masking/api/file-formats'\n\n\n\n\n\nRESPONSE\n\u00b6\n\n\n{ \"fileFormatId\": 123, \"fileFormatName\": \"delimited_format.txt\",\n\"fileFormatType\": \"DELIMITED\"\n}\n\n\n\n\n\nMore info\n\u00b6\n\n\nhttp://<myMaskingEngine>:8282/masking/api-client/#!/fileFormat/createFileFormat\n\n\nCreating an SSH Key\n\u00b6\n\n\nREQUEST\n\u00b6\n\n\ncurl -X POST --header \n'Content-Type: multipart/form-data'\n --header\n\n'Accept: application/json'\n --header \n'Authorization:\n\n\nd1313dd8-2ed9-4699-8e88-2b6a089ae2a6'\n -F\n\nsshKey\n=\n@/path/to/ssh_key/this_file_name_is_your_ssh_key_name.txt\n\n'http://<myMaskingEngine>:8282/masking/api/ssh-keys'\n\n\n\n\n\n\nRESPONSE\n\u00b6\n\n\n{\n \n\"sshKeyName\"\n:\n \n\"this_file_name_is_your_ssh_key_name.txt\"\n\n\n}\n\n\n\n\n\n\nMore info\n\u00b6\n\n\nhttp://<myMaskingEngine>:8282/masking/api-client/#!/sshKey/createSshKey",
            "title": "API Calls Involving File Upload and Download"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_Involving_File_Upload_and_Download/#api-calls-involving-file-upload-and-download",
            "text": "",
            "title": "API Calls Involving File Upload and Download"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_Involving_File_Upload_and_Download/#file-download",
            "text": "API calls involving file download through API client are noteworthy because if the request fails, the API client will continue to show the \"loading\"  icon indefinitely.  To avoid this, make all file download calls through CURL instead. An example of a file download call using CURL is below.  curl -X GET --header 'Accept: application/octet-stream' --header\n'Authorization: ec443730-124e-4958-a872-324a975bb500'\n-o \"/home/user/downloads\"\n'http://<myMaskingEngine>:8282/masking/api/file-downloads/EXPORT-ZXhwb3J0X2RvY3VtZW50X2dGZU9JMVYxLmpzb24%3D'  The  -o  flag from above specifies the location to save the file to.",
            "title": "File Download"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_Involving_File_Upload_and_Download/#file-upload",
            "text": "API calls involving file upload are noteworthy because the generated\ncurl from the Masking API client will be  missing the parameter\nreferencing the file ; as such, those commands from the Masking API\nclient  will not work .  Instead, below are examples of working requests and responses for API\ncalls involving file upload.  For commands specific to your masking engine, work with your interactive\nclient at\nhttp:// <myMaskingEngine> :8282/masking/api-client/   Warning  HTTPS (SSL/TLS) is recommended, but for explanatory purposes these examples use insecure HTTP.    Note  In all code examples, replace  \\<myMaskingEngine>  with the hostname or IP address of your virtual machine.",
            "title": "File Upload"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_Involving_File_Upload_and_Download/#creating-a-file-format",
            "text": "",
            "title": "Creating a File Format"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_Involving_File_Upload_and_Download/#request",
            "text": "curl -X POST --header 'Content-Type: multipart/form-data' --header\n'Accept: application/json' --header 'Authorization:\nd1313dd8-2ed9-4699-8e88-2b6a089ae2a6' -F\nfileFormat=@/path/to/file_format/delimited_format.txt -F\nfileFormatType=DELIMITED\n'http://<myMaskingEngine>:8282/masking/api/file-formats'",
            "title": "REQUEST"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_Involving_File_Upload_and_Download/#response",
            "text": "{ \"fileFormatId\": 123, \"fileFormatName\": \"delimited_format.txt\",\n\"fileFormatType\": \"DELIMITED\"\n}",
            "title": "RESPONSE"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_Involving_File_Upload_and_Download/#more-info",
            "text": "http://<myMaskingEngine>:8282/masking/api-client/#!/fileFormat/createFileFormat",
            "title": "More info"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_Involving_File_Upload_and_Download/#creating-an-ssh-key",
            "text": "",
            "title": "Creating an SSH Key"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_Involving_File_Upload_and_Download/#request_1",
            "text": "curl -X POST --header  'Content-Type: multipart/form-data'  --header 'Accept: application/json'  --header  'Authorization:  d1313dd8-2ed9-4699-8e88-2b6a089ae2a6'  -F sshKey = @/path/to/ssh_key/this_file_name_is_your_ssh_key_name.txt 'http://<myMaskingEngine>:8282/masking/api/ssh-keys'",
            "title": "REQUEST"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_Involving_File_Upload_and_Download/#response_1",
            "text": "{   \"sshKeyName\" :   \"this_file_name_is_your_ssh_key_name.txt\"  }",
            "title": "RESPONSE"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/API_Calls_Involving_File_Upload_and_Download/#more-info_1",
            "text": "http://<myMaskingEngine>:8282/masking/api-client/#!/sshKey/createSshKey",
            "title": "More info"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Backwards_Compatibility_API_Usage/",
            "text": "Backwards Compatibility API Usage\n\u00b6\n\n\n\n\nNote\n\n\nIn all examples, replace \n<myMaskingEngine>\n with the hostname or IP address of your virtual machine. |\n\n\n\n\nIn all examples, replace \n<myMaskingEngine>\n with the hostname or IP\naddress of your virtual machine.\n\n\nAPI Versioning Context\n\u00b6\n\n\nThe Masking API being shipped with the 5.2 series of releases of the\nDelphix Masking Engine is version \nv5.0.0\n in accordance with the\nSemantic Versioning format:\n\nhttp://semver.org/\n.\nIn subsequent maintenance and major releases of the Masking product, the\nMasking API may be updated and a new API version will be released \n(e.g.\n\nv5.0.1\n, \nv5.1.0\n, etc).\n As scripts using the new Masking API are\nbeing written, they must reference an explicit API version or else there\nare no guarantees that the scripts will work on future releases of the\nMasking product.\n\n\nPinning Down a Version Number To Guarantee Backwards-Compatibility\n\u00b6\n\n\n'http://<myMaskingEngine>:8282/masking/api/v5.0.0/environments'\n\n\nThis is the format for specifying a version in the URL of an API request\ntargeting the \nenvironments\n endpoints. The only possible version\nvalue for the Masking API in the first 5.2 release is \nv5.0.0\n. As\nmore releases of the Masking product are shipped in the future, the set\nof possible versions will expand.\n\n\nScripts that specifically pin down the version of the Masking API in the\nURL will continue to work upon future upgrades of the Masking\nproduct--even if a newer version of the API is available in the future\nMasking product\u2013with the exception that\n\nIncubating API Endpoints\n\nare never guaranteed to be backwards-compatible.\n\n\nFor example, consider the scenario where a script is being developed\ntoday with a pinned down version \nv5.0.0\n in the URL of the API\nrequests. Upon upgrade to a future release of the Masking product that\nhas the API \nv5.1.0\n available, the same, untouched script that was\ndeveloped with the pinned down version \nv5.0.0\n in the URL of the API\nrequests is expected to continue working. That said, in order to\nleverage any new features of the API \nv5.1.0\n, the original script\nwill need to be updated to specify the new API version in the URL, and\nthe requests may need to be updated to conform to the new API\nspecification.\n\n\nOmitted Version Numbers\n\u00b6\n\n\n'http://<myMaskingEngine>:8282/masking/api/environments'\n\n\nThis is the format for not specifying a version in the URL of an API\nrequest targeting the \nenvironments\n endpoints. When the API version\nnumber is omitted, the latest API version is taken as a default. In the\nfirst 5.2 release, an API request with an omitted version number will be\ninterpreted as a request against the \nv5.0.0\n version of the API. In a\nfuture release that hypothetically has the API \nv5.3.0\n available, an\nAPI request with an omitted version number will be interpreted as a\nrequest against the \nv5.3.0\n version of the API.\n\n\nScripts that omit the version of the Masking API in the URL are not\nguaranteed to work upon future upgrades of the Masking product because\nthe API specification may change between versions, and requests that\nconform to the old API specification may not work on the new API\nspecification.",
            "title": "Backwards Compatibility API Usage"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Backwards_Compatibility_API_Usage/#backwards-compatibility-api-usage",
            "text": "Note  In all examples, replace  <myMaskingEngine>  with the hostname or IP address of your virtual machine. |   In all examples, replace  <myMaskingEngine>  with the hostname or IP\naddress of your virtual machine.",
            "title": "Backwards Compatibility API Usage"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Backwards_Compatibility_API_Usage/#api-versioning-context",
            "text": "The Masking API being shipped with the 5.2 series of releases of the\nDelphix Masking Engine is version  v5.0.0  in accordance with the\nSemantic Versioning format: http://semver.org/ .\nIn subsequent maintenance and major releases of the Masking product, the\nMasking API may be updated and a new API version will be released  (e.g. v5.0.1 ,  v5.1.0 , etc).  As scripts using the new Masking API are\nbeing written, they must reference an explicit API version or else there\nare no guarantees that the scripts will work on future releases of the\nMasking product.",
            "title": "API Versioning Context"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Backwards_Compatibility_API_Usage/#pinning-down-a-version-number-to-guarantee-backwards-compatibility",
            "text": "'http://<myMaskingEngine>:8282/masking/api/v5.0.0/environments'  This is the format for specifying a version in the URL of an API request\ntargeting the  environments  endpoints. The only possible version\nvalue for the Masking API in the first 5.2 release is  v5.0.0 . As\nmore releases of the Masking product are shipped in the future, the set\nof possible versions will expand.  Scripts that specifically pin down the version of the Masking API in the\nURL will continue to work upon future upgrades of the Masking\nproduct--even if a newer version of the API is available in the future\nMasking product\u2013with the exception that Incubating API Endpoints \nare never guaranteed to be backwards-compatible.  For example, consider the scenario where a script is being developed\ntoday with a pinned down version  v5.0.0  in the URL of the API\nrequests. Upon upgrade to a future release of the Masking product that\nhas the API  v5.1.0  available, the same, untouched script that was\ndeveloped with the pinned down version  v5.0.0  in the URL of the API\nrequests is expected to continue working. That said, in order to\nleverage any new features of the API  v5.1.0 , the original script\nwill need to be updated to specify the new API version in the URL, and\nthe requests may need to be updated to conform to the new API\nspecification.",
            "title": "Pinning Down a Version Number To Guarantee Backwards-Compatibility"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Backwards_Compatibility_API_Usage/#omitted-version-numbers",
            "text": "'http://<myMaskingEngine>:8282/masking/api/environments'  This is the format for not specifying a version in the URL of an API\nrequest targeting the  environments  endpoints. When the API version\nnumber is omitted, the latest API version is taken as a default. In the\nfirst 5.2 release, an API request with an omitted version number will be\ninterpreted as a request against the  v5.0.0  version of the API. In a\nfuture release that hypothetically has the API  v5.3.0  available, an\nAPI request with an omitted version number will be interpreted as a\nrequest against the  v5.3.0  version of the API.  Scripts that omit the version of the Masking API in the URL are not\nguaranteed to work upon future upgrades of the Masking product because\nthe API specification may change between versions, and requests that\nconform to the old API specification may not work on the new API\nspecification.",
            "title": "Omitted Version Numbers"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Incubating_API_Endpoints/",
            "text": "Incubating API Endpoints\n\u00b6\n\n\nContext\n\u00b6\n\n\nAPIs that are released across the industry are expected to have a stable\nspecification that consumers can depend on when writing scripts and\nautomation. This notion of a stable API specification is at odds with\nthe natural process of iteration and refinement that a newly released\nfeature is expected to undergo. As such, in order to accommodate the\nanticipated iteration and refinement of this newly released Masking API,\nDelphix is introducing the notion of Incubating API endpoints.\n\n\nDefinition\n\u00b6\n\n\nAn Incubating API endpoint is available for immediate use, but the\nspecification of an Incubating API endpoint is subject to change in the\nfuture \n(i.e. the specification is not stable)\n.\n\n\nBackwards-Compatibility of Incubating API Endpoints\n\u00b6\n\n\nThere are no backwards-compatibility guarantees when using Incubating\nAPI endpoints, even when \npinning down the API\nversion number\n.\n\n\nThat said, it is not the case that an Incubating API will \nalways\n\nchange in a future release, but rather that it \nmight\n change in a\nfuture release such that any scripts that were developed to use an\nIncubating API would need to be updated to work against a future release\nof the\nAPI.\n\n\n\n\nNote\n\n\nAll changes to the API \n(not just backwards-incompatible changes)\n will be documented and distributed with future releases of the API.\n\n\n\n\nBackwards-incompatible changes to the API are known to be disruptive to\nautomation built around the API, and therefore changes to Incubating\nAPIs will be carefully considered and minimized.\n\n\nList of Incubating API Endpoints\n\u00b6\n\n\nRefer to the The Masking API Client [Need to add link] to see the list\nof Incubating API endpoints.\n\n\nAll Incubating API endpoints are labeled with \nINCUBATING\n in their\ndescription, and they are also accompanied by an \nImplementation Note\n\nexplaining the implications of an Incubating endpoint with respect to\nbackwards-compatibility.",
            "title": "Incubating API Endpoints"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Incubating_API_Endpoints/#incubating-api-endpoints",
            "text": "",
            "title": "Incubating API Endpoints"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Incubating_API_Endpoints/#context",
            "text": "APIs that are released across the industry are expected to have a stable\nspecification that consumers can depend on when writing scripts and\nautomation. This notion of a stable API specification is at odds with\nthe natural process of iteration and refinement that a newly released\nfeature is expected to undergo. As such, in order to accommodate the\nanticipated iteration and refinement of this newly released Masking API,\nDelphix is introducing the notion of Incubating API endpoints.",
            "title": "Context"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Incubating_API_Endpoints/#definition",
            "text": "An Incubating API endpoint is available for immediate use, but the\nspecification of an Incubating API endpoint is subject to change in the\nfuture  (i.e. the specification is not stable) .",
            "title": "Definition"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Incubating_API_Endpoints/#backwards-compatibility-of-incubating-api-endpoints",
            "text": "There are no backwards-compatibility guarantees when using Incubating\nAPI endpoints, even when  pinning down the API\nversion number .  That said, it is not the case that an Incubating API will  always \nchange in a future release, but rather that it  might  change in a\nfuture release such that any scripts that were developed to use an\nIncubating API would need to be updated to work against a future release\nof the\nAPI.   Note  All changes to the API  (not just backwards-incompatible changes)  will be documented and distributed with future releases of the API.   Backwards-incompatible changes to the API are known to be disruptive to\nautomation built around the API, and therefore changes to Incubating\nAPIs will be carefully considered and minimized.",
            "title": "Backwards-Compatibility of Incubating API Endpoints"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Incubating_API_Endpoints/#list-of-incubating-api-endpoints",
            "text": "Refer to the The Masking API Client [Need to add link] to see the list\nof Incubating API endpoints.  All Incubating API endpoints are labeled with  INCUBATING  in their\ndescription, and they are also accompanied by an  Implementation Note \nexplaining the implications of an Incubating endpoint with respect to\nbackwards-compatibility.",
            "title": "List of Incubating API Endpoints"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/",
            "text": "Algorithm Extensions\n\u00b6\n\n\nModels\n\u00b6\n\n\nAlgorithm\n\u00b6\n\n\n\n\nalgorithmName\n(maxLength=500)\n\n\nString\n\nEquivalent to the algorithm name saved by the user through the GUI.\nFor out of the box algorithms, this will be a similar name as that in\nthe GUI, but presented in a more user-friendly\nformat.\n\n\nalgorithmType\n\n\nString\n\nThe type of algorithm\n\n\nEnum:\n\n\nBINARY_LOOKUP\n\n\nCLEANSING\n\n\nLOOKUP\n\n\nMAPPLET\n\n\nMAPPING\n\n\nMINMAX\n\n\nREDACTION\n\n\nSEGMENT\n\n\nTOKENIZATION\n\n\ncreatedBy (optional; readOnly;\nmaxLength=255)\n\n\nString\n\nThe name of the user that created the algorithm\n\n\ndescription (optional;\nmaxLength=255)\n\n\nString\n\nThe description of the algorithm\n\n\nalgorithmExtension (optional)\n\n\nObject\n\n\n\n\nAlgorithmExtension\n\u00b6\n\n\nBinaryLookupExtension\n\u00b6\n\n\n\n\nfileReferenceIds (optional;\nmaxLength=36)\n\n\narray[String]\n\nA list of file reference UUID values returned from the endpoint for\nuploading files to the Masking Engine.\n\n\n\n\nDataCleansingExtension\n\u00b6\n\n\n\n\nfileReferenceId\n(optional)\n\n\nString\n\nThe reference UUID value returned from the endpoint for uploading\nfiles to the Masking Engine. The file should contain a newline\nseparated list of {value, replacement} pairs separated by the\ndelimiter. No extraneous whitespace should be present.\n\n\ndelimiter (optional; minLength=1; maxLength=50;\ndefault=\"=\")\n\n\nString\n\nThe delimiter string used to separate {value, replacement} pairs in\nthe uploaded file\n\n\n\n\nFreeTextRedactionExtension\n\u00b6\n\n\n\n\nblackListRedaction (optional;\ndefault=true)\n\n\nBoolean\n\nBlack list redaction if true, white list redaction if false.\n\n\nlookupFileReferenceId (optional;\nmaxLength=36)\n\n\nString\n\nThe reference UUID value returned from the endpoint for uploading the\nlookup file to the Masking Engine.\n\n\nlookupRedactionValue (optional;\nmaxLength=255)\n\n\nString\n\nThe value to use to redact items matching entries specified in the\nlookup file.\n\n\nprofileSetId\n(optional)\n\n\nInteger\n\nThe ID number of the profile set for defining the pattern matching to\nuse for identifying values for redaction. format: int32\n\n\nprofileSetRedactionValue (optional;\nmaxLength=255)\n\n\nString\n\nThe value to use to redact items matching patterns defined by the\nprofile set.\n\n\n\n\nMappingExtension\n\u00b6\n\n\n\n\nfileReferenceId\n(optional)\n\n\nString\n\nThe reference UUID value returned from the endpoint for uploading\nfiles to the Masking Engine. The file should contain a newline\nseparated list of mapping values.\n\n\nignoreCharacters (optional; minimum=32;\nmaximum=126)\n\n\narray[Integer]\n\nThe integer ASCII values of characters to ignore in the column data to\nmap\n\n\n\n\nMappletExtension\n\u00b6\n\n\n\n\nmappletInput (optional;\nmaxLength=500)\n\n\nString\n\nThe name of the input variable for the custom algorithm\n\n\nmappletOutput (optional;\nmaxLength=500)\n\n\nString\n\nThe name of the output variable for the custom algorithm\n\n\nfileReferenceId (optional;\nmaxLength=36)\n\n\nString\n\nThe reference UUID value returned from the endpoint for uploading\nfiles to the Masking Engine.\n\n\n\n\nMinMaxExtension\n\u00b6\n\n\n\n\nminValue (optional;\nminimum=0)\n\n\nInteger\n\nThe minimum value for a Number range used in conjunction with\nmaxValue. This field cannot be combined with minDate or maxDate.\nformat: int32\n\n\nmaxValue (optional;\nminimum=1)\n\n\nInteger\n\nThe maximum value for a Number range used in conjunction with and must\nbe greater than minValue. This field cannot be combined with minDate\nor maxDate. format: int32\n\n\nminDate\n(optional)\n\n\ndate\n\nThe minimum value for a Date range used in conjunction with maxDate.\nThe Date must be specified in one of the following formats according\nto RFC 3339 Section 5.6: \"yyyy-MM-dd\", \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\",\n\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\", or \"EEE, dd MMM yyyy HH:mm:ss zzz\". If\na timezone is not specified, the Date will be interpreted as UTC. This\nfield cannot be combined with minValue or maxValue. format: date\n\n\nmaxDate\n(optional)\n\n\ndate\n\nThe maximum value for a Date range used in conjunction with and must\nbe greater than minDate. The Date must be specified in one of the\nfollowing formats according to RFC 3339 Section 5.6: \"yyyy-MM-dd\",\n\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\", \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\", or \"EEE,\ndd MMM yyyy HH:mm:ss zzz\". If a timezone is not specified, the Date\nwill be interpreted as UTC. This field cannot be combined with\nminValue or maxValue. format: date\n\n\noutOfRangeDefaultValue (optional;\nmaxLength=255)\n\n\nString\n\nThe default replacement value for any value that is out-of-range.\n\n\n\n\nSecureLookupExtension\n\u00b6\n\n\n\n\nfileReferenceId (optional;\nmaxLength=36)\n\n\nString\n\nThe reference UUID value returned from the endpoint for uploading\nfiles to the Masking Engine.\n\n\n\n\nSegmentMappingExtension\n\u00b6\n\n\n\n\npreservedRanges\n(optional)\n\n\narray[SegmentMappingPreservedRange]\n\nList of character {offset, length} values specifying ranges of the\nreal value to preserve. Offsets begin at 0\n\n\nignoreCharacters\n(optional)\n\n\narray[Integer]\n\nList of decimal values specifying ASCII characters to ignore (not\nmask, not count as part of any segment) in the real value. For\nexample, 65 would ignore 'A'\n\n\nsegments (optional; minItems=2; maxItems=36)\n\n\narray[SegmentMappingSegment]\n\n\n\n\nSegmentMappingPreservedRange\n\u00b6\n\n\n\n\noffset\n(optional)\n\n\nInteger\n\nThe character offset of the range of input to preserve\n\n\nlength\n(optional)\n\n\nInteger\n\nThe character length of the range of input to preserve\n\n\n\n\nSegmentMappingSegment\n\u00b6\n\n\n\n\nlength (optional; minimum=1;\nmaximum=4)\n\n\nInteger\n\nThe length of the segment in digits. This must be 1 for alpha-numeric\nsegments\n\n\nminInt (optional; minimum=0;\nmaximum=9999)\n\n\nInteger\n\nThe minimum value of the integer output range of the mapping function\n\n\nmaxInt (optional; minimum=0;\nmaximum=9999)\n\n\nInteger\n\nThe maximum value of the integer output range of the mapping function\n\n\nminChar (optional; minLength=1;\nmaxLength=1)\n\n\nString\n\nThe minimum value of the character output range of the mapping\nfunction\n\n\nmaxChar (optional; minLength=1;\nmaxLength=1)\n\n\nString\n\nThe maximum value of the character output range of the mapping\nfunction\n\n\nexplicitRange\n(optional)\n\n\nString\n\nExplicitly specify the output range. Format depends on segment type\nand size\n\n\nminRealInt (optional; minimum=0;\nmaximum=9999)\n\n\nInteger\n\nThe minimum value of the integer range specifying which real values\nwill be masked\n\n\nmaxRealInt (optional; minimum=0;\nmaximum=9999)\n\n\nInteger\n\nThe maximum value of the integer range specifying which real values\nwill be masked\n\n\nminRealChar (optional; minLength=1;\nmaxLength=1)\n\n\nString\n\nThe minimum value of the character range specifying which real values\nwill be masked\n\n\nmaxRealChar (optional; minLength=1;\nmaxLength=1)\n\n\nString\n\nThe maximum value of the character range specifying which real values\nwill be masked\n\n\nexplicitRealRange\n(optional)\n\n\nString\n\nExplicitly specify the range of input values that should be masked.\nFormat depends on segment type and size",
            "title": "Algorithm Extensions"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#algorithm-extensions",
            "text": "",
            "title": "Algorithm Extensions"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#models",
            "text": "",
            "title": "Models"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#algorithm",
            "text": "algorithmName\n(maxLength=500)  String \nEquivalent to the algorithm name saved by the user through the GUI.\nFor out of the box algorithms, this will be a similar name as that in\nthe GUI, but presented in a more user-friendly\nformat.  algorithmType  String \nThe type of algorithm  Enum:  BINARY_LOOKUP  CLEANSING  LOOKUP  MAPPLET  MAPPING  MINMAX  REDACTION  SEGMENT  TOKENIZATION  createdBy (optional; readOnly;\nmaxLength=255)  String \nThe name of the user that created the algorithm  description (optional;\nmaxLength=255)  String \nThe description of the algorithm  algorithmExtension (optional)  Object",
            "title": "Algorithm"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#algorithmextension",
            "text": "",
            "title": "AlgorithmExtension"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#binarylookupextension",
            "text": "fileReferenceIds (optional;\nmaxLength=36)  array[String] \nA list of file reference UUID values returned from the endpoint for\nuploading files to the Masking Engine.",
            "title": "BinaryLookupExtension"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#datacleansingextension",
            "text": "fileReferenceId\n(optional)  String \nThe reference UUID value returned from the endpoint for uploading\nfiles to the Masking Engine. The file should contain a newline\nseparated list of {value, replacement} pairs separated by the\ndelimiter. No extraneous whitespace should be present.  delimiter (optional; minLength=1; maxLength=50;\ndefault=\"=\")  String \nThe delimiter string used to separate {value, replacement} pairs in\nthe uploaded file",
            "title": "DataCleansingExtension"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#freetextredactionextension",
            "text": "blackListRedaction (optional;\ndefault=true)  Boolean \nBlack list redaction if true, white list redaction if false.  lookupFileReferenceId (optional;\nmaxLength=36)  String \nThe reference UUID value returned from the endpoint for uploading the\nlookup file to the Masking Engine.  lookupRedactionValue (optional;\nmaxLength=255)  String \nThe value to use to redact items matching entries specified in the\nlookup file.  profileSetId\n(optional)  Integer \nThe ID number of the profile set for defining the pattern matching to\nuse for identifying values for redaction. format: int32  profileSetRedactionValue (optional;\nmaxLength=255)  String \nThe value to use to redact items matching patterns defined by the\nprofile set.",
            "title": "FreeTextRedactionExtension"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#mappingextension",
            "text": "fileReferenceId\n(optional)  String \nThe reference UUID value returned from the endpoint for uploading\nfiles to the Masking Engine. The file should contain a newline\nseparated list of mapping values.  ignoreCharacters (optional; minimum=32;\nmaximum=126)  array[Integer] \nThe integer ASCII values of characters to ignore in the column data to\nmap",
            "title": "MappingExtension"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#mappletextension",
            "text": "mappletInput (optional;\nmaxLength=500)  String \nThe name of the input variable for the custom algorithm  mappletOutput (optional;\nmaxLength=500)  String \nThe name of the output variable for the custom algorithm  fileReferenceId (optional;\nmaxLength=36)  String \nThe reference UUID value returned from the endpoint for uploading\nfiles to the Masking Engine.",
            "title": "MappletExtension"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#minmaxextension",
            "text": "minValue (optional;\nminimum=0)  Integer \nThe minimum value for a Number range used in conjunction with\nmaxValue. This field cannot be combined with minDate or maxDate.\nformat: int32  maxValue (optional;\nminimum=1)  Integer \nThe maximum value for a Number range used in conjunction with and must\nbe greater than minValue. This field cannot be combined with minDate\nor maxDate. format: int32  minDate\n(optional)  date \nThe minimum value for a Date range used in conjunction with maxDate.\nThe Date must be specified in one of the following formats according\nto RFC 3339 Section 5.6: \"yyyy-MM-dd\", \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\",\n\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\", or \"EEE, dd MMM yyyy HH:mm:ss zzz\". If\na timezone is not specified, the Date will be interpreted as UTC. This\nfield cannot be combined with minValue or maxValue. format: date  maxDate\n(optional)  date \nThe maximum value for a Date range used in conjunction with and must\nbe greater than minDate. The Date must be specified in one of the\nfollowing formats according to RFC 3339 Section 5.6: \"yyyy-MM-dd\",\n\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\", \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\", or \"EEE,\ndd MMM yyyy HH:mm:ss zzz\". If a timezone is not specified, the Date\nwill be interpreted as UTC. This field cannot be combined with\nminValue or maxValue. format: date  outOfRangeDefaultValue (optional;\nmaxLength=255)  String \nThe default replacement value for any value that is out-of-range.",
            "title": "MinMaxExtension"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#securelookupextension",
            "text": "fileReferenceId (optional;\nmaxLength=36)  String \nThe reference UUID value returned from the endpoint for uploading\nfiles to the Masking Engine.",
            "title": "SecureLookupExtension"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#segmentmappingextension",
            "text": "preservedRanges\n(optional)  array[SegmentMappingPreservedRange] \nList of character {offset, length} values specifying ranges of the\nreal value to preserve. Offsets begin at 0  ignoreCharacters\n(optional)  array[Integer] \nList of decimal values specifying ASCII characters to ignore (not\nmask, not count as part of any segment) in the real value. For\nexample, 65 would ignore 'A'  segments (optional; minItems=2; maxItems=36)  array[SegmentMappingSegment]",
            "title": "SegmentMappingExtension"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#segmentmappingpreservedrange",
            "text": "offset\n(optional)  Integer \nThe character offset of the range of input to preserve  length\n(optional)  Integer \nThe character length of the range of input to preserve",
            "title": "SegmentMappingPreservedRange"
        },
        {
            "location": "/Delphix_Masking_APIs/Masking_Client/Algorithm_Extensions/#segmentmappingsegment",
            "text": "length (optional; minimum=1;\nmaximum=4)  Integer \nThe length of the segment in digits. This must be 1 for alpha-numeric\nsegments  minInt (optional; minimum=0;\nmaximum=9999)  Integer \nThe minimum value of the integer output range of the mapping function  maxInt (optional; minimum=0;\nmaximum=9999)  Integer \nThe maximum value of the integer output range of the mapping function  minChar (optional; minLength=1;\nmaxLength=1)  String \nThe minimum value of the character output range of the mapping\nfunction  maxChar (optional; minLength=1;\nmaxLength=1)  String \nThe maximum value of the character output range of the mapping\nfunction  explicitRange\n(optional)  String \nExplicitly specify the output range. Format depends on segment type\nand size  minRealInt (optional; minimum=0;\nmaximum=9999)  Integer \nThe minimum value of the integer range specifying which real values\nwill be masked  maxRealInt (optional; minimum=0;\nmaximum=9999)  Integer \nThe maximum value of the integer range specifying which real values\nwill be masked  minRealChar (optional; minLength=1;\nmaxLength=1)  String \nThe minimum value of the character range specifying which real values\nwill be masked  maxRealChar (optional; minLength=1;\nmaxLength=1)  String \nThe maximum value of the character range specifying which real values\nwill be masked  explicitRealRange\n(optional)  String \nExplicitly specify the range of input values that should be masked.\nFormat depends on segment type and size",
            "title": "SegmentMappingSegment"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/loginCredentials/",
            "text": "loginCredentials\n\u00b6\n\n\nLogin credentials for the Masking Engine.\n\n\nUSERNAME=\"myUsername\"\nPASSWORD=\"myPassword\"",
            "title": "loginCredentials"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/loginCredentials/#logincredentials",
            "text": "Login credentials for the Masking Engine.  USERNAME=\"myUsername\"\nPASSWORD=\"myPassword\"",
            "title": "loginCredentials"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/helpers/",
            "text": "helpers\n\u00b6\n\n\n#!/bin/bash\n\n\n\n#\n\n\n# This file contains helpers for the various Masking API cookbook scripts.\n\n\n# This script uses jq to process JSON. More information can be found here - https://stedolan.github.io/jq/.\n\n\n#\n\n\n\n# Login and set the correct $AUTH_HEADER.\n\nlogin\n()\n \n{\n\n    \necho\n \n\"* logging in...\"\n\n    \nLOGIN_RESPONSE\n=\n$(\ncurl -s \n$SSL_CERT\n -X POST -H \n'Content-Type: application/json'\n -H \n'Accept: application/json'\n --data @- \n$MASKING_ENGINE\n/login \n<<EOF\n\n\n{\n\n\n  \"username\": \"$USERNAME\",\n\n\n  \"password\": \"$PASSWORD\"\n\n\n}\n\n\nEOF\n)\n\n    check_error \n\"\n$LOGIN_RESPONSE\n\"\n\n    \nTOKEN\n=\n$(\necho\n \n$LOGIN_RESPONSE\n \n|\n jq -r \n'.Authorization'\n)\n\n    \nAUTH_HEADER\n=\n\"Authorization: \n$TOKEN\n\"\n\n\n}\n\n\n\n# Get all applications and select the first one. Place the applicationName in $APPLICATION_ID.\n\nget_application_id\n()\n \n{\n\n    \necho\n \n\"* getting all applications and selecting first one\"\n\n    \nAPPLICATIONS_RESPONSE\n=\n$(\ncurl -s \n$SSL_CERT\n -X GET -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n \n$MASKING_ENGINE\n/applications\n)\n\n    check_error \n\"\n$APPLICATIONS_RESPONSE\n\"\n\n    \nNUM_APPLICATIONS\n=\n$(\necho\n \n$APPLICATIONS_RESPONSE\n \n|\n jq -r \n'._pageInfo.total'\n)\n\n    check_empty \n$NUM_APPLICATIONS\n \n\"found no applications to use\"\n\n    \nAPPLICATION_ID\n=\n$(\necho\n \n$APPLICATIONS_RESPONSE\n \n|\n jq -r \n'.responseList[0].applicationName'\n)\n\n    \necho\n \n\"using application '\n$APPLICATION_ID\n'\"\n\n\n}\n\n\n# Get all environments and select the first one. Place the environmentId in $ENVIRONMENT_ID.\n\nget_environment_id\n()\n \n{\n\n    \necho\n \n\"* getting all environments and selecting first one\"\n\n    \nENVIRONMENTS_RESPONSE\n=\n$(\ncurl -s \n$SSL_CERT\n -X GET -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n \n$MASKING_ENGINE\n/environments\n)\n\n    check_error \n\"\n$ENVIRONMENTS_RESPONSE\n\"\n\n    \nNUM_ENVIRONMENTS\n=\n$(\necho\n \n$ENVIRONMENTS_RESPONSE\n \n|\n jq -r \n'._pageInfo.total'\n)\n\n    check_empty \n$NUM_ENVIRONMENTS\n \n\"found no environments to use\"\n\n    \nENVIRONMENT_ID\n=\n$(\necho\n \n$ENVIRONMENTS_RESPONSE\n \n|\n jq -r \n'.responseList[0].environmentId'\n)\n\n    \necho\n \n\"using environment '\n$ENVIRONMENT_ID\n'\"\n\n\n}\n\n\n# Get all database connectors and select the first one. Place the databaseConnectorId in $CONNECTOR_ID.\n\nget_connector_id\n()\n \n{\n\n    \necho\n \n\"* getting all database connectors and selecting first one\"\n\n    \nCONNECTORS_RESPONSE\n=\n$(\ncurl -s \n$SSL_CERT\n -X GET -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n \n$MASKING_ENGINE\n/database-connectors\n)\n\n    check_error \n\"\n$CONNECTORS_RESPONSE\n\"\n\n    \nNUM_CONNECTORS\n=\n$(\necho\n \n$CONNECTORS_RESPONSE\n \n|\n jq -r \n'._pageInfo.total'\n)\n\n    check_empty \n$NUM_CONNECTORS\n \n\"found no db connectors to use\"\n\n    \nCONNECTOR_ID\n=\n$(\necho\n \n$CONNECTORS_RESPONSE\n \n|\n jq -r \n'.responseList[0].databaseConnectorId'\n)\n\n    \necho\n \n\"using database connector '\n$CONNECTOR_ID\n'\"\n\n\n}\n\n\n\n# Get all database rulesets and select the first one. Place the databaseRulesetId in $RULESET_ID.\n\nget_ruleset_id\n()\n \n{\n\n    \necho\n \n\"* getting all database rulesets and selecting first one\"\n\n    \nRULESETS_RESPONSE\n=\n$(\ncurl -s \n$SSL_CERT\n -X GET -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n \n$MASKING_ENGINE\n/database-rulesets\n)\n\n    check_error \n\"\n$RULESETS_RESPONSE\n\"\n\n    \nNUM_RULESETS\n=\n$(\necho\n \n$RULESETS_RESPONSE\n \n|\n jq -r \n'._pageInfo.total'\n)\n\n    check_empty \n$NUM_RULESETS\n \n\"found no db rulesets to use\"\n\n    \nRULESET_ID\n=\n$(\necho\n \n$RULESETS_RESPONSE\n \n|\n jq -r \n'.responseList[0].databaseRulesetId'\n)\n\n    \necho\n \n\"using database ruleset '\n$RULESET_ID\n'\"\n\n\n}\n\n\n# Get all database tables for a database connector specificed by $CONNECTOR_ID. Select the first one and place in $TABLE_NAME.\n\nget_table\n()\n \n{\n\n    \necho\n \n\"* getting all tables for connector '\n$CONNECTOR_ID\n' and selecting first one\"\n\n    \nTABLES_RESPONSE\n=\n$(\ncurl -s \n$SSL_CERT\n -X GET -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n \n$MASKING_ENGINE\n/database-connectors/\n$CONNECTOR_ID\n/fetch\n)\n\n    check_error \n\"\n$TABLES_RESPONSE\n\"\n\n    \nNUM_TABLES\n=\n$(\necho\n \n$TABLES_RESPONSE\n \n|\n jq -r \n'. | length'\n)\n\n    check_empty \n$NUM_TABLES\n \n\"found no tables to use\"\n\n    \nTABLE_NAME\n=\n$(\necho\n \n$TABLES_RESPONSE\n \n|\n jq -r \n'.[0]'\n)\n\n    \necho\n \n\"using table '\n$TABLE_NAME\n'\"\n\n\n}\n\n\n\n# Get all column metadata for table metadata specified by $TABLE_METADATA_ID. Select the first one and place in $COLUMN_METADATA_ID.\n\nget_column_metadata_id\n()\n \n{\n\n    \necho\n \n\"* getting all column metadata belonging to table metadata '\n$TABLE_METADATA_ID\n' and selecting the first one\"\n\n    \nCOLUMNS_RESPONSE\n=\n$(\ncurl -s \n$SSL_CERT\n -X GET -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n \n$MASKING_ENGINE\n/column-metadata?table_metadata_id\n=\n$TABLE_METADATA_ID\n)\n\n    check_error \n\"\n$COLUMNS_RESPONSE\n\"\n\n    \nNUM_COLUMNS\n=\n$(\necho\n \n$COLUMNS_RESPONSE\n \n|\n jq -r \n'. | length'\n)\n\n    check_empty \n$NUM_COLUMNS\n \n\"found no columns to use\"\n\n    \nCOLUMN_METADATA\n=\n$(\necho\n \n$COLUMNS_RESPONSE\n \n|\n jq -r \n'.responseList[0]'\n)\n\n    \nCOLUMN_METADATA_ID\n=\n$(\necho\n \n$COLUMN_METADATA\n \n|\n jq -r \n'.columnMetadataId'\n)\n\n    \necho\n \n\"using column '\n$COLUMN_METADATA_ID\n'\"\n\n\n}\n\n\n\n# Check if $1 is equal to 0. If so print out message specified in $2 and exit.\n\ncheck_empty\n()\n \n{\n\n    \nif\n \n[\n \n$1\n -eq \n0\n \n]\n;\n \nthen\n\n        \necho\n \n$2\n\n        \nexit\n \n1\n\n    \nfi\n\n\n}\n\n\n\n# Check if $1 is an object and if it has an 'errorMessage' specified. If so, print the object and exit.\n\ncheck_error\n()\n \n{\n\n    \n# jq returns a literal null so we have to check againt that...\n\n    \nif\n \n[\n \n\"\n$(\necho\n \n\"\n$1\n\"\n \n|\n jq -r \n'if type==\"object\" then .errorMessage else \"null\" end'\n)\n\"\n !\n=\n \n'null'\n \n]\n;\n \nthen\n\n        \necho\n \n$1\n\n        \nexit\n \n1\n\n    \nfi\n\n\n}",
            "title": "helpers"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/helpers/#helpers",
            "text": "#!/bin/bash  #  # This file contains helpers for the various Masking API cookbook scripts.  # This script uses jq to process JSON. More information can be found here - https://stedolan.github.io/jq/.  #  # Login and set the correct $AUTH_HEADER. \nlogin ()   { \n     echo   \"* logging in...\" \n     LOGIN_RESPONSE = $( curl -s  $SSL_CERT  -X POST -H  'Content-Type: application/json'  -H  'Accept: application/json'  --data @-  $MASKING_ENGINE /login  <<EOF  {    \"username\": \"$USERNAME\",    \"password\": \"$PASSWORD\"  }  EOF ) \n    check_error  \" $LOGIN_RESPONSE \" \n     TOKEN = $( echo   $LOGIN_RESPONSE   |  jq -r  '.Authorization' ) \n     AUTH_HEADER = \"Authorization:  $TOKEN \"  }  # Get all applications and select the first one. Place the applicationName in $APPLICATION_ID. \nget_application_id ()   { \n     echo   \"* getting all applications and selecting first one\" \n     APPLICATIONS_RESPONSE = $( curl -s  $SSL_CERT  -X GET -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'   $MASKING_ENGINE /applications ) \n    check_error  \" $APPLICATIONS_RESPONSE \" \n     NUM_APPLICATIONS = $( echo   $APPLICATIONS_RESPONSE   |  jq -r  '._pageInfo.total' ) \n    check_empty  $NUM_APPLICATIONS   \"found no applications to use\" \n     APPLICATION_ID = $( echo   $APPLICATIONS_RESPONSE   |  jq -r  '.responseList[0].applicationName' ) \n     echo   \"using application ' $APPLICATION_ID '\"  }  # Get all environments and select the first one. Place the environmentId in $ENVIRONMENT_ID. \nget_environment_id ()   { \n     echo   \"* getting all environments and selecting first one\" \n     ENVIRONMENTS_RESPONSE = $( curl -s  $SSL_CERT  -X GET -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'   $MASKING_ENGINE /environments ) \n    check_error  \" $ENVIRONMENTS_RESPONSE \" \n     NUM_ENVIRONMENTS = $( echo   $ENVIRONMENTS_RESPONSE   |  jq -r  '._pageInfo.total' ) \n    check_empty  $NUM_ENVIRONMENTS   \"found no environments to use\" \n     ENVIRONMENT_ID = $( echo   $ENVIRONMENTS_RESPONSE   |  jq -r  '.responseList[0].environmentId' ) \n     echo   \"using environment ' $ENVIRONMENT_ID '\"  }  # Get all database connectors and select the first one. Place the databaseConnectorId in $CONNECTOR_ID. \nget_connector_id ()   { \n     echo   \"* getting all database connectors and selecting first one\" \n     CONNECTORS_RESPONSE = $( curl -s  $SSL_CERT  -X GET -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'   $MASKING_ENGINE /database-connectors ) \n    check_error  \" $CONNECTORS_RESPONSE \" \n     NUM_CONNECTORS = $( echo   $CONNECTORS_RESPONSE   |  jq -r  '._pageInfo.total' ) \n    check_empty  $NUM_CONNECTORS   \"found no db connectors to use\" \n     CONNECTOR_ID = $( echo   $CONNECTORS_RESPONSE   |  jq -r  '.responseList[0].databaseConnectorId' ) \n     echo   \"using database connector ' $CONNECTOR_ID '\"  }  # Get all database rulesets and select the first one. Place the databaseRulesetId in $RULESET_ID. \nget_ruleset_id ()   { \n     echo   \"* getting all database rulesets and selecting first one\" \n     RULESETS_RESPONSE = $( curl -s  $SSL_CERT  -X GET -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'   $MASKING_ENGINE /database-rulesets ) \n    check_error  \" $RULESETS_RESPONSE \" \n     NUM_RULESETS = $( echo   $RULESETS_RESPONSE   |  jq -r  '._pageInfo.total' ) \n    check_empty  $NUM_RULESETS   \"found no db rulesets to use\" \n     RULESET_ID = $( echo   $RULESETS_RESPONSE   |  jq -r  '.responseList[0].databaseRulesetId' ) \n     echo   \"using database ruleset ' $RULESET_ID '\"  }  # Get all database tables for a database connector specificed by $CONNECTOR_ID. Select the first one and place in $TABLE_NAME. \nget_table ()   { \n     echo   \"* getting all tables for connector ' $CONNECTOR_ID ' and selecting first one\" \n     TABLES_RESPONSE = $( curl -s  $SSL_CERT  -X GET -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'   $MASKING_ENGINE /database-connectors/ $CONNECTOR_ID /fetch ) \n    check_error  \" $TABLES_RESPONSE \" \n     NUM_TABLES = $( echo   $TABLES_RESPONSE   |  jq -r  '. | length' ) \n    check_empty  $NUM_TABLES   \"found no tables to use\" \n     TABLE_NAME = $( echo   $TABLES_RESPONSE   |  jq -r  '.[0]' ) \n     echo   \"using table ' $TABLE_NAME '\"  }  # Get all column metadata for table metadata specified by $TABLE_METADATA_ID. Select the first one and place in $COLUMN_METADATA_ID. \nget_column_metadata_id ()   { \n     echo   \"* getting all column metadata belonging to table metadata ' $TABLE_METADATA_ID ' and selecting the first one\" \n     COLUMNS_RESPONSE = $( curl -s  $SSL_CERT  -X GET -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'   $MASKING_ENGINE /column-metadata?table_metadata_id = $TABLE_METADATA_ID ) \n    check_error  \" $COLUMNS_RESPONSE \" \n     NUM_COLUMNS = $( echo   $COLUMNS_RESPONSE   |  jq -r  '. | length' ) \n    check_empty  $NUM_COLUMNS   \"found no columns to use\" \n     COLUMN_METADATA = $( echo   $COLUMNS_RESPONSE   |  jq -r  '.responseList[0]' ) \n     COLUMN_METADATA_ID = $( echo   $COLUMN_METADATA   |  jq -r  '.columnMetadataId' ) \n     echo   \"using column ' $COLUMN_METADATA_ID '\"  }  # Check if $1 is equal to 0. If so print out message specified in $2 and exit. \ncheck_empty ()   { \n     if   [   $1  -eq  0   ] ;   then \n         echo   $2 \n         exit   1 \n     fi  }  # Check if $1 is an object and if it has an 'errorMessage' specified. If so, print the object and exit. \ncheck_error ()   { \n     # jq returns a literal null so we have to check againt that... \n     if   [   \" $( echo   \" $1 \"   |  jq -r  'if type==\"object\" then .errorMessage else \"null\" end' ) \"  ! =   'null'   ] ;   then \n         echo   $1 \n         exit   1 \n     fi  }",
            "title": "helpers"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/apiHostInfo/",
            "text": "apiHostInfo\n\u00b6\n\n\n#!/bin/bash\n\n\n\n#\n\n\n# This file contains all the host information for the masking engine. Additionally,\n\n\n# this file allows configuration of SSL if desired.\n\n\n#\n\n\n\n\n# update host name\n\n\nHOST\n=\n\"myMaskingEngine.com\"\n\n\nAPI_PATH\n=\n\"masking/api\"\n\n\n\n# To connect via SSL, set $SSL to \"on\" and update the port if necessary (default 8443).\n\n\n# Additionally, you must update the path to the ssl certificate.\n\n\nSSL\n=\n\"off\"\n\n\nSSL_PORT\n=\n\"8443\"\n\n\n# update cert name\n\n\nSSL_CERT_PATH\n=\n\"self-signed.cer\"\n\n\n\nif\n \n[\n \n\"\n$SSL\n\"\n \n=\n \n\"on\"\n \n]\n\n\nthen\n\n    \nMASKING_ENGINE\n=\n\"https://\n$HOST\n:\n$SSL_PORT\n/\n$API_PATH\n\"\n\n    \nSSL_CERT\n=\n\"--cacert \n$SSL_CERT_PATH\n\"\n\n\nelse\n\n    \nMASKING_ENGINE\n=\n\"http://\n$HOST\n:8282/\n$API_PATH\n\"\n\n    \nSSL_CERT\n=\n\"\"\n\n\nfi",
            "title": "apiHostInfo"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/apiHostInfo/#apihostinfo",
            "text": "#!/bin/bash  #  # This file contains all the host information for the masking engine. Additionally,  # this file allows configuration of SSL if desired.  #  # update host name  HOST = \"myMaskingEngine.com\"  API_PATH = \"masking/api\"  # To connect via SSL, set $SSL to \"on\" and update the port if necessary (default 8443).  # Additionally, you must update the path to the ssl certificate.  SSL = \"off\"  SSL_PORT = \"8443\"  # update cert name  SSL_CERT_PATH = \"self-signed.cer\"  if   [   \" $SSL \"   =   \"on\"   ]  then \n     MASKING_ENGINE = \"https:// $HOST : $SSL_PORT / $API_PATH \" \n     SSL_CERT = \"--cacert  $SSL_CERT_PATH \"  else \n     MASKING_ENGINE = \"http:// $HOST :8282/ $API_PATH \" \n     SSL_CERT = \"\"  fi",
            "title": "apiHostInfo"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/createApplication/",
            "text": "createApplication\n\u00b6\n\n\n#!/bin/bash\n\n\n\n#\n\n\n# This script will login and create an application. It depends on helpers in the helpers script as well as host and login\n\n\n# information found in apiHostInfo and loginCredentials, respectively.\n\n\n#\n\n\n\nsource\n apiHostInfo\n\neval\n \n$(\ncat loginCredentials\n)\n\n\nsource\n helpers\n\nlogin\n\n\necho\n \n\"* creating application 'App123'...\"\n\ncurl \n$SSL_CERT\n -X POST -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n -H \n'Accept: application/json'\n --data @- \n$MASKING_ENGINE\n/applications \n<<EOF\n\n\n{\n\n\n    \"applicationName\": \"App123\"\n\n\n}\n\n\nEOF\n\n\n\necho",
            "title": "createApplication"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/createApplication/#createapplication",
            "text": "#!/bin/bash  #  # This script will login and create an application. It depends on helpers in the helpers script as well as host and login  # information found in apiHostInfo and loginCredentials, respectively.  #  source  apiHostInfo eval   $( cat loginCredentials )  source  helpers\n\nlogin echo   \"* creating application 'App123'...\" \ncurl  $SSL_CERT  -X POST -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'  -H  'Accept: application/json'  --data @-  $MASKING_ENGINE /applications  <<EOF  {      \"applicationName\": \"App123\"  }  EOF  echo",
            "title": "createApplication"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/createEnvironment/",
            "text": "createEnvironment\n\u00b6\n\n\n#!/bin/bash\n\n\n\n#\n\n\n# This script will login and create an environment with an application. It depends on helpers in the helpers\n\n\n# script as well as host and login information found in apiHostInfo and loginCredentials, respectively.\n\n\n#\n\n\n\nsource\n apiHostInfo\n\neval\n \n$(\ncat loginCredentials\n)\n\n\nsource\n helpers\n\nlogin\n\n\n#\n\n\n# When deciding which application to place the environment in we simply choose the first application found. You are\n\n\n# encouraged to modify this to suit your needs. Please see get_application_id in helpers for more information.\n\n\n#\n\nget_application_id\n\n\necho\n \n\"* creating environment 'newEnv' in application '\n$APPLICATION_ID\n'...\"\n\ncurl \n$SSL_CERT\n -X POST -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n -H \n'Accept: application/json'\n --data @- \n$MASKING_ENGINE\n/environments \n<<EOF\n\n\n{\n\n\n    \"environmentName\": \"newEnv\",\n\n\n    \"application\": \"$APPLICATION_ID\",\n\n\n    \"purpose\": \"MASK\"\n\n\n}\n\n\nEOF\n\n\n\necho",
            "title": "createEnvironment"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/createEnvironment/#createenvironment",
            "text": "#!/bin/bash  #  # This script will login and create an environment with an application. It depends on helpers in the helpers  # script as well as host and login information found in apiHostInfo and loginCredentials, respectively.  #  source  apiHostInfo eval   $( cat loginCredentials )  source  helpers\n\nlogin #  # When deciding which application to place the environment in we simply choose the first application found. You are  # encouraged to modify this to suit your needs. Please see get_application_id in helpers for more information.  # \nget_application_id echo   \"* creating environment 'newEnv' in application ' $APPLICATION_ID '...\" \ncurl  $SSL_CERT  -X POST -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'  -H  'Accept: application/json'  --data @-  $MASKING_ENGINE /environments  <<EOF  {      \"environmentName\": \"newEnv\",      \"application\": \"$APPLICATION_ID\",      \"purpose\": \"MASK\"  }  EOF  echo",
            "title": "createEnvironment"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/createInventory/",
            "text": "createInventory\n\u00b6\n\n\n#!/bin/bash\n\n\n\n#\n\n\n# This script will login, create table metadata for a given table name and ruleset, and then update an\n\n\n# inventory (i.e. assign an algorithm and domain to a specific column of the table). It depends on helpers\n\n\n# in the helpers script as well as host and login information found in apiHostInfo and loginCredentials, respectively.\n\n\n# This script uses jq to process JSON. More information can be found here - https://stedolan.github.io/jq/.\n\n\n#\n\n\n\nsource\n apiHostInfo\n\neval\n \n$(\ncat loginCredentials\n)\n\n\nsource\n helpers\n\nlogin\n\n\n#\n\n\n# When deciding which connector, ruleset, and table to use we simply use the first ones found of each. You are\n\n\n# encouraged to modify this to suit your needs. Please see the respective functions in helpers for more information.\n\n\n#\n\nget_connector_id\nget_ruleset_id\nget_table\n\n\necho\n \n\"* creating table metadata for ruleset id '\n$RULESET_ID\n' with table '\n$TABLE_NAME\n'...\"\n\n\nTABLE_METADATA_RESPONSE\n=\n$(\ncurl \n$SSL_CERT\n -s -X POST -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n -H \n'Accept: application/json'\n --data @- \n$MASKING_ENGINE\n/table-metadata \n<<EOF\n\n\n{\n\n\n    \"tableName\": \"$TABLE_NAME\",\n\n\n    \"rulesetId\": $RULESET_ID\n\n\n}\n\n\nEOF\n)\n\ncheck_error \n\"\n$TABLE_METADATA_RESPONSE\n\"\n\n\nTABLE_METADATA_ID\n=\n$(\necho\n \n$TABLE_METADATA_RESPONSE\n \n|\n jq -r \n'.tableMetadataId'\n)\n\n\necho\n \n\"using table metadata '\n$TABLE_METADATA_ID\n'\"\n\n\nget_column_metadata_id\n\ncurl \n$SSL_CERT\n -X PUT -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n -H \n'Accept: application/json'\n --data @- \n$MASKING_ENGINE\n/column-metadata/\n$COLUMN_METADATA_ID\n  \n<<EOF\n\n\n{\n\n\n    \"algorithmName\": \"AddrLine2Lookup\",\n\n\n    \"domainName\": \"ADDRESS_LINE2\"\n\n\n}\n\n\nEOF\n\n\n\necho",
            "title": "createInventory"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/createInventory/#createinventory",
            "text": "#!/bin/bash  #  # This script will login, create table metadata for a given table name and ruleset, and then update an  # inventory (i.e. assign an algorithm and domain to a specific column of the table). It depends on helpers  # in the helpers script as well as host and login information found in apiHostInfo and loginCredentials, respectively.  # This script uses jq to process JSON. More information can be found here - https://stedolan.github.io/jq/.  #  source  apiHostInfo eval   $( cat loginCredentials )  source  helpers\n\nlogin #  # When deciding which connector, ruleset, and table to use we simply use the first ones found of each. You are  # encouraged to modify this to suit your needs. Please see the respective functions in helpers for more information.  # \nget_connector_id\nget_ruleset_id\nget_table echo   \"* creating table metadata for ruleset id ' $RULESET_ID ' with table ' $TABLE_NAME '...\"  TABLE_METADATA_RESPONSE = $( curl  $SSL_CERT  -s -X POST -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'  -H  'Accept: application/json'  --data @-  $MASKING_ENGINE /table-metadata  <<EOF  {      \"tableName\": \"$TABLE_NAME\",      \"rulesetId\": $RULESET_ID  }  EOF ) \ncheck_error  \" $TABLE_METADATA_RESPONSE \"  TABLE_METADATA_ID = $( echo   $TABLE_METADATA_RESPONSE   |  jq -r  '.tableMetadataId' )  echo   \"using table metadata ' $TABLE_METADATA_ID '\" \n\nget_column_metadata_id\n\ncurl  $SSL_CERT  -X PUT -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'  -H  'Accept: application/json'  --data @-  $MASKING_ENGINE /column-metadata/ $COLUMN_METADATA_ID    <<EOF  {      \"algorithmName\": \"AddrLine2Lookup\",      \"domainName\": \"ADDRESS_LINE2\"  }  EOF  echo",
            "title": "createInventory"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/create_DatabaseConnector/",
            "text": "create DatabaseConnector\n\u00b6\n\n\n#!/bin/bash\n\n\n\n#\n\n\n# This script will login and create a database connector in an environment. It depends on helpers in the helpers\n\n\n# script as well as host and login information found in apiHostInfo and loginCredentials, respectively.\n\n\n#\n\n\n\nsource\n apiHostInfo\n\neval\n \n$(\ncat loginCredentials\n)\n\n\nsource\n helpers\n\nlogin\n\n\n#\n\n\n# When deciding which environment to place the connector in we simply choose the first environment found. You are\n\n\n# encouraged to modify this to suit your needs. Please see get_environment_id in helpers for more information.\n\n\n#\n\nget_environment_id\n\n\necho\n \n\"* creating database connector 'connector' in environment '\n$ENVIRONMENT_ID\n'...\"\n\ncurl \n$SSL_CERT\n -X POST -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n -H \n'Accept: application/json'\n --data @- \n$MASKING_ENGINE\n/database-connectors \n<<EOF\n\n\n{\n\n\n    \"connectorName\": \"connector\",\n\n\n    \"databaseType\": \"ORACLE\",\n\n\n    \"environmentId\": $ENVIRONMENT_ID,\n\n\n    \"host\": \"myHost\",\n\n\n    \"password\": \"myPassword\",\n\n\n    \"port\": 1234,\n\n\n    \"schemaName\": \"MYSCHEMA\",\n\n\n    \"sid\": \"mySID\",\n\n\n    \"username\": \"MYUSERNAME\"\n\n\n}\n\n\nEOF\n\n\n\necho",
            "title": "create DatabaseConnector"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/create_DatabaseConnector/#create-databaseconnector",
            "text": "#!/bin/bash  #  # This script will login and create a database connector in an environment. It depends on helpers in the helpers  # script as well as host and login information found in apiHostInfo and loginCredentials, respectively.  #  source  apiHostInfo eval   $( cat loginCredentials )  source  helpers\n\nlogin #  # When deciding which environment to place the connector in we simply choose the first environment found. You are  # encouraged to modify this to suit your needs. Please see get_environment_id in helpers for more information.  # \nget_environment_id echo   \"* creating database connector 'connector' in environment ' $ENVIRONMENT_ID '...\" \ncurl  $SSL_CERT  -X POST -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'  -H  'Accept: application/json'  --data @-  $MASKING_ENGINE /database-connectors  <<EOF  {      \"connectorName\": \"connector\",      \"databaseType\": \"ORACLE\",      \"environmentId\": $ENVIRONMENT_ID,      \"host\": \"myHost\",      \"password\": \"myPassword\",      \"port\": 1234,      \"schemaName\": \"MYSCHEMA\",      \"sid\": \"mySID\",      \"username\": \"MYUSERNAME\"  }  EOF  echo",
            "title": "create DatabaseConnector"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/create_DatabaseRuleset/",
            "text": "create DatabaseRuleset\n\u00b6\n\n\n#!/bin/bash\n\n\n\n#\n\n\n# This script will login and create a database ruleset for a database connector. It depends on helpers in the helpers\n\n\n# script as well as host and login information found in apiHostInfo and loginCredentials, respectively.\n\n\n#\n\n\n\nsource\n apiHostInfo\n\neval\n \n$(\ncat loginCredentials\n)\n\n\nsource\n helpers\n\nlogin\n\n\n#\n\n\n# When deciding which database connector we will use, we simply choose the first database connector found. You are\n\n\n# encouraged to modify this to suit your needs. Please see get_connector_id in helpers for more information.\n\n\n#\n\nget_connector_id\n\n\necho\n \n\"* creating database ruleset 'myRuleset' in db connector '\n$CONNECTOR_ID\n'...\"\n\ncurl \n$SSL_CERT\n -X POST -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n -H \n'Accept: application/json'\n --data @- \n$MASKING_ENGINE\n/database-rulesets \n<<EOF\n\n\n{\n\n\n    \"rulesetName\": \"myRuleset\",\n\n\n    \"databaseConnectorId\": $CONNECTOR_ID\n\n\n}\n\n\nEOF\n\n\n\necho",
            "title": "create DatabaseRuleset"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/create_DatabaseRuleset/#create-databaseruleset",
            "text": "#!/bin/bash  #  # This script will login and create a database ruleset for a database connector. It depends on helpers in the helpers  # script as well as host and login information found in apiHostInfo and loginCredentials, respectively.  #  source  apiHostInfo eval   $( cat loginCredentials )  source  helpers\n\nlogin #  # When deciding which database connector we will use, we simply choose the first database connector found. You are  # encouraged to modify this to suit your needs. Please see get_connector_id in helpers for more information.  # \nget_connector_id echo   \"* creating database ruleset 'myRuleset' in db connector ' $CONNECTOR_ID '...\" \ncurl  $SSL_CERT  -X POST -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'  -H  'Accept: application/json'  --data @-  $MASKING_ENGINE /database-rulesets  <<EOF  {      \"rulesetName\": \"myRuleset\",      \"databaseConnectorId\": $CONNECTOR_ID  }  EOF  echo",
            "title": "create DatabaseRuleset"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/getAuditLogs/",
            "text": "getAuditLogs\n\u00b6\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n#!/bin/bash\n\n\n\n#\n\n\n# This script is an \"out of the box\" script that goes through\n\n\n# Login and GET /audit-logs with the authentication\n\n\n# token from Login\n\n\n#\n\n\n\nsource\n apiHostInfo\n\neval\n \n$(\ncat loginCredentials\n)\n\n\nsource\n helpers\n\nlogin\n\n\necho\n \n\"* GET /audit-logs from \n$EXPORT_ENGINE\n\"\n\n\nEXPORT_RESPONSE\n=\n$(\ncurl \n$SSL_CERT\n -X GET -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Accept: application/json'\n \n$MASKING_ENGINE\n/audit-logs\n)\n\n\n\n# Calculate the number of audit log entries and the proximity to the entry limit.\n\n\nAUDIT_ENTRY_COUNT\n=\n$(\njq \n'._pageInfo.total'\n \n<<<\n\"\n$EXPORT_RESPONSE\n\"\n)\n\n\nMAX_ENTRIES\n=\n1000000\n\n\nDIFFERENCE\n=\n$((\nMAX_ENTRIES-AUDIT_ENTRY_COUNT\n))\n\n\n\n# Retrieve the date of the oldest audit entry retained.\n\n\nOLDEST_DATE\n=\n$(\njq \n'.responseList[1].activityTime'\n \n<<<\n\"\n$EXPORT_RESPONSE\n\"\n)\n\n\n\necho\n \n\"There are \n$AUDIT_ENTRY_COUNT\n entries in the audit log. After \n$DIFFERENCE\n more audits you will hit the \n$MAX_ENTRIES\n limit and will begin to overwrite entries starting from the oldest, which was created on: \n$OLDEST_DATE\n\"",
            "title": "getAuditLogs"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/getAuditLogs/#getauditlogs",
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 #!/bin/bash  #  # This script is an \"out of the box\" script that goes through  # Login and GET /audit-logs with the authentication  # token from Login  #  source  apiHostInfo eval   $( cat loginCredentials )  source  helpers\n\nlogin echo   \"* GET /audit-logs from  $EXPORT_ENGINE \"  EXPORT_RESPONSE = $( curl  $SSL_CERT  -X GET -H  '' \" $AUTH_HEADER \" ''  -H  'Accept: application/json'   $MASKING_ENGINE /audit-logs )  # Calculate the number of audit log entries and the proximity to the entry limit.  AUDIT_ENTRY_COUNT = $( jq  '._pageInfo.total'   <<< \" $EXPORT_RESPONSE \" )  MAX_ENTRIES = 1000000  DIFFERENCE = $(( MAX_ENTRIES-AUDIT_ENTRY_COUNT ))  # Retrieve the date of the oldest audit entry retained.  OLDEST_DATE = $( jq  '.responseList[1].activityTime'   <<< \" $EXPORT_RESPONSE \" )  echo   \"There are  $AUDIT_ENTRY_COUNT  entries in the audit log. After  $DIFFERENCE  more audits you will hit the  $MAX_ENTRIES  limit and will begin to overwrite entries starting from the oldest, which was created on:  $OLDEST_DATE \"",
            "title": "getAuditLogs"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/getSyncableObjects/",
            "text": "getSyncableObjects\n\u00b6\n\n\n#!/bin/bash\n\n\n\n#\n\n\n# This script is an \"out of the box\" script that goes through\n\n\n# Login and GET /syncable-objects with the authentication\n\n\n# token from Login\n\n\n#\n\n\n\nsource\n apiHostInfo\n\neval\n \n$(\ncat loginCredentials\n)\n\n\nsource\n helpers\n\nlogin\n\n\necho\n \n\"* GET /syncable-objects from \n$EXPORT_ENGINE\n\"\n\n\nEXPORT_RESPONSE\n=\n$(\ncurl \n$SSL_CERT\n -X GET -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Accept: application/json'\n \n$MASKING_ENGINE\n/syncable-objects\n)\n\n\necho\n \n$EXPORT_RESPONSE",
            "title": "getSyncableObjects"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/getSyncableObjects/#getsyncableobjects",
            "text": "#!/bin/bash  #  # This script is an \"out of the box\" script that goes through  # Login and GET /syncable-objects with the authentication  # token from Login  #  source  apiHostInfo eval   $( cat loginCredentials )  source  helpers\n\nlogin echo   \"* GET /syncable-objects from  $EXPORT_ENGINE \"  EXPORT_RESPONSE = $( curl  $SSL_CERT  -X GET -H  '' \" $AUTH_HEADER \" ''  -H  'Accept: application/json'   $MASKING_ENGINE /syncable-objects )  echo   $EXPORT_RESPONSE",
            "title": "getSyncableObjects"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/getSyncableObjectsExport/",
            "text": "getSyncableObjectsExport\n\u00b6\n\n\n#!/bin/bash\n\n\n\n#\n\n\n# This script will log in and get all syncable objects on\n\n\n# the Masking Engine and then, given a grouping command, save the\n\n\n# exported document in a file and export all syncable objects\n\n\n# in the indicated group \n\n\n#\n\n\n# Grouping command:\n\n\n# algoType: -t <LOOKUP | BINARYLOOKUP | SEGMENT | TOKENIZATION | MAPPLET | KEY>\n\n\n# algoCd: -n <RegexForAlgoName>\n\n\n#\n\n\n# Currently the response from GET /syncable-objects is saved\n\n\n# to getobj_response.json, and the grouped input for /export\n\n\n# in grouped_export_list.json, and the final export response\n\n\n# into export_response.json. But of course, this can script\n\n\n# can be modified to save to other specified places.\n\n\n#\n\n\n\nsource\n apiHostInfo\n\neval\n \n$(\ncat loginCredentials\n)\n\n\nsource\n helpers\n\nlogin\n\n\necho\n \n\"* GET /syncable-objects\"\n\n\nGETOBJ_RESPONSE\n=\n$(\ncurl \n$SSL_CERT\n -X GET -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n \n$MASKING_ENGINE\n/syncable-objects\n)\n\n\necho\n \n$GETOBJ_RESPONSE\n > \n\"./getobj_response.json\"\n\n\n\n# Create a temporary export list file\n\n\nGROUPED_EXPORT_LIST\n=\n\"./grouped_export_list.json\"\n\n\necho\n \n\"[]\"\n > \n$GROUPED_EXPORT_LIST\n\n\n\nif\n \n[[\n \n$1\n \n==\n \n\"-t\"\n \n]]\n;\n \nthen\n\n   \nALGO_TYPE\n=\n$2\n\n   \necho\n \n\"* Filter for all syncable objects of algorithm type \n$ALGO_TYPE\n\"\n \n\n   jq -c \n'.responseList[]'\n getobj_response.json \n|\n \nwhile\n \nread\n i\n;\n \ndo\n\n      \nif\n \n[[\n \n$(\necho\n \n$i\n \n|\n jq \n'.objectType'\n)\n \n==\n \n\\\"\n$ALGO_TYPE\n\\\"\n \n]]\n;\n \nthen\n\n         \n# The key to getting the correct json format here was to use\n\n         \n# the --argjson instead of --arg. --arg will stringify everything\n\n         \n# and escape all special characters like {, \", etc.\n\n         \necho\n \n$(\ncat \n$GROUPED_EXPORT_LIST\n \n|\n jq --argjson obj \n\"\n$i\n\"\n \n'. |= . + [$obj]'\n)\n > \n$GROUPED_EXPORT_LIST\n\n      \nfi\n\n   \ndone\n\n\nelif\n \n[[\n \n$1\n \n==\n \n\"-n\"\n \n]]\n;\n \nthen\n\n   \nALGO_NAME_REGEX\n=\n$2\n\n   \necho\n \n\"* Filter for all syncable objects where algorithmCd matches the regex \n$ALGO_NAME_REGEX\n\"\n\n\n   jq -c \n'.responseList[]'\n getobj_response.json \n|\n \nwhile\n \nread\n i\n;\n \ndo\n\n      \nif\n \n[[\n \n\"\n$(\necho\n \n$i\n \n|\n jq \n'.objectIdentifier.algorithmName'\n)\n\"\n \n=\n~ \n\\\"\n$ALGO_NAME_REGEX\n\\\"\n \n]]\n;\n \nthen\n\n          \necho\n \n$(\ncat \n$GROUPED_EXPORT_LIST\n \n|\n jq --argjson obj \n\"\n$i\n\"\n \n'. |= . + [$obj]'\n)\n > \n$GROUPED_EXPORT_LIST\n\n      \nfi\n\n   \ndone\n\n\nfi\n\n\n\necho\n \n\"* Export syncable objects from \n$GROUPED_EXPORT_LIST\n\"\n\n\nEXPORT_RESPONSE\n=\n$(\ncurl \n$SSL_CERT\n -X POST -H \n''\n\"\n$AUTH_HEADER\n\"\n''\n -H \n'Content-Type: application/json'\n -H \n'Accept: application/json'\n -d \n\"\n$(\n<\n$GROUPED_EXPORT_LIST\n)\n\"\n \n$MASKING_ENGINE\n/export\n)\n\n\n\n# Save the grouped export response into a file\n\n\necho\n \n$EXPORT_RESPONSE\n > export_response.json\n\necho\n \n'* Completed exporting. Check \"export_response.json\" for the export document. This export document json object will be what you literally put in as the input for import'",
            "title": "getSyncableObjectsExport"
        },
        {
            "location": "/Delphix_Masking_APIs/API_Examples/getSyncableObjectsExport/#getsyncableobjectsexport",
            "text": "#!/bin/bash  #  # This script will log in and get all syncable objects on  # the Masking Engine and then, given a grouping command, save the  # exported document in a file and export all syncable objects  # in the indicated group   #  # Grouping command:  # algoType: -t <LOOKUP | BINARYLOOKUP | SEGMENT | TOKENIZATION | MAPPLET | KEY>  # algoCd: -n <RegexForAlgoName>  #  # Currently the response from GET /syncable-objects is saved  # to getobj_response.json, and the grouped input for /export  # in grouped_export_list.json, and the final export response  # into export_response.json. But of course, this can script  # can be modified to save to other specified places.  #  source  apiHostInfo eval   $( cat loginCredentials )  source  helpers\n\nlogin echo   \"* GET /syncable-objects\"  GETOBJ_RESPONSE = $( curl  $SSL_CERT  -X GET -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'   $MASKING_ENGINE /syncable-objects )  echo   $GETOBJ_RESPONSE  >  \"./getobj_response.json\"  # Create a temporary export list file  GROUPED_EXPORT_LIST = \"./grouped_export_list.json\"  echo   \"[]\"  >  $GROUPED_EXPORT_LIST  if   [[   $1   ==   \"-t\"   ]] ;   then \n    ALGO_TYPE = $2 \n    echo   \"* Filter for all syncable objects of algorithm type  $ALGO_TYPE \"  \n\n   jq -c  '.responseList[]'  getobj_response.json  |   while   read  i ;   do \n       if   [[   $( echo   $i   |  jq  '.objectType' )   ==   \\\" $ALGO_TYPE \\\"   ]] ;   then \n          # The key to getting the correct json format here was to use \n          # the --argjson instead of --arg. --arg will stringify everything \n          # and escape all special characters like {, \", etc. \n          echo   $( cat  $GROUPED_EXPORT_LIST   |  jq --argjson obj  \" $i \"   '. |= . + [$obj]' )  >  $GROUPED_EXPORT_LIST \n       fi \n    done  elif   [[   $1   ==   \"-n\"   ]] ;   then \n    ALGO_NAME_REGEX = $2 \n    echo   \"* Filter for all syncable objects where algorithmCd matches the regex  $ALGO_NAME_REGEX \" \n\n   jq -c  '.responseList[]'  getobj_response.json  |   while   read  i ;   do \n       if   [[   \" $( echo   $i   |  jq  '.objectIdentifier.algorithmName' ) \"   = ~  \\\" $ALGO_NAME_REGEX \\\"   ]] ;   then \n           echo   $( cat  $GROUPED_EXPORT_LIST   |  jq --argjson obj  \" $i \"   '. |= . + [$obj]' )  >  $GROUPED_EXPORT_LIST \n       fi \n    done  fi  echo   \"* Export syncable objects from  $GROUPED_EXPORT_LIST \"  EXPORT_RESPONSE = $( curl  $SSL_CERT  -X POST -H  '' \" $AUTH_HEADER \" ''  -H  'Content-Type: application/json'  -H  'Accept: application/json'  -d  \" $( < $GROUPED_EXPORT_LIST ) \"   $MASKING_ENGINE /export )  # Save the grouped export response into a file  echo   $EXPORT_RESPONSE  > export_response.json echo   '* Completed exporting. Check \"export_response.json\" for the export document. This export document json object will be what you literally put in as the input for import'",
            "title": "getSyncableObjectsExport"
        }
    ]
}